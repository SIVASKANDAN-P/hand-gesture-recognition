{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharathsanjai/hand-gesture-recognition/blob/main/Neural_Nets_Project_Starter_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sawJV_5K_x6"
      },
      "source": [
        "# Gesture Recognition\n",
        "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running on GPU\n",
        "\n",
        "import platform\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"Python version: \", platform.python_version())\n",
        "print(\"TensorFlow version: \", tf.__version__)\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    print('GPU not found. Running on CPU.')\n",
        "else:\n",
        "    print('GPU found. Running on GPU.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgIVyEQ4LBZA",
        "outputId": "e8455de5-8a8b-4878-c36b-b3fd12c8caac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version:  3.10.12\n",
            "TensorFlow version:  2.12.0\n",
            "GPU found. Running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imageio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSMemTBiLXvw",
        "outputId": "779dd0f5-dbb8-4b2e-ce5d-3dda4b0db0a3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.31.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio) (1.23.5)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBmXL4DJL74X",
        "outputId": "c99fd1e0-a5aa-4ce7-9ce5-2322bb7966ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "phRPEtO4K_x8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "# from scipy.misc import imread, imresize\n",
        "from skimage.transform import resize\n",
        "from imageio import imread\n",
        "import imageio\n",
        "import datetime\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNz67W2HK_x9"
      },
      "source": [
        "We set the random seed so that the results don't vary drastically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zEsCXFXtK_x-"
      },
      "outputs": [],
      "source": [
        "np.random.seed(30)\n",
        "import random as rn\n",
        "rn.seed(30)\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "# tf.set_random_seed(30)\n",
        "tf.random.set_seed(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np4XHrNUK_x-"
      },
      "source": [
        "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rHivD4ZpK_x-"
      },
      "outputs": [],
      "source": [
        "train_doc = np.random.permutation(open('/content/drive/My Drive/Project_data/train.csv').readlines())\n",
        "val_doc = np.random.permutation(open('/content/drive/My Drive/Project_data/val.csv').readlines())\n",
        "batch_size = 10 # experiment with the batch size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# np.random.seed(5)\n",
        "# random_numbers = np.random.randint(0, 30, size=18)\n",
        "# random_numbers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgDl0xVSWNQj",
        "outputId": "15678f82-017d-4351-df8f-6d9cc5102b8d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3, 14, 15, 29,  6, 22, 16,  9,  8,  4,  7, 27, 16, 16,  7, 12, 15,\n",
              "       17])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# samples_per_folder = 18\n",
        "\n",
        "# def batchsize(folder_list):\n",
        "#   result = math.ceil(folder_list.length *  samples_per_folder / batch_size)\n",
        "#   print('result is =', result)"
      ],
      "metadata": {
        "id": "VtrNSyl4XcPL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size = 10 # experiment with the batch size\n",
        "# selecting alternate frames\n",
        "# seq_idx =  np.round(np.linspace(0, 29, 20)).astype(int)\n",
        "# image dimensions\n",
        "# dim_x, dim_y = 120, 120"
      ],
      "metadata": {
        "id": "a1wV8Ujt560w"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cropResize(image, y, z):\n",
        "    h, w = image.shape\n",
        "\n",
        "    # if smaller image crop at center for 120x120\n",
        "    if w == 160:\n",
        "        image = image[:120, 20:140]\n",
        "\n",
        "    # resize every image\n",
        "    return resize(image, (y,z))"
      ],
      "metadata": {
        "id": "LU48hYcLyJF6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getImgTensor(n_frames):\n",
        "    img_idx = np.round(np.linspace(0, 29, n_frames)).astype(int)\n",
        "    return [img_idx, 120, 120, 3]\n",
        "\n",
        "# define image tensor size\n",
        "img_tensor = getImgTensor(20)\n",
        "print ('# img_tensor =', img_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMSQr8qVuie-",
        "outputId": "02303a01-0dda-47f6-9596-0c1168f166d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# img_tensor = [array([ 0,  2,  3,  5,  6,  8,  9, 11, 12, 14, 15, 17, 18, 20, 21, 23, 24,\n",
            "       26, 27, 29]), 120, 120, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizeImage(image):\n",
        "    # applying normalization\n",
        "    return image/255.0"
      ],
      "metadata": {
        "id": "JT0VSSh_DZOV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessImage(image, y, z):\n",
        "    return normalizeImage(cropResize(image, y, z))"
      ],
      "metadata": {
        "id": "12jNyoU7EuS0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genhelper(source_path, folder_list, batch_size, img_tensor, t, batch):\n",
        "  [x,y,z] = [len(img_tensor[0]),img_tensor[1], img_tensor[2]]\n",
        "  # img_idx = [1,4,5,6,7,9,21]\n",
        "  # print('IMG tensor ...', img_tensor[0])\n",
        "  img_idx = img_tensor[0]\n",
        "  # print('IMG DX list', img_idx)\n",
        "  batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "  batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
        "  for folder in range(batch_size): # iterate over the batch_size\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
        "                # print(' Foder currently reading : ='+ source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0])\n",
        "                # print('IMGS...', imgs)\n",
        "\n",
        "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "                    # print('image index count : = ', idx , ' ; item =', item)\n",
        "                    # print('Batch: = ', batch , ' ; Batch size: =', batch_size , ' ; index =' , batch*batch_size)\n",
        "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "\n",
        "                    #crop the images and resize them. Note that the images are of 2 different shape\n",
        "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
        "\n",
        "                    batch_data[folder,idx,:,:,0] = preprocessImage(image[:, :, 0], y, z) #normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,1] = preprocessImage(image[:, :, 1], y, z) #normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,2] = preprocessImage(image[:, :, 2], y, z) #normalise and feed in the image\n",
        "\n",
        "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "  return batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
        "\n",
        "        # write the code for the remaining data points which are left after full batches\n"
      ],
      "metadata": {
        "id": "dOiNUJNGd8nw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOkg6cJzK_x-"
      },
      "source": [
        "## Generator\n",
        "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XdYPnHbWK_x_"
      },
      "outputs": [],
      "source": [
        "def generator(source_path, folder_list, batch_size, img_tensor):\n",
        "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
        "    # print('folder list length = ', folder_list)\n",
        "    # [x,y,z] = [len(img_tensor[0]),img_tensor[1], img_tensor[2]]\n",
        "    # img_idx = img_tensor[0]\n",
        "\n",
        "    while True:\n",
        "        t = np.random.permutation(folder_list)\n",
        "        num_batches = int(len(folder_list)/batch_size) # calculate the number of batches\n",
        "        print('; Num of batches =', num_batches)\n",
        "        for batch in range(num_batches): # we iterate over the number of batches\n",
        "            print('; Current Batch =', batch)\n",
        "            yield genhelper(source_path, folder_list, batch_size, img_tensor, t, batch)#you yield the batch_data and the batch_labels, remember what does yield do\n",
        "\n",
        "\n",
        "        # write the code for the remaining data points which are left after full batches\n",
        "        if len(folder_list)%batch_size != 0:\n",
        "          batch_size = len(folder_list)%batch_size\n",
        "          yield genhelper(source_path, folder_list, batch_size, img_tensor, t, batch)#you yield the batch_data and the batch_labels, remember what does yield do"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vwj3-x5pK_x_"
      },
      "source": [
        "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WqlDzAsK_yA",
        "outputId": "3fee3331-e64d-4873-9b4a-0936f5846c94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n",
            "# epochs = 10\n"
          ]
        }
      ],
      "source": [
        "curr_dt_time = datetime.datetime.now()\n",
        "train_path = '/content/drive/My Drive/Project_data/train'\n",
        "val_path = '/content/drive/My Drive/Project_data/val'\n",
        "num_train_sequences = len(train_doc)\n",
        "print('# training sequences =', num_train_sequences)\n",
        "num_val_sequences = len(val_doc)\n",
        "print('# validation sequences =', num_val_sequences)\n",
        "num_epochs = 10\n",
        "print ('# epochs =', num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check complete batch shape\n",
        "sample_generator = generator(train_path, train_doc, 32, img_tensor)\n",
        "sample_batch_data, sample_batch_labels = next(sample_generator)\n",
        "print(sample_batch_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OLuBnXlsBam",
        "outputId": "eb098829-7ea2-4c5e-e246-a7d06e010032"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source path =  /content/drive/My Drive/Project_data/train ; batch size = 32\n",
            "; Num of batches = 20\n",
            "; Current Batch = 0\n",
            "IMG tensor 0 [ 0  2  3  5  6  8  9 11 12 14 15 17 18 20 21 23 24 26 27 29]\n",
            "IMG DX list [ 0  2  3  5  6  8  9 11 12 14 15 17 18 20 21 23 24 26 27 29]\n",
            "IMGS... ['WIN_20180926_17_24_58_Pro_00035.png', 'WIN_20180926_17_24_58_Pro_00007.png', 'WIN_20180926_17_24_58_Pro_00049.png', 'WIN_20180926_17_24_58_Pro_00051.png', 'WIN_20180926_17_24_58_Pro_00047.png', 'WIN_20180926_17_24_58_Pro_00063.png', 'WIN_20180926_17_24_58_Pro_00033.png', 'WIN_20180926_17_24_58_Pro_00023.png', 'WIN_20180926_17_24_58_Pro_00021.png', 'WIN_20180926_17_24_58_Pro_00065.png', 'WIN_20180926_17_24_58_Pro_00013.png', 'WIN_20180926_17_24_58_Pro_00009.png', 'WIN_20180926_17_24_58_Pro_00039.png', 'WIN_20180926_17_24_58_Pro_00019.png', 'WIN_20180926_17_24_58_Pro_00045.png', 'WIN_20180926_17_24_58_Pro_00025.png', 'WIN_20180926_17_24_58_Pro_00059.png', 'WIN_20180926_17_24_58_Pro_00043.png', 'WIN_20180926_17_24_58_Pro_00055.png', 'WIN_20180926_17_24_58_Pro_00053.png', 'WIN_20180926_17_24_58_Pro_00029.png', 'WIN_20180926_17_24_58_Pro_00017.png', 'WIN_20180926_17_24_58_Pro_00015.png', 'WIN_20180926_17_24_58_Pro_00057.png', 'WIN_20180926_17_24_58_Pro_00037.png', 'WIN_20180926_17_24_58_Pro_00027.png', 'WIN_20180926_17_24_58_Pro_00031.png', 'WIN_20180926_17_24_58_Pro_00061.png', 'WIN_20180926_17_24_58_Pro_00041.png', 'WIN_20180926_17_24_58_Pro_00011.png']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-37d2658fef7f>:16: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMGS... ['WIN_20180926_17_35_17_Pro_00060.png', 'WIN_20180926_17_35_17_Pro_00022.png', 'WIN_20180926_17_35_17_Pro_00038.png', 'WIN_20180926_17_35_17_Pro_00020.png', 'WIN_20180926_17_35_17_Pro_00026.png', 'WIN_20180926_17_35_17_Pro_00028.png', 'WIN_20180926_17_35_17_Pro_00058.png', 'WIN_20180926_17_35_17_Pro_00034.png', 'WIN_20180926_17_35_17_Pro_00042.png', 'WIN_20180926_17_35_17_Pro_00050.png', 'WIN_20180926_17_35_17_Pro_00056.png', 'WIN_20180926_17_35_17_Pro_00014.png', 'WIN_20180926_17_35_17_Pro_00018.png', 'WIN_20180926_17_35_17_Pro_00030.png', 'WIN_20180926_17_35_17_Pro_00040.png', 'WIN_20180926_17_35_17_Pro_00010.png', 'WIN_20180926_17_35_17_Pro_00006.png', 'WIN_20180926_17_35_17_Pro_00024.png', 'WIN_20180926_17_35_17_Pro_00048.png', 'WIN_20180926_17_35_17_Pro_00062.png', 'WIN_20180926_17_35_17_Pro_00016.png', 'WIN_20180926_17_35_17_Pro_00052.png', 'WIN_20180926_17_35_17_Pro_00008.png', 'WIN_20180926_17_35_17_Pro_00044.png', 'WIN_20180926_17_35_17_Pro_00004.png', 'WIN_20180926_17_35_17_Pro_00054.png', 'WIN_20180926_17_35_17_Pro_00032.png', 'WIN_20180926_17_35_17_Pro_00036.png', 'WIN_20180926_17_35_17_Pro_00046.png', 'WIN_20180926_17_35_17_Pro_00012.png']\n",
            "IMGS... ['WIN_20180925_17_58_10_Pro_00056.png', 'WIN_20180925_17_58_10_Pro_00034.png', 'WIN_20180925_17_58_10_Pro_00040.png', 'WIN_20180925_17_58_10_Pro_00026.png', 'WIN_20180925_17_58_10_Pro_00020.png', 'WIN_20180925_17_58_10_Pro_00018.png', 'WIN_20180925_17_58_10_Pro_00042.png', 'WIN_20180925_17_58_10_Pro_00032.png', 'WIN_20180925_17_58_10_Pro_00002.png', 'WIN_20180925_17_58_10_Pro_00058.png', 'WIN_20180925_17_58_10_Pro_00024.png', 'WIN_20180925_17_58_10_Pro_00050.png', 'WIN_20180925_17_58_10_Pro_00030.png', 'WIN_20180925_17_58_10_Pro_00048.png', 'WIN_20180925_17_58_10_Pro_00036.png', 'WIN_20180925_17_58_10_Pro_00028.png', 'WIN_20180925_17_58_10_Pro_00010.png', 'WIN_20180925_17_58_10_Pro_00022.png', 'WIN_20180925_17_58_10_Pro_00014.png', 'WIN_20180925_17_58_10_Pro_00004.png', 'WIN_20180925_17_58_10_Pro_00060.png', 'WIN_20180925_17_58_10_Pro_00052.png', 'WIN_20180925_17_58_10_Pro_00054.png', 'WIN_20180925_17_58_10_Pro_00016.png', 'WIN_20180925_17_58_10_Pro_00008.png', 'WIN_20180925_17_58_10_Pro_00046.png', 'WIN_20180925_17_58_10_Pro_00006.png', 'WIN_20180925_17_58_10_Pro_00038.png', 'WIN_20180925_17_58_10_Pro_00044.png', 'WIN_20180925_17_58_10_Pro_00012.png']\n",
            "IMGS... ['WIN_20180925_17_20_16_Pro_00014.png', 'WIN_20180925_17_20_16_Pro_00011.png', 'WIN_20180925_17_20_16_Pro_00039.png', 'WIN_20180925_17_20_16_Pro_00023.png', 'WIN_20180925_17_20_16_Pro_00034.png', 'WIN_20180925_17_20_16_Pro_00025.png', 'WIN_20180925_17_20_16_Pro_00026.png', 'WIN_20180925_17_20_16_Pro_00020.png', 'WIN_20180925_17_20_16_Pro_00032.png', 'WIN_20180925_17_20_16_Pro_00015.png', 'WIN_20180925_17_20_16_Pro_00027.png', 'WIN_20180925_17_20_16_Pro_00024.png', 'WIN_20180925_17_20_16_Pro_00036.png', 'WIN_20180925_17_20_16_Pro_00030.png', 'WIN_20180925_17_20_16_Pro_00037.png', 'WIN_20180925_17_20_16_Pro_00012.png', 'WIN_20180925_17_20_16_Pro_00019.png', 'WIN_20180925_17_20_16_Pro_00038.png', 'WIN_20180925_17_20_16_Pro_00018.png', 'WIN_20180925_17_20_16_Pro_00031.png', 'WIN_20180925_17_20_16_Pro_00033.png', 'WIN_20180925_17_20_16_Pro_00035.png', 'WIN_20180925_17_20_16_Pro_00029.png', 'WIN_20180925_17_20_16_Pro_00028.png', 'WIN_20180925_17_20_16_Pro_00021.png', 'WIN_20180925_17_20_16_Pro_00017.png', 'WIN_20180925_17_20_16_Pro_00022.png', 'WIN_20180925_17_20_16_Pro_00013.png', 'WIN_20180925_17_20_16_Pro_00010.png', 'WIN_20180925_17_20_16_Pro_00016.png']\n",
            "IMGS... ['WIN_20180926_16_58_12_Pro_00030.png', 'WIN_20180926_16_58_12_Pro_00010.png', 'WIN_20180926_16_58_12_Pro_00014.png', 'WIN_20180926_16_58_12_Pro_00040.png', 'WIN_20180926_16_58_12_Pro_00018.png', 'WIN_20180926_16_58_12_Pro_00034.png', 'WIN_20180926_16_58_12_Pro_00058.png', 'WIN_20180926_16_58_12_Pro_00052.png', 'WIN_20180926_16_58_12_Pro_00032.png', 'WIN_20180926_16_58_12_Pro_00044.png', 'WIN_20180926_16_58_12_Pro_00056.png', 'WIN_20180926_16_58_12_Pro_00038.png', 'WIN_20180926_16_58_12_Pro_00020.png', 'WIN_20180926_16_58_12_Pro_00042.png', 'WIN_20180926_16_58_12_Pro_00026.png', 'WIN_20180926_16_58_12_Pro_00054.png', 'WIN_20180926_16_58_12_Pro_00012.png', 'WIN_20180926_16_58_12_Pro_00016.png', 'WIN_20180926_16_58_12_Pro_00036.png', 'WIN_20180926_16_58_12_Pro_00050.png', 'WIN_20180926_16_58_12_Pro_00008.png', 'WIN_20180926_16_58_12_Pro_00060.png', 'WIN_20180926_16_58_12_Pro_00022.png', 'WIN_20180926_16_58_12_Pro_00006.png', 'WIN_20180926_16_58_12_Pro_00028.png', 'WIN_20180926_16_58_12_Pro_00046.png', 'WIN_20180926_16_58_12_Pro_00048.png', 'WIN_20180926_16_58_12_Pro_00024.png', 'WIN_20180926_16_58_12_Pro_00064.png', 'WIN_20180926_16_58_12_Pro_00062.png']\n",
            "IMGS... ['WIN_20180926_17_33_51_Pro_00056.png', 'WIN_20180926_17_33_51_Pro_00054.png', 'WIN_20180926_17_33_51_Pro_00020.png', 'WIN_20180926_17_33_51_Pro_00002.png', 'WIN_20180926_17_33_51_Pro_00026.png', 'WIN_20180926_17_33_51_Pro_00006.png', 'WIN_20180926_17_33_51_Pro_00010.png', 'WIN_20180926_17_33_51_Pro_00060.png', 'WIN_20180926_17_33_51_Pro_00028.png', 'WIN_20180926_17_33_51_Pro_00016.png', 'WIN_20180926_17_33_51_Pro_00048.png', 'WIN_20180926_17_33_51_Pro_00038.png', 'WIN_20180926_17_33_51_Pro_00034.png', 'WIN_20180926_17_33_51_Pro_00024.png', 'WIN_20180926_17_33_51_Pro_00030.png', 'WIN_20180926_17_33_51_Pro_00004.png', 'WIN_20180926_17_33_51_Pro_00040.png', 'WIN_20180926_17_33_51_Pro_00022.png', 'WIN_20180926_17_33_51_Pro_00046.png', 'WIN_20180926_17_33_51_Pro_00012.png', 'WIN_20180926_17_33_51_Pro_00044.png', 'WIN_20180926_17_33_51_Pro_00052.png', 'WIN_20180926_17_33_51_Pro_00050.png', 'WIN_20180926_17_33_51_Pro_00036.png', 'WIN_20180926_17_33_51_Pro_00018.png', 'WIN_20180926_17_33_51_Pro_00042.png', 'WIN_20180926_17_33_51_Pro_00058.png', 'WIN_20180926_17_33_51_Pro_00008.png', 'WIN_20180926_17_33_51_Pro_00032.png', 'WIN_20180926_17_33_51_Pro_00014.png']\n",
            "IMGS... ['WIN_20180925_17_22_11_Pro_00038.png', 'WIN_20180925_17_22_11_Pro_00015.png', 'WIN_20180925_17_22_11_Pro_00036.png', 'WIN_20180925_17_22_11_Pro_00025.png', 'WIN_20180925_17_22_11_Pro_00019.png', 'WIN_20180925_17_22_11_Pro_00040.png', 'WIN_20180925_17_22_11_Pro_00013.png', 'WIN_20180925_17_22_11_Pro_00024.png', 'WIN_20180925_17_22_11_Pro_00027.png', 'WIN_20180925_17_22_11_Pro_00042.png', 'WIN_20180925_17_22_11_Pro_00032.png', 'WIN_20180925_17_22_11_Pro_00029.png', 'WIN_20180925_17_22_11_Pro_00039.png', 'WIN_20180925_17_22_11_Pro_00028.png', 'WIN_20180925_17_22_11_Pro_00034.png', 'WIN_20180925_17_22_11_Pro_00021.png', 'WIN_20180925_17_22_11_Pro_00020.png', 'WIN_20180925_17_22_11_Pro_00016.png', 'WIN_20180925_17_22_11_Pro_00014.png', 'WIN_20180925_17_22_11_Pro_00018.png', 'WIN_20180925_17_22_11_Pro_00023.png', 'WIN_20180925_17_22_11_Pro_00033.png', 'WIN_20180925_17_22_11_Pro_00022.png', 'WIN_20180925_17_22_11_Pro_00031.png', 'WIN_20180925_17_22_11_Pro_00030.png', 'WIN_20180925_17_22_11_Pro_00035.png', 'WIN_20180925_17_22_11_Pro_00041.png', 'WIN_20180925_17_22_11_Pro_00017.png', 'WIN_20180925_17_22_11_Pro_00026.png', 'WIN_20180925_17_22_11_Pro_00037.png']\n",
            "IMGS... ['WIN_20180926_16_49_15_Pro_00019.png', 'WIN_20180926_16_49_15_Pro_00011.png', 'WIN_20180926_16_49_15_Pro_00017.png', 'WIN_20180926_16_49_15_Pro_00007.png', 'WIN_20180926_16_49_15_Pro_00013.png', 'WIN_20180926_16_49_15_Pro_00009.png', 'WIN_20180926_16_49_15_Pro_00015.png', 'WIN_20180926_16_49_15_Pro_00005.png', 'WIN_20180926_16_49_15_Pro_00035.png', 'WIN_20180926_16_49_15_Pro_00045.png', 'WIN_20180926_16_49_15_Pro_00043.png', 'WIN_20180926_16_49_15_Pro_00029.png', 'WIN_20180926_16_49_15_Pro_00061.png', 'WIN_20180926_16_49_15_Pro_00039.png', 'WIN_20180926_16_49_15_Pro_00021.png', 'WIN_20180926_16_49_15_Pro_00033.png', 'WIN_20180926_16_49_15_Pro_00023.png', 'WIN_20180926_16_49_15_Pro_00049.png', 'WIN_20180926_16_49_15_Pro_00063.png', 'WIN_20180926_16_49_15_Pro_00031.png', 'WIN_20180926_16_49_15_Pro_00055.png', 'WIN_20180926_16_49_15_Pro_00037.png', 'WIN_20180926_16_49_15_Pro_00041.png', 'WIN_20180926_16_49_15_Pro_00053.png', 'WIN_20180926_16_49_15_Pro_00027.png', 'WIN_20180926_16_49_15_Pro_00059.png', 'WIN_20180926_16_49_15_Pro_00051.png', 'WIN_20180926_16_49_15_Pro_00047.png', 'WIN_20180926_16_49_15_Pro_00025.png', 'WIN_20180926_16_49_15_Pro_00057.png']\n",
            "IMGS... ['WIN_20180925_18_14_35_Pro_00024.png', 'WIN_20180925_18_14_35_Pro_00036.png', 'WIN_20180925_18_14_35_Pro_00017.png', 'WIN_20180925_18_14_35_Pro_00042.png', 'WIN_20180925_18_14_35_Pro_00016.png', 'WIN_20180925_18_14_35_Pro_00030.png', 'WIN_20180925_18_14_35_Pro_00018.png', 'WIN_20180925_18_14_35_Pro_00029.png', 'WIN_20180925_18_14_35_Pro_00026.png', 'WIN_20180925_18_14_35_Pro_00027.png', 'WIN_20180925_18_14_35_Pro_00039.png', 'WIN_20180925_18_14_35_Pro_00043.png', 'WIN_20180925_18_14_35_Pro_00028.png', 'WIN_20180925_18_14_35_Pro_00034.png', 'WIN_20180925_18_14_35_Pro_00044.png', 'WIN_20180925_18_14_35_Pro_00021.png', 'WIN_20180925_18_14_35_Pro_00015.png', 'WIN_20180925_18_14_35_Pro_00025.png', 'WIN_20180925_18_14_35_Pro_00032.png', 'WIN_20180925_18_14_35_Pro_00020.png', 'WIN_20180925_18_14_35_Pro_00037.png', 'WIN_20180925_18_14_35_Pro_00023.png', 'WIN_20180925_18_14_35_Pro_00033.png', 'WIN_20180925_18_14_35_Pro_00031.png', 'WIN_20180925_18_14_35_Pro_00035.png', 'WIN_20180925_18_14_35_Pro_00019.png', 'WIN_20180925_18_14_35_Pro_00038.png', 'WIN_20180925_18_14_35_Pro_00022.png', 'WIN_20180925_18_14_35_Pro_00041.png', 'WIN_20180925_18_14_35_Pro_00040.png']\n",
            "IMGS... ['WIN_20180925_18_01_30_Pro_00015.png', 'WIN_20180925_18_01_30_Pro_00016.png', 'WIN_20180925_18_01_30_Pro_00025.png', 'WIN_20180925_18_01_30_Pro_00018.png', 'WIN_20180925_18_01_30_Pro_00028.png', 'WIN_20180925_18_01_30_Pro_00042.png', 'WIN_20180925_18_01_30_Pro_00044.png', 'WIN_20180925_18_01_30_Pro_00032.png', 'WIN_20180925_18_01_30_Pro_00043.png', 'WIN_20180925_18_01_30_Pro_00038.png', 'WIN_20180925_18_01_30_Pro_00022.png', 'WIN_20180925_18_01_30_Pro_00027.png', 'WIN_20180925_18_01_30_Pro_00041.png', 'WIN_20180925_18_01_30_Pro_00017.png', 'WIN_20180925_18_01_30_Pro_00020.png', 'WIN_20180925_18_01_30_Pro_00039.png', 'WIN_20180925_18_01_30_Pro_00034.png', 'WIN_20180925_18_01_30_Pro_00031.png', 'WIN_20180925_18_01_30_Pro_00023.png', 'WIN_20180925_18_01_30_Pro_00035.png', 'WIN_20180925_18_01_30_Pro_00019.png', 'WIN_20180925_18_01_30_Pro_00026.png', 'WIN_20180925_18_01_30_Pro_00036.png', 'WIN_20180925_18_01_30_Pro_00040.png', 'WIN_20180925_18_01_30_Pro_00033.png', 'WIN_20180925_18_01_30_Pro_00024.png', 'WIN_20180925_18_01_30_Pro_00021.png', 'WIN_20180925_18_01_30_Pro_00037.png', 'WIN_20180925_18_01_30_Pro_00029.png', 'WIN_20180925_18_01_30_Pro_00030.png']\n",
            "IMGS... ['WIN_20180925_17_31_02_Pro_00021.png', 'WIN_20180925_17_31_02_Pro_00011.png', 'WIN_20180925_17_31_02_Pro_00015.png', 'WIN_20180925_17_31_02_Pro_00031.png', 'WIN_20180925_17_31_02_Pro_00025.png', 'WIN_20180925_17_31_02_Pro_00029.png', 'WIN_20180925_17_31_02_Pro_00009.png', 'WIN_20180925_17_31_02_Pro_00023.png', 'WIN_20180925_17_31_02_Pro_00007.png', 'WIN_20180925_17_31_02_Pro_00017.png', 'WIN_20180925_17_31_02_Pro_00033.png', 'WIN_20180925_17_31_02_Pro_00013.png', 'WIN_20180925_17_31_02_Pro_00019.png', 'WIN_20180925_17_31_02_Pro_00027.png', 'WIN_20180925_17_31_02_Pro_00045.png', 'WIN_20180925_17_31_02_Pro_00037.png', 'WIN_20180925_17_31_02_Pro_00061.png', 'WIN_20180925_17_31_02_Pro_00049.png', 'WIN_20180925_17_31_02_Pro_00057.png', 'WIN_20180925_17_31_02_Pro_00063.png', 'WIN_20180925_17_31_02_Pro_00053.png', 'WIN_20180925_17_31_02_Pro_00039.png', 'WIN_20180925_17_31_02_Pro_00059.png', 'WIN_20180925_17_31_02_Pro_00047.png', 'WIN_20180925_17_31_02_Pro_00035.png', 'WIN_20180925_17_31_02_Pro_00065.png', 'WIN_20180925_17_31_02_Pro_00051.png', 'WIN_20180925_17_31_02_Pro_00055.png', 'WIN_20180925_17_31_02_Pro_00041.png', 'WIN_20180925_17_31_02_Pro_00043.png']\n",
            "IMGS... ['WIN_20180907_16_05_08_Pro_00014.png', 'WIN_20180907_16_05_08_Pro_00020.png', 'WIN_20180907_16_05_08_Pro_00018.png', 'WIN_20180907_16_05_08_Pro_00030.png', 'WIN_20180907_16_05_08_Pro_00028.png', 'WIN_20180907_16_05_08_Pro_00016.png', 'WIN_20180907_16_05_08_Pro_00026.png', 'WIN_20180907_16_05_08_Pro_00022.png', 'WIN_20180907_16_05_08_Pro_00024.png', 'WIN_20180907_16_05_08_Pro_00032.png', 'WIN_20180907_16_05_08_Pro_00042.png', 'WIN_20180907_16_05_08_Pro_00038.png', 'WIN_20180907_16_05_08_Pro_00036.png', 'WIN_20180907_16_05_08_Pro_00034.png', 'WIN_20180907_16_05_08_Pro_00040.png', 'WIN_20180907_16_05_08_Pro_00044.png', 'WIN_20180907_16_05_08_Pro_00050.png', 'WIN_20180907_16_05_08_Pro_00046.png', 'WIN_20180907_16_05_08_Pro_00048.png', 'WIN_20180907_16_05_08_Pro_00052.png', 'WIN_20180907_16_05_08_Pro_00072.png', 'WIN_20180907_16_05_08_Pro_00060.png', 'WIN_20180907_16_05_08_Pro_00064.png', 'WIN_20180907_16_05_08_Pro_00068.png', 'WIN_20180907_16_05_08_Pro_00058.png', 'WIN_20180907_16_05_08_Pro_00070.png', 'WIN_20180907_16_05_08_Pro_00066.png', 'WIN_20180907_16_05_08_Pro_00062.png', 'WIN_20180907_16_05_08_Pro_00054.png', 'WIN_20180907_16_05_08_Pro_00056.png']\n",
            "IMGS... ['WIN_20180925_17_57_22_Pro_00015.png', 'WIN_20180925_17_57_22_Pro_00011.png', 'WIN_20180925_17_57_22_Pro_00013.png', 'WIN_20180925_17_57_22_Pro_00017.png', 'WIN_20180925_17_57_22_Pro_00047.png', 'WIN_20180925_17_57_22_Pro_00039.png', 'WIN_20180925_17_57_22_Pro_00059.png', 'WIN_20180925_17_57_22_Pro_00027.png', 'WIN_20180925_17_57_22_Pro_00055.png', 'WIN_20180925_17_57_22_Pro_00051.png', 'WIN_20180925_17_57_22_Pro_00061.png', 'WIN_20180925_17_57_22_Pro_00037.png', 'WIN_20180925_17_57_22_Pro_00053.png', 'WIN_20180925_17_57_22_Pro_00045.png', 'WIN_20180925_17_57_22_Pro_00031.png', 'WIN_20180925_17_57_22_Pro_00033.png', 'WIN_20180925_17_57_22_Pro_00049.png', 'WIN_20180925_17_57_22_Pro_00063.png', 'WIN_20180925_17_57_22_Pro_00029.png', 'WIN_20180925_17_57_22_Pro_00041.png', 'WIN_20180925_17_57_22_Pro_00065.png', 'WIN_20180925_17_57_22_Pro_00035.png', 'WIN_20180925_17_57_22_Pro_00023.png', 'WIN_20180925_17_57_22_Pro_00025.png', 'WIN_20180925_17_57_22_Pro_00019.png', 'WIN_20180925_17_57_22_Pro_00057.png', 'WIN_20180925_17_57_22_Pro_00067.png', 'WIN_20180925_17_57_22_Pro_00069.png', 'WIN_20180925_17_57_22_Pro_00043.png', 'WIN_20180925_17_57_22_Pro_00021.png']\n",
            "IMGS... ['WIN_20180926_16_58_27_Pro_00045.png', 'WIN_20180926_16_58_27_Pro_00061.png', 'WIN_20180926_16_58_27_Pro_00035.png', 'WIN_20180926_16_58_27_Pro_00011.png', 'WIN_20180926_16_58_27_Pro_00025.png', 'WIN_20180926_16_58_27_Pro_00031.png', 'WIN_20180926_16_58_27_Pro_00027.png', 'WIN_20180926_16_58_27_Pro_00021.png', 'WIN_20180926_16_58_27_Pro_00059.png', 'WIN_20180926_16_58_27_Pro_00055.png', 'WIN_20180926_16_58_27_Pro_00007.png', 'WIN_20180926_16_58_27_Pro_00017.png', 'WIN_20180926_16_58_27_Pro_00043.png', 'WIN_20180926_16_58_27_Pro_00013.png', 'WIN_20180926_16_58_27_Pro_00037.png', 'WIN_20180926_16_58_27_Pro_00041.png', 'WIN_20180926_16_58_27_Pro_00039.png', 'WIN_20180926_16_58_27_Pro_00033.png', 'WIN_20180926_16_58_27_Pro_00023.png', 'WIN_20180926_16_58_27_Pro_00005.png', 'WIN_20180926_16_58_27_Pro_00053.png', 'WIN_20180926_16_58_27_Pro_00019.png', 'WIN_20180926_16_58_27_Pro_00057.png', 'WIN_20180926_16_58_27_Pro_00015.png', 'WIN_20180926_16_58_27_Pro_00047.png', 'WIN_20180926_16_58_27_Pro_00003.png', 'WIN_20180926_16_58_27_Pro_00009.png', 'WIN_20180926_16_58_27_Pro_00049.png', 'WIN_20180926_16_58_27_Pro_00051.png', 'WIN_20180926_16_58_27_Pro_00029.png']\n",
            "IMGS... ['WIN_20180926_16_52_43_Pro_00031.png', 'WIN_20180926_16_52_43_Pro_00017.png', 'WIN_20180926_16_52_43_Pro_00012.png', 'WIN_20180926_16_52_43_Pro_00014.png', 'WIN_20180926_16_52_43_Pro_00024.png', 'WIN_20180926_16_52_43_Pro_00018.png', 'WIN_20180926_16_52_43_Pro_00040.png', 'WIN_20180926_16_52_43_Pro_00025.png', 'WIN_20180926_16_52_43_Pro_00016.png', 'WIN_20180926_16_52_43_Pro_00038.png', 'WIN_20180926_16_52_43_Pro_00034.png', 'WIN_20180926_16_52_43_Pro_00019.png', 'WIN_20180926_16_52_43_Pro_00028.png', 'WIN_20180926_16_52_43_Pro_00037.png', 'WIN_20180926_16_52_43_Pro_00021.png', 'WIN_20180926_16_52_43_Pro_00032.png', 'WIN_20180926_16_52_43_Pro_00029.png', 'WIN_20180926_16_52_43_Pro_00022.png', 'WIN_20180926_16_52_43_Pro_00015.png', 'WIN_20180926_16_52_43_Pro_00033.png', 'WIN_20180926_16_52_43_Pro_00013.png', 'WIN_20180926_16_52_43_Pro_00020.png', 'WIN_20180926_16_52_43_Pro_00011.png', 'WIN_20180926_16_52_43_Pro_00030.png', 'WIN_20180926_16_52_43_Pro_00039.png', 'WIN_20180926_16_52_43_Pro_00035.png', 'WIN_20180926_16_52_43_Pro_00036.png', 'WIN_20180926_16_52_43_Pro_00023.png', 'WIN_20180926_16_52_43_Pro_00026.png', 'WIN_20180926_16_52_43_Pro_00027.png']\n",
            "IMGS... ['WIN_20180907_15_59_44_Pro_00027.png', 'WIN_20180907_15_59_44_Pro_00021.png', 'WIN_20180907_15_59_44_Pro_00023.png', 'WIN_20180907_15_59_44_Pro_00017.png', 'WIN_20180907_15_59_44_Pro_00029.png', 'WIN_20180907_15_59_44_Pro_00015.png', 'WIN_20180907_15_59_44_Pro_00025.png', 'WIN_20180907_15_59_44_Pro_00013.png', 'WIN_20180907_15_59_44_Pro_00019.png', 'WIN_20180907_15_59_44_Pro_00039.png', 'WIN_20180907_15_59_44_Pro_00041.png', 'WIN_20180907_15_59_44_Pro_00035.png', 'WIN_20180907_15_59_44_Pro_00031.png', 'WIN_20180907_15_59_44_Pro_00045.png', 'WIN_20180907_15_59_44_Pro_00043.png', 'WIN_20180907_15_59_44_Pro_00033.png', 'WIN_20180907_15_59_44_Pro_00037.png', 'WIN_20180907_15_59_44_Pro_00065.png', 'WIN_20180907_15_59_44_Pro_00071.png', 'WIN_20180907_15_59_44_Pro_00057.png', 'WIN_20180907_15_59_44_Pro_00051.png', 'WIN_20180907_15_59_44_Pro_00069.png', 'WIN_20180907_15_59_44_Pro_00055.png', 'WIN_20180907_15_59_44_Pro_00067.png', 'WIN_20180907_15_59_44_Pro_00053.png', 'WIN_20180907_15_59_44_Pro_00063.png', 'WIN_20180907_15_59_44_Pro_00059.png', 'WIN_20180907_15_59_44_Pro_00047.png', 'WIN_20180907_15_59_44_Pro_00061.png', 'WIN_20180907_15_59_44_Pro_00049.png']\n",
            "IMGS... ['WIN_20180925_17_36_07_Pro_00014.png', 'WIN_20180925_17_36_07_Pro_00054.png', 'WIN_20180925_17_36_07_Pro_00024.png', 'WIN_20180925_17_36_07_Pro_00034.png', 'WIN_20180925_17_36_07_Pro_00058.png', 'WIN_20180925_17_36_07_Pro_00022.png', 'WIN_20180925_17_36_07_Pro_00046.png', 'WIN_20180925_17_36_07_Pro_00032.png', 'WIN_20180925_17_36_07_Pro_00044.png', 'WIN_20180925_17_36_07_Pro_00060.png', 'WIN_20180925_17_36_07_Pro_00020.png', 'WIN_20180925_17_36_07_Pro_00012.png', 'WIN_20180925_17_36_07_Pro_00008.png', 'WIN_20180925_17_36_07_Pro_00006.png', 'WIN_20180925_17_36_07_Pro_00010.png', 'WIN_20180925_17_36_07_Pro_00016.png', 'WIN_20180925_17_36_07_Pro_00030.png', 'WIN_20180925_17_36_07_Pro_00062.png', 'WIN_20180925_17_36_07_Pro_00048.png', 'WIN_20180925_17_36_07_Pro_00056.png', 'WIN_20180925_17_36_07_Pro_00026.png', 'WIN_20180925_17_36_07_Pro_00018.png', 'WIN_20180925_17_36_07_Pro_00038.png', 'WIN_20180925_17_36_07_Pro_00036.png', 'WIN_20180925_17_36_07_Pro_00042.png', 'WIN_20180925_17_36_07_Pro_00028.png', 'WIN_20180925_17_36_07_Pro_00040.png', 'WIN_20180925_17_36_07_Pro_00052.png', 'WIN_20180925_17_36_07_Pro_00004.png', 'WIN_20180925_17_36_07_Pro_00050.png']\n",
            "IMGS... ['WIN_20180926_17_24_33_Pro_00014.png', 'WIN_20180926_17_24_33_Pro_00033.png', 'WIN_20180926_17_24_33_Pro_00031.png', 'WIN_20180926_17_24_33_Pro_00016.png', 'WIN_20180926_17_24_33_Pro_00038.png', 'WIN_20180926_17_24_33_Pro_00040.png', 'WIN_20180926_17_24_33_Pro_00043.png', 'WIN_20180926_17_24_33_Pro_00025.png', 'WIN_20180926_17_24_33_Pro_00019.png', 'WIN_20180926_17_24_33_Pro_00027.png', 'WIN_20180926_17_24_33_Pro_00026.png', 'WIN_20180926_17_24_33_Pro_00023.png', 'WIN_20180926_17_24_33_Pro_00021.png', 'WIN_20180926_17_24_33_Pro_00039.png', 'WIN_20180926_17_24_33_Pro_00029.png', 'WIN_20180926_17_24_33_Pro_00022.png', 'WIN_20180926_17_24_33_Pro_00030.png', 'WIN_20180926_17_24_33_Pro_00032.png', 'WIN_20180926_17_24_33_Pro_00035.png', 'WIN_20180926_17_24_33_Pro_00034.png', 'WIN_20180926_17_24_33_Pro_00036.png', 'WIN_20180926_17_24_33_Pro_00015.png', 'WIN_20180926_17_24_33_Pro_00018.png', 'WIN_20180926_17_24_33_Pro_00028.png', 'WIN_20180926_17_24_33_Pro_00024.png', 'WIN_20180926_17_24_33_Pro_00017.png', 'WIN_20180926_17_24_33_Pro_00037.png', 'WIN_20180926_17_24_33_Pro_00042.png', 'WIN_20180926_17_24_33_Pro_00020.png', 'WIN_20180926_17_24_33_Pro_00041.png']\n",
            "IMGS... ['WIN_20180907_15_48_01_Pro_00052.png', 'WIN_20180907_15_48_01_Pro_00028.png', 'WIN_20180907_15_48_01_Pro_00040.png', 'WIN_20180907_15_48_01_Pro_00034.png', 'WIN_20180907_15_48_01_Pro_00026.png', 'WIN_20180907_15_48_01_Pro_00046.png', 'WIN_20180907_15_48_01_Pro_00010.png', 'WIN_20180907_15_48_01_Pro_00050.png', 'WIN_20180907_15_48_01_Pro_00008.png', 'WIN_20180907_15_48_01_Pro_00012.png', 'WIN_20180907_15_48_01_Pro_00016.png', 'WIN_20180907_15_48_01_Pro_00030.png', 'WIN_20180907_15_48_01_Pro_00054.png', 'WIN_20180907_15_48_01_Pro_00024.png', 'WIN_20180907_15_48_01_Pro_00014.png', 'WIN_20180907_15_48_01_Pro_00036.png', 'WIN_20180907_15_48_01_Pro_00022.png', 'WIN_20180907_15_48_01_Pro_00042.png', 'WIN_20180907_15_48_01_Pro_00018.png', 'WIN_20180907_15_48_01_Pro_00038.png', 'WIN_20180907_15_48_01_Pro_00048.png', 'WIN_20180907_15_48_01_Pro_00032.png', 'WIN_20180907_15_48_01_Pro_00020.png', 'WIN_20180907_15_48_01_Pro_00044.png', 'WIN_20180907_15_48_01_Pro_00066.png', 'WIN_20180907_15_48_01_Pro_00060.png', 'WIN_20180907_15_48_01_Pro_00062.png', 'WIN_20180907_15_48_01_Pro_00058.png', 'WIN_20180907_15_48_01_Pro_00064.png', 'WIN_20180907_15_48_01_Pro_00056.png']\n",
            "IMGS... ['WIN_20180925_17_39_11_Pro_00013.png', 'WIN_20180925_17_39_11_Pro_00019.png', 'WIN_20180925_17_39_11_Pro_00022.png', 'WIN_20180925_17_39_11_Pro_00026.png', 'WIN_20180925_17_39_11_Pro_00015.png', 'WIN_20180925_17_39_11_Pro_00023.png', 'WIN_20180925_17_39_11_Pro_00016.png', 'WIN_20180925_17_39_11_Pro_00008.png', 'WIN_20180925_17_39_11_Pro_00024.png', 'WIN_20180925_17_39_11_Pro_00012.png', 'WIN_20180925_17_39_11_Pro_00036.png', 'WIN_20180925_17_39_11_Pro_00027.png', 'WIN_20180925_17_39_11_Pro_00021.png', 'WIN_20180925_17_39_11_Pro_00028.png', 'WIN_20180925_17_39_11_Pro_00034.png', 'WIN_20180925_17_39_11_Pro_00009.png', 'WIN_20180925_17_39_11_Pro_00011.png', 'WIN_20180925_17_39_11_Pro_00014.png', 'WIN_20180925_17_39_11_Pro_00030.png', 'WIN_20180925_17_39_11_Pro_00020.png', 'WIN_20180925_17_39_11_Pro_00033.png', 'WIN_20180925_17_39_11_Pro_00032.png', 'WIN_20180925_17_39_11_Pro_00035.png', 'WIN_20180925_17_39_11_Pro_00017.png', 'WIN_20180925_17_39_11_Pro_00037.png', 'WIN_20180925_17_39_11_Pro_00018.png', 'WIN_20180925_17_39_11_Pro_00029.png', 'WIN_20180925_17_39_11_Pro_00031.png', 'WIN_20180925_17_39_11_Pro_00010.png', 'WIN_20180925_17_39_11_Pro_00025.png']\n",
            "IMGS... ['WIN_20180926_17_06_30_Pro_00021.png', 'WIN_20180926_17_06_30_Pro_00003.png', 'WIN_20180926_17_06_30_Pro_00025.png', 'WIN_20180926_17_06_30_Pro_00045.png', 'WIN_20180926_17_06_30_Pro_00053.png', 'WIN_20180926_17_06_30_Pro_00039.png', 'WIN_20180926_17_06_30_Pro_00015.png', 'WIN_20180926_17_06_30_Pro_00051.png', 'WIN_20180926_17_06_30_Pro_00043.png', 'WIN_20180926_17_06_30_Pro_00019.png', 'WIN_20180926_17_06_30_Pro_00033.png', 'WIN_20180926_17_06_30_Pro_00023.png', 'WIN_20180926_17_06_30_Pro_00041.png', 'WIN_20180926_17_06_30_Pro_00005.png', 'WIN_20180926_17_06_30_Pro_00031.png', 'WIN_20180926_17_06_30_Pro_00035.png', 'WIN_20180926_17_06_30_Pro_00037.png', 'WIN_20180926_17_06_30_Pro_00013.png', 'WIN_20180926_17_06_30_Pro_00047.png', 'WIN_20180926_17_06_30_Pro_00049.png', 'WIN_20180926_17_06_30_Pro_00027.png', 'WIN_20180926_17_06_30_Pro_00011.png', 'WIN_20180926_17_06_30_Pro_00007.png', 'WIN_20180926_17_06_30_Pro_00009.png', 'WIN_20180926_17_06_30_Pro_00029.png', 'WIN_20180926_17_06_30_Pro_00017.png', 'WIN_20180926_17_06_30_Pro_00061.png', 'WIN_20180926_17_06_30_Pro_00055.png', 'WIN_20180926_17_06_30_Pro_00059.png', 'WIN_20180926_17_06_30_Pro_00057.png']\n",
            "IMGS... ['WIN_20180926_17_21_24_Pro_00050.png', 'WIN_20180926_17_21_24_Pro_00040.png', 'WIN_20180926_17_21_24_Pro_00058.png', 'WIN_20180926_17_21_24_Pro_00006.png', 'WIN_20180926_17_21_24_Pro_00002.png', 'WIN_20180926_17_21_24_Pro_00022.png', 'WIN_20180926_17_21_24_Pro_00048.png', 'WIN_20180926_17_21_24_Pro_00034.png', 'WIN_20180926_17_21_24_Pro_00024.png', 'WIN_20180926_17_21_24_Pro_00018.png', 'WIN_20180926_17_21_24_Pro_00008.png', 'WIN_20180926_17_21_24_Pro_00016.png', 'WIN_20180926_17_21_24_Pro_00020.png', 'WIN_20180926_17_21_24_Pro_00026.png', 'WIN_20180926_17_21_24_Pro_00032.png', 'WIN_20180926_17_21_24_Pro_00010.png', 'WIN_20180926_17_21_24_Pro_00030.png', 'WIN_20180926_17_21_24_Pro_00046.png', 'WIN_20180926_17_21_24_Pro_00052.png', 'WIN_20180926_17_21_24_Pro_00012.png', 'WIN_20180926_17_21_24_Pro_00054.png', 'WIN_20180926_17_21_24_Pro_00044.png', 'WIN_20180926_17_21_24_Pro_00056.png', 'WIN_20180926_17_21_24_Pro_00036.png', 'WIN_20180926_17_21_24_Pro_00004.png', 'WIN_20180926_17_21_24_Pro_00014.png', 'WIN_20180926_17_21_24_Pro_00042.png', 'WIN_20180926_17_21_24_Pro_00038.png', 'WIN_20180926_17_21_24_Pro_00028.png', 'WIN_20180926_17_21_24_Pro_00060.png']\n",
            "IMGS... ['WIN_20180907_16_30_24_Pro_00026.png', 'WIN_20180907_16_30_24_Pro_00018.png', 'WIN_20180907_16_30_24_Pro_00020.png', 'WIN_20180907_16_30_24_Pro_00012.png', 'WIN_20180907_16_30_24_Pro_00024.png', 'WIN_20180907_16_30_24_Pro_00022.png', 'WIN_20180907_16_30_24_Pro_00016.png', 'WIN_20180907_16_30_24_Pro_00014.png', 'WIN_20180907_16_30_24_Pro_00028.png', 'WIN_20180907_16_30_24_Pro_00032.png', 'WIN_20180907_16_30_24_Pro_00034.png', 'WIN_20180907_16_30_24_Pro_00030.png', 'WIN_20180907_16_30_24_Pro_00036.png', 'WIN_20180907_16_30_24_Pro_00038.png', 'WIN_20180907_16_30_24_Pro_00040.png', 'WIN_20180907_16_30_24_Pro_00052.png', 'WIN_20180907_16_30_24_Pro_00044.png', 'WIN_20180907_16_30_24_Pro_00066.png', 'WIN_20180907_16_30_24_Pro_00046.png', 'WIN_20180907_16_30_24_Pro_00062.png', 'WIN_20180907_16_30_24_Pro_00050.png', 'WIN_20180907_16_30_24_Pro_00048.png', 'WIN_20180907_16_30_24_Pro_00060.png', 'WIN_20180907_16_30_24_Pro_00054.png', 'WIN_20180907_16_30_24_Pro_00058.png', 'WIN_20180907_16_30_24_Pro_00070.png', 'WIN_20180907_16_30_24_Pro_00042.png', 'WIN_20180907_16_30_24_Pro_00064.png', 'WIN_20180907_16_30_24_Pro_00056.png', 'WIN_20180907_16_30_24_Pro_00068.png']\n",
            "IMGS... ['WIN_20180926_17_26_41_Pro_00039.png', 'WIN_20180926_17_26_41_Pro_00043.png', 'WIN_20180926_17_26_41_Pro_00021.png', 'WIN_20180926_17_26_41_Pro_00013.png', 'WIN_20180926_17_26_41_Pro_00049.png', 'WIN_20180926_17_26_41_Pro_00025.png', 'WIN_20180926_17_26_41_Pro_00011.png', 'WIN_20180926_17_26_41_Pro_00053.png', 'WIN_20180926_17_26_41_Pro_00061.png', 'WIN_20180926_17_26_41_Pro_00029.png', 'WIN_20180926_17_26_41_Pro_00051.png', 'WIN_20180926_17_26_41_Pro_00047.png', 'WIN_20180926_17_26_41_Pro_00033.png', 'WIN_20180926_17_26_41_Pro_00035.png', 'WIN_20180926_17_26_41_Pro_00045.png', 'WIN_20180926_17_26_41_Pro_00041.png', 'WIN_20180926_17_26_41_Pro_00023.png', 'WIN_20180926_17_26_41_Pro_00031.png', 'WIN_20180926_17_26_41_Pro_00009.png', 'WIN_20180926_17_26_41_Pro_00017.png', 'WIN_20180926_17_26_41_Pro_00015.png', 'WIN_20180926_17_26_41_Pro_00037.png', 'WIN_20180926_17_26_41_Pro_00027.png', 'WIN_20180926_17_26_41_Pro_00019.png', 'WIN_20180926_17_26_41_Pro_00057.png', 'WIN_20180926_17_26_41_Pro_00055.png', 'WIN_20180926_17_26_41_Pro_00059.png', 'WIN_20180926_17_26_41_Pro_00067.png', 'WIN_20180926_17_26_41_Pro_00065.png', 'WIN_20180926_17_26_41_Pro_00063.png']\n",
            "IMGS... ['WIN_20180926_17_39_18_Pro_00040.png', 'WIN_20180926_17_39_18_Pro_00004.png', 'WIN_20180926_17_39_18_Pro_00010.png', 'WIN_20180926_17_39_18_Pro_00028.png', 'WIN_20180926_17_39_18_Pro_00006.png', 'WIN_20180926_17_39_18_Pro_00022.png', 'WIN_20180926_17_39_18_Pro_00002.png', 'WIN_20180926_17_39_18_Pro_00014.png', 'WIN_20180926_17_39_18_Pro_00026.png', 'WIN_20180926_17_39_18_Pro_00016.png', 'WIN_20180926_17_39_18_Pro_00008.png', 'WIN_20180926_17_39_18_Pro_00034.png', 'WIN_20180926_17_39_18_Pro_00030.png', 'WIN_20180926_17_39_18_Pro_00036.png', 'WIN_20180926_17_39_18_Pro_00012.png', 'WIN_20180926_17_39_18_Pro_00038.png', 'WIN_20180926_17_39_18_Pro_00018.png', 'WIN_20180926_17_39_18_Pro_00032.png', 'WIN_20180926_17_39_18_Pro_00020.png', 'WIN_20180926_17_39_18_Pro_00024.png', 'WIN_20180926_17_39_18_Pro_00056.png', 'WIN_20180926_17_39_18_Pro_00058.png', 'WIN_20180926_17_39_18_Pro_00060.png', 'WIN_20180926_17_39_18_Pro_00052.png', 'WIN_20180926_17_39_18_Pro_00048.png', 'WIN_20180926_17_39_18_Pro_00054.png', 'WIN_20180926_17_39_18_Pro_00046.png', 'WIN_20180926_17_39_18_Pro_00050.png', 'WIN_20180926_17_39_18_Pro_00044.png', 'WIN_20180926_17_39_18_Pro_00042.png']\n",
            "IMGS... ['WIN_20180926_17_49_29_Pro_00023.png', 'WIN_20180926_17_49_29_Pro_00016.png', 'WIN_20180926_17_49_29_Pro_00017.png', 'WIN_20180926_17_49_29_Pro_00020.png', 'WIN_20180926_17_49_29_Pro_00022.png', 'WIN_20180926_17_49_29_Pro_00021.png', 'WIN_20180926_17_49_29_Pro_00015.png', 'WIN_20180926_17_49_29_Pro_00018.png', 'WIN_20180926_17_49_29_Pro_00014.png', 'WIN_20180926_17_49_29_Pro_00019.png', 'WIN_20180926_17_49_29_Pro_00032.png', 'WIN_20180926_17_49_29_Pro_00038.png', 'WIN_20180926_17_49_29_Pro_00025.png', 'WIN_20180926_17_49_29_Pro_00036.png', 'WIN_20180926_17_49_29_Pro_00027.png', 'WIN_20180926_17_49_29_Pro_00035.png', 'WIN_20180926_17_49_29_Pro_00026.png', 'WIN_20180926_17_49_29_Pro_00042.png', 'WIN_20180926_17_49_29_Pro_00024.png', 'WIN_20180926_17_49_29_Pro_00041.png', 'WIN_20180926_17_49_29_Pro_00043.png', 'WIN_20180926_17_49_29_Pro_00040.png', 'WIN_20180926_17_49_29_Pro_00039.png', 'WIN_20180926_17_49_29_Pro_00034.png', 'WIN_20180926_17_49_29_Pro_00030.png', 'WIN_20180926_17_49_29_Pro_00031.png', 'WIN_20180926_17_49_29_Pro_00028.png', 'WIN_20180926_17_49_29_Pro_00037.png', 'WIN_20180926_17_49_29_Pro_00033.png', 'WIN_20180926_17_49_29_Pro_00029.png']\n",
            "IMGS... ['WIN_20180925_17_20_53_Pro_00037.png', 'WIN_20180925_17_20_53_Pro_00031.png', 'WIN_20180925_17_20_53_Pro_00019.png', 'WIN_20180925_17_20_53_Pro_00028.png', 'WIN_20180925_17_20_53_Pro_00038.png', 'WIN_20180925_17_20_53_Pro_00043.png', 'WIN_20180925_17_20_53_Pro_00025.png', 'WIN_20180925_17_20_53_Pro_00027.png', 'WIN_20180925_17_20_53_Pro_00032.png', 'WIN_20180925_17_20_53_Pro_00026.png', 'WIN_20180925_17_20_53_Pro_00039.png', 'WIN_20180925_17_20_53_Pro_00023.png', 'WIN_20180925_17_20_53_Pro_00041.png', 'WIN_20180925_17_20_53_Pro_00035.png', 'WIN_20180925_17_20_53_Pro_00029.png', 'WIN_20180925_17_20_53_Pro_00033.png', 'WIN_20180925_17_20_53_Pro_00018.png', 'WIN_20180925_17_20_53_Pro_00016.png', 'WIN_20180925_17_20_53_Pro_00034.png', 'WIN_20180925_17_20_53_Pro_00036.png', 'WIN_20180925_17_20_53_Pro_00022.png', 'WIN_20180925_17_20_53_Pro_00044.png', 'WIN_20180925_17_20_53_Pro_00021.png', 'WIN_20180925_17_20_53_Pro_00042.png', 'WIN_20180925_17_20_53_Pro_00040.png', 'WIN_20180925_17_20_53_Pro_00030.png', 'WIN_20180925_17_20_53_Pro_00024.png', 'WIN_20180925_17_20_53_Pro_00015.png', 'WIN_20180925_17_20_53_Pro_00017.png', 'WIN_20180925_17_20_53_Pro_00020.png']\n",
            "IMGS... ['WIN_20180925_17_42_57_Pro_00034.png', 'WIN_20180925_17_42_57_Pro_00037.png', 'WIN_20180925_17_42_57_Pro_00017.png', 'WIN_20180925_17_42_57_Pro_00014.png', 'WIN_20180925_17_42_57_Pro_00020.png', 'WIN_20180925_17_42_57_Pro_00011.png', 'WIN_20180925_17_42_57_Pro_00024.png', 'WIN_20180925_17_42_57_Pro_00038.png', 'WIN_20180925_17_42_57_Pro_00028.png', 'WIN_20180925_17_42_57_Pro_00012.png', 'WIN_20180925_17_42_57_Pro_00022.png', 'WIN_20180925_17_42_57_Pro_00016.png', 'WIN_20180925_17_42_57_Pro_00013.png', 'WIN_20180925_17_42_57_Pro_00040.png', 'WIN_20180925_17_42_57_Pro_00025.png', 'WIN_20180925_17_42_57_Pro_00015.png', 'WIN_20180925_17_42_57_Pro_00027.png', 'WIN_20180925_17_42_57_Pro_00021.png', 'WIN_20180925_17_42_57_Pro_00026.png', 'WIN_20180925_17_42_57_Pro_00029.png', 'WIN_20180925_17_42_57_Pro_00035.png', 'WIN_20180925_17_42_57_Pro_00033.png', 'WIN_20180925_17_42_57_Pro_00036.png', 'WIN_20180925_17_42_57_Pro_00030.png', 'WIN_20180925_17_42_57_Pro_00023.png', 'WIN_20180925_17_42_57_Pro_00019.png', 'WIN_20180925_17_42_57_Pro_00031.png', 'WIN_20180925_17_42_57_Pro_00018.png', 'WIN_20180925_17_42_57_Pro_00032.png', 'WIN_20180925_17_42_57_Pro_00039.png']\n",
            "IMGS... ['WIN_20180907_15_44_13_Pro_00012.png', 'WIN_20180907_15_44_13_Pro_00008.png', 'WIN_20180907_15_44_13_Pro_00010.png', 'WIN_20180907_15_44_13_Pro_00060.png', 'WIN_20180907_15_44_13_Pro_00014.png', 'WIN_20180907_15_44_13_Pro_00028.png', 'WIN_20180907_15_44_13_Pro_00022.png', 'WIN_20180907_15_44_13_Pro_00036.png', 'WIN_20180907_15_44_13_Pro_00018.png', 'WIN_20180907_15_44_13_Pro_00056.png', 'WIN_20180907_15_44_13_Pro_00050.png', 'WIN_20180907_15_44_13_Pro_00064.png', 'WIN_20180907_15_44_13_Pro_00020.png', 'WIN_20180907_15_44_13_Pro_00058.png', 'WIN_20180907_15_44_13_Pro_00034.png', 'WIN_20180907_15_44_13_Pro_00026.png', 'WIN_20180907_15_44_13_Pro_00038.png', 'WIN_20180907_15_44_13_Pro_00046.png', 'WIN_20180907_15_44_13_Pro_00032.png', 'WIN_20180907_15_44_13_Pro_00040.png', 'WIN_20180907_15_44_13_Pro_00044.png', 'WIN_20180907_15_44_13_Pro_00054.png', 'WIN_20180907_15_44_13_Pro_00052.png', 'WIN_20180907_15_44_13_Pro_00016.png', 'WIN_20180907_15_44_13_Pro_00062.png', 'WIN_20180907_15_44_13_Pro_00066.png', 'WIN_20180907_15_44_13_Pro_00042.png', 'WIN_20180907_15_44_13_Pro_00024.png', 'WIN_20180907_15_44_13_Pro_00048.png', 'WIN_20180907_15_44_13_Pro_00030.png']\n",
            "IMGS... ['WIN_20180926_16_42_06_Pro_00025.png', 'WIN_20180926_16_42_06_Pro_00014.png', 'WIN_20180926_16_42_06_Pro_00015.png', 'WIN_20180926_16_42_06_Pro_00013.png', 'WIN_20180926_16_42_06_Pro_00023.png', 'WIN_20180926_16_42_06_Pro_00019.png', 'WIN_20180926_16_42_06_Pro_00016.png', 'WIN_20180926_16_42_06_Pro_00021.png', 'WIN_20180926_16_42_06_Pro_00024.png', 'WIN_20180926_16_42_06_Pro_00018.png', 'WIN_20180926_16_42_06_Pro_00017.png', 'WIN_20180926_16_42_06_Pro_00026.png', 'WIN_20180926_16_42_06_Pro_00020.png', 'WIN_20180926_16_42_06_Pro_00012.png', 'WIN_20180926_16_42_06_Pro_00022.png', 'WIN_20180926_16_42_06_Pro_00029.png', 'WIN_20180926_16_42_06_Pro_00037.png', 'WIN_20180926_16_42_06_Pro_00030.png', 'WIN_20180926_16_42_06_Pro_00038.png', 'WIN_20180926_16_42_06_Pro_00036.png', 'WIN_20180926_16_42_06_Pro_00039.png', 'WIN_20180926_16_42_06_Pro_00035.png', 'WIN_20180926_16_42_06_Pro_00027.png', 'WIN_20180926_16_42_06_Pro_00031.png', 'WIN_20180926_16_42_06_Pro_00028.png', 'WIN_20180926_16_42_06_Pro_00033.png', 'WIN_20180926_16_42_06_Pro_00040.png', 'WIN_20180926_16_42_06_Pro_00041.png', 'WIN_20180926_16_42_06_Pro_00034.png', 'WIN_20180926_16_42_06_Pro_00032.png']\n",
            "IMGS... ['WIN_20180926_17_27_48_Pro_00048.png', 'WIN_20180926_17_27_48_Pro_00020.png', 'WIN_20180926_17_27_48_Pro_00016.png', 'WIN_20180926_17_27_48_Pro_00008.png', 'WIN_20180926_17_27_48_Pro_00030.png', 'WIN_20180926_17_27_48_Pro_00058.png', 'WIN_20180926_17_27_48_Pro_00050.png', 'WIN_20180926_17_27_48_Pro_00042.png', 'WIN_20180926_17_27_48_Pro_00018.png', 'WIN_20180926_17_27_48_Pro_00038.png', 'WIN_20180926_17_27_48_Pro_00036.png', 'WIN_20180926_17_27_48_Pro_00006.png', 'WIN_20180926_17_27_48_Pro_00028.png', 'WIN_20180926_17_27_48_Pro_00022.png', 'WIN_20180926_17_27_48_Pro_00046.png', 'WIN_20180926_17_27_48_Pro_00024.png', 'WIN_20180926_17_27_48_Pro_00026.png', 'WIN_20180926_17_27_48_Pro_00014.png', 'WIN_20180926_17_27_48_Pro_00040.png', 'WIN_20180926_17_27_48_Pro_00044.png', 'WIN_20180926_17_27_48_Pro_00034.png', 'WIN_20180926_17_27_48_Pro_00054.png', 'WIN_20180926_17_27_48_Pro_00052.png', 'WIN_20180926_17_27_48_Pro_00010.png', 'WIN_20180926_17_27_48_Pro_00012.png', 'WIN_20180926_17_27_48_Pro_00056.png', 'WIN_20180926_17_27_48_Pro_00032.png', 'WIN_20180926_17_27_48_Pro_00004.png', 'WIN_20180926_17_27_48_Pro_00062.png', 'WIN_20180926_17_27_48_Pro_00060.png']\n",
            "IMGS... ['WIN_20180907_16_42_55_Pro_00026.png', 'WIN_20180907_16_42_55_Pro_00008.png', 'WIN_20180907_16_42_55_Pro_00024.png', 'WIN_20180907_16_42_55_Pro_00018.png', 'WIN_20180907_16_42_55_Pro_00010.png', 'WIN_20180907_16_42_55_Pro_00020.png', 'WIN_20180907_16_42_55_Pro_00012.png', 'WIN_20180907_16_42_55_Pro_00006.png', 'WIN_20180907_16_42_55_Pro_00016.png', 'WIN_20180907_16_42_55_Pro_00022.png', 'WIN_20180907_16_42_55_Pro_00014.png', 'WIN_20180907_16_42_55_Pro_00058.png', 'WIN_20180907_16_42_55_Pro_00046.png', 'WIN_20180907_16_42_55_Pro_00064.png', 'WIN_20180907_16_42_55_Pro_00030.png', 'WIN_20180907_16_42_55_Pro_00052.png', 'WIN_20180907_16_42_55_Pro_00032.png', 'WIN_20180907_16_42_55_Pro_00054.png', 'WIN_20180907_16_42_55_Pro_00034.png', 'WIN_20180907_16_42_55_Pro_00050.png', 'WIN_20180907_16_42_55_Pro_00028.png', 'WIN_20180907_16_42_55_Pro_00044.png', 'WIN_20180907_16_42_55_Pro_00048.png', 'WIN_20180907_16_42_55_Pro_00042.png', 'WIN_20180907_16_42_55_Pro_00038.png', 'WIN_20180907_16_42_55_Pro_00056.png', 'WIN_20180907_16_42_55_Pro_00062.png', 'WIN_20180907_16_42_55_Pro_00040.png', 'WIN_20180907_16_42_55_Pro_00060.png', 'WIN_20180907_16_42_55_Pro_00036.png']\n",
            "(32, 20, 120, 120, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation batch sample\n",
        "sample_val_generator = generator(val_path, val_doc, batch_size, img_tensor)\n",
        "sample_val_batch_data, sample_val_batch_labels = next(sample_val_generator)\n",
        "print(sample_val_batch_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeespUvo9fYD",
        "outputId": "27233bab-8bcf-4cad-a16e-6f3d999407e4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source path =  /content/drive/My Drive/Project_data/val ; batch size = 10\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "IMG tensor 0 [ 0  2  3  5  6  8  9 11 12 14 15 17 18 20 21 23 24 26 27 29]\n",
            "IMG DX list [ 0  2  3  5  6  8  9 11 12 14 15 17 18 20 21 23 24 26 27 29]\n",
            "IMGS... ['WIN_20180907_15_57_43_Pro_00017.png', 'WIN_20180907_15_57_43_Pro_00031.png', 'WIN_20180907_15_57_43_Pro_00023.png', 'WIN_20180907_15_57_43_Pro_00021.png', 'WIN_20180907_15_57_43_Pro_00025.png', 'WIN_20180907_15_57_43_Pro_00029.png', 'WIN_20180907_15_57_43_Pro_00033.png', 'WIN_20180907_15_57_43_Pro_00015.png', 'WIN_20180907_15_57_43_Pro_00019.png', 'WIN_20180907_15_57_43_Pro_00013.png', 'WIN_20180907_15_57_43_Pro_00011.png', 'WIN_20180907_15_57_43_Pro_00009.png', 'WIN_20180907_15_57_43_Pro_00027.png', 'WIN_20180907_15_57_43_Pro_00055.png', 'WIN_20180907_15_57_43_Pro_00049.png', 'WIN_20180907_15_57_43_Pro_00057.png', 'WIN_20180907_15_57_43_Pro_00047.png', 'WIN_20180907_15_57_43_Pro_00043.png', 'WIN_20180907_15_57_43_Pro_00041.png', 'WIN_20180907_15_57_43_Pro_00051.png', 'WIN_20180907_15_57_43_Pro_00067.png', 'WIN_20180907_15_57_43_Pro_00037.png', 'WIN_20180907_15_57_43_Pro_00059.png', 'WIN_20180907_15_57_43_Pro_00053.png', 'WIN_20180907_15_57_43_Pro_00035.png', 'WIN_20180907_15_57_43_Pro_00045.png', 'WIN_20180907_15_57_43_Pro_00039.png', 'WIN_20180907_15_57_43_Pro_00063.png', 'WIN_20180907_15_57_43_Pro_00061.png', 'WIN_20180907_15_57_43_Pro_00065.png']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-37d2658fef7f>:16: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMGS... ['WIN_20180925_17_43_01_Pro_00030.png', 'WIN_20180925_17_43_01_Pro_00026.png', 'WIN_20180925_17_43_01_Pro_00011.png', 'WIN_20180925_17_43_01_Pro_00010.png', 'WIN_20180925_17_43_01_Pro_00029.png', 'WIN_20180925_17_43_01_Pro_00028.png', 'WIN_20180925_17_43_01_Pro_00034.png', 'WIN_20180925_17_43_01_Pro_00019.png', 'WIN_20180925_17_43_01_Pro_00024.png', 'WIN_20180925_17_43_01_Pro_00012.png', 'WIN_20180925_17_43_01_Pro_00008.png', 'WIN_20180925_17_43_01_Pro_00015.png', 'WIN_20180925_17_43_01_Pro_00031.png', 'WIN_20180925_17_43_01_Pro_00018.png', 'WIN_20180925_17_43_01_Pro_00021.png', 'WIN_20180925_17_43_01_Pro_00020.png', 'WIN_20180925_17_43_01_Pro_00022.png', 'WIN_20180925_17_43_01_Pro_00016.png', 'WIN_20180925_17_43_01_Pro_00009.png', 'WIN_20180925_17_43_01_Pro_00032.png', 'WIN_20180925_17_43_01_Pro_00033.png', 'WIN_20180925_17_43_01_Pro_00036.png', 'WIN_20180925_17_43_01_Pro_00007.png', 'WIN_20180925_17_43_01_Pro_00023.png', 'WIN_20180925_17_43_01_Pro_00017.png', 'WIN_20180925_17_43_01_Pro_00014.png', 'WIN_20180925_17_43_01_Pro_00025.png', 'WIN_20180925_17_43_01_Pro_00027.png', 'WIN_20180925_17_43_01_Pro_00035.png', 'WIN_20180925_17_43_01_Pro_00013.png']\n",
            "IMGS... ['WIN_20180925_17_40_03_Pro_00036.png', 'WIN_20180925_17_40_03_Pro_00010.png', 'WIN_20180925_17_40_03_Pro_00020.png', 'WIN_20180925_17_40_03_Pro_00052.png', 'WIN_20180925_17_40_03_Pro_00042.png', 'WIN_20180925_17_40_03_Pro_00040.png', 'WIN_20180925_17_40_03_Pro_00014.png', 'WIN_20180925_17_40_03_Pro_00050.png', 'WIN_20180925_17_40_03_Pro_00016.png', 'WIN_20180925_17_40_03_Pro_00048.png', 'WIN_20180925_17_40_03_Pro_00006.png', 'WIN_20180925_17_40_03_Pro_00028.png', 'WIN_20180925_17_40_03_Pro_00030.png', 'WIN_20180925_17_40_03_Pro_00008.png', 'WIN_20180925_17_40_03_Pro_00044.png', 'WIN_20180925_17_40_03_Pro_00054.png', 'WIN_20180925_17_40_03_Pro_00004.png', 'WIN_20180925_17_40_03_Pro_00024.png', 'WIN_20180925_17_40_03_Pro_00018.png', 'WIN_20180925_17_40_03_Pro_00012.png', 'WIN_20180925_17_40_03_Pro_00032.png', 'WIN_20180925_17_40_03_Pro_00046.png', 'WIN_20180925_17_40_03_Pro_00038.png', 'WIN_20180925_17_40_03_Pro_00034.png', 'WIN_20180925_17_40_03_Pro_00022.png', 'WIN_20180925_17_40_03_Pro_00026.png', 'WIN_20180925_17_40_03_Pro_00060.png', 'WIN_20180925_17_40_03_Pro_00056.png', 'WIN_20180925_17_40_03_Pro_00062.png', 'WIN_20180925_17_40_03_Pro_00058.png']\n",
            "IMGS... ['WIN_20180907_16_31_41_Pro_00018.png', 'WIN_20180907_16_31_41_Pro_00030.png', 'WIN_20180907_16_31_41_Pro_00048.png', 'WIN_20180907_16_31_41_Pro_00026.png', 'WIN_20180907_16_31_41_Pro_00038.png', 'WIN_20180907_16_31_41_Pro_00046.png', 'WIN_20180907_16_31_41_Pro_00020.png', 'WIN_20180907_16_31_41_Pro_00014.png', 'WIN_20180907_16_31_41_Pro_00010.png', 'WIN_20180907_16_31_41_Pro_00040.png', 'WIN_20180907_16_31_41_Pro_00050.png', 'WIN_20180907_16_31_41_Pro_00024.png', 'WIN_20180907_16_31_41_Pro_00022.png', 'WIN_20180907_16_31_41_Pro_00016.png', 'WIN_20180907_16_31_41_Pro_00008.png', 'WIN_20180907_16_31_41_Pro_00042.png', 'WIN_20180907_16_31_41_Pro_00012.png', 'WIN_20180907_16_31_41_Pro_00028.png', 'WIN_20180907_16_31_41_Pro_00032.png', 'WIN_20180907_16_31_41_Pro_00044.png', 'WIN_20180907_16_31_41_Pro_00036.png', 'WIN_20180907_16_31_41_Pro_00034.png', 'WIN_20180907_16_31_41_Pro_00066.png', 'WIN_20180907_16_31_41_Pro_00064.png', 'WIN_20180907_16_31_41_Pro_00058.png', 'WIN_20180907_16_31_41_Pro_00062.png', 'WIN_20180907_16_31_41_Pro_00052.png', 'WIN_20180907_16_31_41_Pro_00054.png', 'WIN_20180907_16_31_41_Pro_00056.png', 'WIN_20180907_16_31_41_Pro_00060.png']\n",
            "IMGS... ['WIN_20180926_17_08_11_Pro_00014.png', 'WIN_20180926_17_08_11_Pro_00010.png', 'WIN_20180926_17_08_11_Pro_00006.png', 'WIN_20180926_17_08_11_Pro_00024.png', 'WIN_20180926_17_08_11_Pro_00026.png', 'WIN_20180926_17_08_11_Pro_00036.png', 'WIN_20180926_17_08_11_Pro_00012.png', 'WIN_20180926_17_08_11_Pro_00032.png', 'WIN_20180926_17_08_11_Pro_00034.png', 'WIN_20180926_17_08_11_Pro_00028.png', 'WIN_20180926_17_08_11_Pro_00022.png', 'WIN_20180926_17_08_11_Pro_00020.png', 'WIN_20180926_17_08_11_Pro_00038.png', 'WIN_20180926_17_08_11_Pro_00056.png', 'WIN_20180926_17_08_11_Pro_00052.png', 'WIN_20180926_17_08_11_Pro_00060.png', 'WIN_20180926_17_08_11_Pro_00044.png', 'WIN_20180926_17_08_11_Pro_00046.png', 'WIN_20180926_17_08_11_Pro_00058.png', 'WIN_20180926_17_08_11_Pro_00018.png', 'WIN_20180926_17_08_11_Pro_00042.png', 'WIN_20180926_17_08_11_Pro_00048.png', 'WIN_20180926_17_08_11_Pro_00062.png', 'WIN_20180926_17_08_11_Pro_00016.png', 'WIN_20180926_17_08_11_Pro_00054.png', 'WIN_20180926_17_08_11_Pro_00008.png', 'WIN_20180926_17_08_11_Pro_00040.png', 'WIN_20180926_17_08_11_Pro_00050.png', 'WIN_20180926_17_08_11_Pro_00004.png', 'WIN_20180926_17_08_11_Pro_00030.png']\n",
            "IMGS... ['WIN_20180907_15_52_05_Pro_00032.png', 'WIN_20180907_15_52_05_Pro_00022.png', 'WIN_20180907_15_52_05_Pro_00018.png', 'WIN_20180907_15_52_05_Pro_00014.png', 'WIN_20180907_15_52_05_Pro_00034.png', 'WIN_20180907_15_52_05_Pro_00030.png', 'WIN_20180907_15_52_05_Pro_00042.png', 'WIN_20180907_15_52_05_Pro_00012.png', 'WIN_20180907_15_52_05_Pro_00024.png', 'WIN_20180907_15_52_05_Pro_00026.png', 'WIN_20180907_15_52_05_Pro_00044.png', 'WIN_20180907_15_52_05_Pro_00040.png', 'WIN_20180907_15_52_05_Pro_00028.png', 'WIN_20180907_15_52_05_Pro_00020.png', 'WIN_20180907_15_52_05_Pro_00036.png', 'WIN_20180907_15_52_05_Pro_00008.png', 'WIN_20180907_15_52_05_Pro_00016.png', 'WIN_20180907_15_52_05_Pro_00038.png', 'WIN_20180907_15_52_05_Pro_00010.png', 'WIN_20180907_15_52_05_Pro_00052.png', 'WIN_20180907_15_52_05_Pro_00046.png', 'WIN_20180907_15_52_05_Pro_00054.png', 'WIN_20180907_15_52_05_Pro_00064.png', 'WIN_20180907_15_52_05_Pro_00060.png', 'WIN_20180907_15_52_05_Pro_00048.png', 'WIN_20180907_15_52_05_Pro_00050.png', 'WIN_20180907_15_52_05_Pro_00058.png', 'WIN_20180907_15_52_05_Pro_00062.png', 'WIN_20180907_15_52_05_Pro_00066.png', 'WIN_20180907_15_52_05_Pro_00056.png']\n",
            "IMGS... ['WIN_20180926_17_15_35_Pro_00002.png', 'WIN_20180926_17_15_35_Pro_00028.png', 'WIN_20180926_17_15_35_Pro_00034.png', 'WIN_20180926_17_15_35_Pro_00022.png', 'WIN_20180926_17_15_35_Pro_00010.png', 'WIN_20180926_17_15_35_Pro_00032.png', 'WIN_20180926_17_15_35_Pro_00004.png', 'WIN_20180926_17_15_35_Pro_00018.png', 'WIN_20180926_17_15_35_Pro_00024.png', 'WIN_20180926_17_15_35_Pro_00016.png', 'WIN_20180926_17_15_35_Pro_00020.png', 'WIN_20180926_17_15_35_Pro_00040.png', 'WIN_20180926_17_15_35_Pro_00014.png', 'WIN_20180926_17_15_35_Pro_00006.png', 'WIN_20180926_17_15_35_Pro_00030.png', 'WIN_20180926_17_15_35_Pro_00012.png', 'WIN_20180926_17_15_35_Pro_00038.png', 'WIN_20180926_17_15_35_Pro_00036.png', 'WIN_20180926_17_15_35_Pro_00026.png', 'WIN_20180926_17_15_35_Pro_00008.png', 'WIN_20180926_17_15_35_Pro_00042.png', 'WIN_20180926_17_15_35_Pro_00048.png', 'WIN_20180926_17_15_35_Pro_00058.png', 'WIN_20180926_17_15_35_Pro_00052.png', 'WIN_20180926_17_15_35_Pro_00054.png', 'WIN_20180926_17_15_35_Pro_00060.png', 'WIN_20180926_17_15_35_Pro_00046.png', 'WIN_20180926_17_15_35_Pro_00044.png', 'WIN_20180926_17_15_35_Pro_00056.png', 'WIN_20180926_17_15_35_Pro_00050.png']\n",
            "IMGS... ['WIN_20180926_17_29_34_Pro_00020.png', 'WIN_20180926_17_29_34_Pro_00006.png', 'WIN_20180926_17_29_34_Pro_00026.png', 'WIN_20180926_17_29_34_Pro_00008.png', 'WIN_20180926_17_29_34_Pro_00000.png', 'WIN_20180926_17_29_34_Pro_00012.png', 'WIN_20180926_17_29_34_Pro_00022.png', 'WIN_20180926_17_29_34_Pro_00014.png', 'WIN_20180926_17_29_34_Pro_00028.png', 'WIN_20180926_17_29_34_Pro_00016.png', 'WIN_20180926_17_29_34_Pro_00004.png', 'WIN_20180926_17_29_34_Pro_00034.png', 'WIN_20180926_17_29_34_Pro_00032.png', 'WIN_20180926_17_29_34_Pro_00010.png', 'WIN_20180926_17_29_34_Pro_00018.png', 'WIN_20180926_17_29_34_Pro_00024.png', 'WIN_20180926_17_29_34_Pro_00036.png', 'WIN_20180926_17_29_34_Pro_00030.png', 'WIN_20180926_17_29_34_Pro_00002.png', 'WIN_20180926_17_29_34_Pro_00038.png', 'WIN_20180926_17_29_34_Pro_00042.png', 'WIN_20180926_17_29_34_Pro_00040.png', 'WIN_20180926_17_29_34_Pro_00058.png', 'WIN_20180926_17_29_34_Pro_00056.png', 'WIN_20180926_17_29_34_Pro_00046.png', 'WIN_20180926_17_29_34_Pro_00044.png', 'WIN_20180926_17_29_34_Pro_00052.png', 'WIN_20180926_17_29_34_Pro_00048.png', 'WIN_20180926_17_29_34_Pro_00050.png', 'WIN_20180926_17_29_34_Pro_00054.png']\n",
            "IMGS... ['WIN_20180926_17_32_40_Pro_00007.png', 'WIN_20180926_17_32_40_Pro_00025.png', 'WIN_20180926_17_32_40_Pro_00021.png', 'WIN_20180926_17_32_40_Pro_00029.png', 'WIN_20180926_17_32_40_Pro_00051.png', 'WIN_20180926_17_32_40_Pro_00027.png', 'WIN_20180926_17_32_40_Pro_00057.png', 'WIN_20180926_17_32_40_Pro_00017.png', 'WIN_20180926_17_32_40_Pro_00065.png', 'WIN_20180926_17_32_40_Pro_00039.png', 'WIN_20180926_17_32_40_Pro_00037.png', 'WIN_20180926_17_32_40_Pro_00033.png', 'WIN_20180926_17_32_40_Pro_00053.png', 'WIN_20180926_17_32_40_Pro_00049.png', 'WIN_20180926_17_32_40_Pro_00041.png', 'WIN_20180926_17_32_40_Pro_00009.png', 'WIN_20180926_17_32_40_Pro_00045.png', 'WIN_20180926_17_32_40_Pro_00059.png', 'WIN_20180926_17_32_40_Pro_00055.png', 'WIN_20180926_17_32_40_Pro_00043.png', 'WIN_20180926_17_32_40_Pro_00061.png', 'WIN_20180926_17_32_40_Pro_00013.png', 'WIN_20180926_17_32_40_Pro_00063.png', 'WIN_20180926_17_32_40_Pro_00023.png', 'WIN_20180926_17_32_40_Pro_00019.png', 'WIN_20180926_17_32_40_Pro_00015.png', 'WIN_20180926_17_32_40_Pro_00011.png', 'WIN_20180926_17_32_40_Pro_00035.png', 'WIN_20180926_17_32_40_Pro_00031.png', 'WIN_20180926_17_32_40_Pro_00047.png']\n",
            "IMGS... ['WIN_20180907_16_05_32_Pro_00022.png', 'WIN_20180907_16_05_32_Pro_00018.png', 'WIN_20180907_16_05_32_Pro_00036.png', 'WIN_20180907_16_05_32_Pro_00032.png', 'WIN_20180907_16_05_32_Pro_00020.png', 'WIN_20180907_16_05_32_Pro_00034.png', 'WIN_20180907_16_05_32_Pro_00030.png', 'WIN_20180907_16_05_32_Pro_00026.png', 'WIN_20180907_16_05_32_Pro_00028.png', 'WIN_20180907_16_05_32_Pro_00024.png', 'WIN_20180907_16_05_32_Pro_00056.png', 'WIN_20180907_16_05_32_Pro_00058.png', 'WIN_20180907_16_05_32_Pro_00050.png', 'WIN_20180907_16_05_32_Pro_00062.png', 'WIN_20180907_16_05_32_Pro_00054.png', 'WIN_20180907_16_05_32_Pro_00060.png', 'WIN_20180907_16_05_32_Pro_00042.png', 'WIN_20180907_16_05_32_Pro_00070.png', 'WIN_20180907_16_05_32_Pro_00044.png', 'WIN_20180907_16_05_32_Pro_00076.png', 'WIN_20180907_16_05_32_Pro_00046.png', 'WIN_20180907_16_05_32_Pro_00038.png', 'WIN_20180907_16_05_32_Pro_00072.png', 'WIN_20180907_16_05_32_Pro_00068.png', 'WIN_20180907_16_05_32_Pro_00066.png', 'WIN_20180907_16_05_32_Pro_00074.png', 'WIN_20180907_16_05_32_Pro_00040.png', 'WIN_20180907_16_05_32_Pro_00064.png', 'WIN_20180907_16_05_32_Pro_00048.png', 'WIN_20180907_16_05_32_Pro_00052.png']\n",
            "(10, 20, 120, 120, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make3dFilter(x):\n",
        "    return tuple([x]*3)\n",
        "\n",
        "def make2dFilter(x):\n",
        "    return tuple([x]*2)\n",
        "\n",
        "print(make2dFilter(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8icHtw2KLYVK",
        "outputId": "501a8d60-78df-431f-8648-09a66602e093"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " inputShape = (len(img_tensor[0]), img_tensor[1], img_tensor[2], img_tensor[3])\n",
        " print(inputShape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqbaQtkrMaYg",
        "outputId": "6bbe8649-ad79-40a9-94f6-c6c4fadd1c52"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20, 120, 120, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pZZSKluK_yA"
      },
      "source": [
        "## Model\n",
        "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rcliwfdpK_yA"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout, LSTM\n",
        "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "\n",
        "# #write your model here\n",
        "\n",
        "# # Create a Sequential model\n",
        "# model = Sequential()\n",
        "\n",
        "# # Add Conv3D layer\n",
        "# model.add(Conv3D(filters=16, kernel_size=(5, 5, 5), activation='relu', input_shape=(inputShape)))\n",
        "\n",
        "# # Add Conv3D layer\n",
        "# model.add(Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu'))\n",
        "\n",
        "# # Add Conv3D layer\n",
        "# model.add(Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu'))\n",
        "\n",
        "# # Add MaxPooling3D layer\n",
        "# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# # Add Flatten layer\n",
        "# model.add(Flatten())\n",
        "\n",
        "# # Add Dense layers\n",
        "# model.add(Dense(128, activation='relu'))\n",
        "# model.add(Dense(64, activation='softmax'))  # num_classes is the number of classes in your problem\n",
        "\n",
        "# Compile the model\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOJ42LwtK_yB"
      },
      "source": [
        "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "M5VWkJPVK_yB"
      },
      "outputs": [],
      "source": [
        "# optimiser = 'adam'\n",
        "# model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "# print (model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def defineModel(img_tensor):\n",
        "    inputShape = (len(img_tensor[0]), img_tensor[1], img_tensor[2], img_tensor[3])\n",
        "\n",
        "    model = Sequential([\n",
        "        Conv3D(16, make3dFilter(5), activation='relu', input_shape=inputShape),\n",
        "        MaxPooling3D(make3dFilter(2), padding='same'),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        Conv3D(32, make3dFilter(3), activation='relu'),\n",
        "        MaxPooling3D(pool_size=(1,2,2), padding='same'),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        Conv3D(64, make3dFilter(3), activation='relu'),\n",
        "        MaxPooling3D(pool_size=(1,2,2), padding='same'),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        Dense(64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        Dense(5, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "    return model\n",
        "\n",
        "model = defineModel(img_tensor)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rD2l9GOhEpS",
        "outputId": "bdc6fc4f-b4e4-47bc-fc42-8e2bf9b97d51"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d (Conv3D)             (None, 16, 116, 116, 16)  6016      \n",
            "                                                                 \n",
            " max_pooling3d (MaxPooling3D  (None, 8, 58, 58, 16)    0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 8, 58, 58, 16)    64        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv3d_1 (Conv3D)           (None, 6, 56, 56, 32)     13856     \n",
            "                                                                 \n",
            " max_pooling3d_1 (MaxPooling  (None, 6, 28, 28, 32)    0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 6, 28, 28, 32)    128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv3d_2 (Conv3D)           (None, 4, 26, 26, 64)     55360     \n",
            "                                                                 \n",
            " max_pooling3d_2 (MaxPooling  (None, 4, 13, 13, 64)    0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 4, 13, 13, 64)    256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 43264)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               5537920   \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 128)              512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 64)               256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,622,949\n",
            "Trainable params: 5,622,341\n",
            "Non-trainable params: 608\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBJtILzoK_yB"
      },
      "source": [
        "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gPJqw0EmK_yB"
      },
      "outputs": [],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size, img_tensor)\n",
        "val_generator = generator(val_path, val_doc, batch_size, img_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfMSHnHHK_yB",
        "outputId": "2549073a-dca8-4980-8aa7-999c7f3d4241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ],
      "source": [
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "\n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "\n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)# write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e23DAZfCK_yC"
      },
      "source": [
        "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "e_67QZIrK_yC"
      },
      "outputs": [],
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 25"
      ],
      "metadata": {
        "id": "-L2E58t-_elj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ehPu0ceDPje5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plotModelHistory(h):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(15,4))\n",
        "    ax[0].plot(h.history['loss'])\n",
        "    ax[0].plot(h.history['val_loss'])\n",
        "    ax[0].legend(['loss','val_loss'])\n",
        "    ax[0].title.set_text(\"Train loss vs Validation loss\")\n",
        "\n",
        "    ax[1].plot(h.history['categorical_accuracy'])\n",
        "    ax[1].plot(h.history['val_categorical_accuracy'])\n",
        "    ax[1].legend(['categorical_accuracy','val_categorical_accuracy'])\n",
        "    ax[1].title.set_text(\"Train accuracy vs Validation accuracy\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Max. Training Accuracy\", max(h.history['categorical_accuracy']))\n",
        "    print(\"Max. Validaiton Accuracy\", max(h.history['val_categorical_accuracy']))"
      ],
      "metadata": {
        "id": "9aWH-ZOeEt_l"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n7fYOv9K_yC"
      },
      "source": [
        "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgpz68ebK_yC",
        "outputId": "280e9d4d-80ed-4822-fbdf-ad32e1620c10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-11e90372dca3>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  base_model_history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
            "<ipython-input-11-9e653c9fc82e>:17: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "; Current Batch = 135\n",
            "Epoch 1/25\n",
            "; Current Batch = 136\n",
            " 1/67 [..............................] - ETA: 4s - loss: 2.3967 - categorical_accuracy: 0.0000e+00; Current Batch = 137\n",
            " 2/67 [..............................] - ETA: 44s - loss: 2.1140 - categorical_accuracy: 0.0000e+00; Current Batch = 138\n",
            " 3/67 [>.............................] - ETA: 42s - loss: 2.4729 - categorical_accuracy: 0.0000e+00; Current Batch = 139\n",
            " 4/67 [>.............................] - ETA: 53s - loss: 2.3147 - categorical_accuracy: 0.0000e+00; Current Batch = 140\n",
            " 5/67 [=>............................] - ETA: 47s - loss: 2.1299 - categorical_accuracy: 0.1333    ; Current Batch = 141\n",
            " 6/67 [=>............................] - ETA: 41s - loss: 2.0788 - categorical_accuracy: 0.1667; Current Batch = 142\n",
            " 7/67 [==>...........................] - ETA: 38s - loss: 2.1166 - categorical_accuracy: 0.1429; Current Batch = 143\n",
            " 8/67 [==>...........................] - ETA: 38s - loss: 2.0799 - categorical_accuracy: 0.1667; Current Batch = 144\n",
            " 9/67 [===>..........................] - ETA: 35s - loss: 1.9340 - categorical_accuracy: 0.2593; Current Batch = 145\n",
            "10/67 [===>..........................] - ETA: 37s - loss: 1.9416 - categorical_accuracy: 0.2333; Current Batch = 146\n",
            "11/67 [===>..........................] - ETA: 35s - loss: 2.0605 - categorical_accuracy: 0.2121; Current Batch = 147\n",
            "12/67 [====>.........................] - ETA: 33s - loss: 1.9844 - categorical_accuracy: 0.2222; Current Batch = 148\n",
            "13/67 [====>.........................] - ETA: 32s - loss: 1.9436 - categorical_accuracy: 0.2308; Current Batch = 149\n",
            "14/67 [=====>........................] - ETA: 31s - loss: 1.9237 - categorical_accuracy: 0.2143; Current Batch = 150\n",
            "15/67 [=====>........................] - ETA: 29s - loss: 1.9195 - categorical_accuracy: 0.2000; Current Batch = 151\n",
            "16/67 [======>.......................] - ETA: 28s - loss: 1.8671 - categorical_accuracy: 0.2292; Current Batch = 152\n",
            "17/67 [======>.......................] - ETA: 29s - loss: 1.8597 - categorical_accuracy: 0.2549; Current Batch = 153\n",
            "18/67 [=======>......................] - ETA: 27s - loss: 1.7733 - categorical_accuracy: 0.2963; Current Batch = 154\n",
            "19/67 [=======>......................] - ETA: 27s - loss: 1.8158 - categorical_accuracy: 0.2807; Current Batch = 155\n",
            "20/67 [=======>......................] - ETA: 27s - loss: 1.8592 - categorical_accuracy: 0.2833; Current Batch = 156\n",
            "21/67 [========>.....................] - ETA: 27s - loss: 1.8663 - categorical_accuracy: 0.2857; Current Batch = 157\n",
            "22/67 [========>.....................] - ETA: 26s - loss: 1.8541 - categorical_accuracy: 0.2879; Current Batch = 158\n",
            "23/67 [=========>....................] - ETA: 26s - loss: 1.8317 - categorical_accuracy: 0.3043; Current Batch = 159\n",
            "24/67 [=========>....................] - ETA: 25s - loss: 1.8011 - categorical_accuracy: 0.3194; Current Batch = 160\n",
            "25/67 [==========>...................] - ETA: 24s - loss: 1.7945 - categorical_accuracy: 0.3200; Current Batch = 161\n",
            "26/67 [==========>...................] - ETA: 23s - loss: 1.8489 - categorical_accuracy: 0.3077; Current Batch = 162\n",
            "27/67 [===========>..................] - ETA: 22s - loss: 1.8762 - categorical_accuracy: 0.3086; Current Batch = 163\n",
            "28/67 [===========>..................] - ETA: 22s - loss: 1.8907 - categorical_accuracy: 0.2976; Current Batch = 164\n",
            "29/67 [===========>..................] - ETA: 21s - loss: 1.8728 - categorical_accuracy: 0.2989; Current Batch = 165\n",
            "30/67 [============>.................] - ETA: 20s - loss: 1.8797 - categorical_accuracy: 0.2889; Current Batch = 166\n",
            "31/67 [============>.................] - ETA: 20s - loss: 1.8572 - categorical_accuracy: 0.3011; Current Batch = 167\n",
            "32/67 [=============>................] - ETA: 24s - loss: 1.8563 - categorical_accuracy: 0.3021; Current Batch = 168\n",
            "33/67 [=============>................] - ETA: 23s - loss: 1.8775 - categorical_accuracy: 0.2929; Current Batch = 169\n",
            "34/67 [==============>...............] - ETA: 22s - loss: 1.8721 - categorical_accuracy: 0.2843; Current Batch = 170\n",
            "35/67 [==============>...............] - ETA: 21s - loss: 1.9289 - categorical_accuracy: 0.2762; Current Batch = 171\n",
            "36/67 [===============>..............] - ETA: 20s - loss: 1.9561 - categorical_accuracy: 0.2685; Current Batch = 172\n",
            "37/67 [===============>..............] - ETA: 19s - loss: 1.9612 - categorical_accuracy: 0.2613; Current Batch = 173\n",
            "38/67 [================>.............] - ETA: 18s - loss: 1.9489 - categorical_accuracy: 0.2544; Current Batch = 174\n",
            "39/67 [================>.............] - ETA: 18s - loss: 1.9484 - categorical_accuracy: 0.2564; Current Batch = 175\n",
            "40/67 [================>.............] - ETA: 17s - loss: 1.9259 - categorical_accuracy: 0.2667; Current Batch = 176\n",
            "41/67 [=================>............] - ETA: 16s - loss: 1.9106 - categorical_accuracy: 0.2602; Current Batch = 177\n",
            "42/67 [=================>............] - ETA: 15s - loss: 1.9108 - categorical_accuracy: 0.2619; Current Batch = 178\n",
            "43/67 [==================>...........] - ETA: 14s - loss: 1.8969 - categorical_accuracy: 0.2636; Current Batch = 179\n",
            "44/67 [==================>...........] - ETA: 14s - loss: 1.9066 - categorical_accuracy: 0.2576; Current Batch = 180\n",
            "45/67 [===================>..........] - ETA: 13s - loss: 1.9056 - categorical_accuracy: 0.2519; Current Batch = 181\n",
            "46/67 [===================>..........] - ETA: 12s - loss: 1.9063 - categorical_accuracy: 0.2536; Current Batch = 182\n",
            "47/67 [====================>.........] - ETA: 12s - loss: 1.9067 - categorical_accuracy: 0.2482; Current Batch = 183\n",
            "48/67 [====================>.........] - ETA: 11s - loss: 1.9051 - categorical_accuracy: 0.2431; Current Batch = 184\n",
            "49/67 [====================>.........] - ETA: 11s - loss: 1.9162 - categorical_accuracy: 0.2449; Current Batch = 185\n",
            "50/67 [=====================>........] - ETA: 10s - loss: 1.9095 - categorical_accuracy: 0.2467; Current Batch = 186\n",
            "51/67 [=====================>........] - ETA: 9s - loss: 1.9211 - categorical_accuracy: 0.2418 ; Current Batch = 187\n",
            "52/67 [======================>.......] - ETA: 9s - loss: 1.9357 - categorical_accuracy: 0.2372; Current Batch = 188\n",
            "53/67 [======================>.......] - ETA: 8s - loss: 1.9255 - categorical_accuracy: 0.2390; Current Batch = 189\n",
            "54/67 [=======================>......] - ETA: 7s - loss: 1.9348 - categorical_accuracy: 0.2346; Current Batch = 190\n",
            "55/67 [=======================>......] - ETA: 7s - loss: 1.9481 - categorical_accuracy: 0.2303; Current Batch = 191\n",
            "56/67 [========================>.....] - ETA: 6s - loss: 1.9512 - categorical_accuracy: 0.2262; Current Batch = 192\n",
            "57/67 [========================>.....] - ETA: 6s - loss: 1.9432 - categorical_accuracy: 0.2281; Current Batch = 193\n",
            "58/67 [========================>.....] - ETA: 5s - loss: 1.9403 - categorical_accuracy: 0.2299; Current Batch = 194\n",
            "59/67 [=========================>....] - ETA: 4s - loss: 1.9494 - categorical_accuracy: 0.2316; Current Batch = 195\n",
            "60/67 [=========================>....] - ETA: 4s - loss: 1.9372 - categorical_accuracy: 0.2333; Current Batch = 196\n",
            "61/67 [==========================>...] - ETA: 3s - loss: 1.9522 - categorical_accuracy: 0.2295; Current Batch = 197\n",
            "62/67 [==========================>...] - ETA: 3s - loss: 1.9609 - categorical_accuracy: 0.2258; Current Batch = 198\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.9526 - categorical_accuracy: 0.2222; Current Batch = 199\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.9661 - categorical_accuracy: 0.2188; Current Batch = 200\n",
            "65/67 [============================>.] - ETA: 1s - loss: 1.9708 - categorical_accuracy: 0.2205; Current Batch = 201\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.9614 - categorical_accuracy: 0.2273; Current Batch = 202\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.9583 - categorical_accuracy: 0.2289; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "\n",
            "Epoch 1: saving model to model_init_2023-08-3109_43_59.510610/model-00001-1.95834-0.22886-4.47054-0.28000.h5\n",
            "67/67 [==============================] - 59s 889ms/step - loss: 1.9583 - categorical_accuracy: 0.2289 - val_loss: 4.4705 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
            "Epoch 2/25\n",
            "; Current Batch = 203\n",
            " 1/67 [..............................] - ETA: 3s - loss: 1.8355 - categorical_accuracy: 0.3333; Current Batch = 204\n",
            " 2/67 [..............................] - ETA: 1:03 - loss: 2.0179 - categorical_accuracy: 0.1667; Current Batch = 205\n",
            " 3/67 [>.............................] - ETA: 54s - loss: 2.0672 - categorical_accuracy: 0.2222 ; Current Batch = 206\n",
            " 4/67 [>.............................] - ETA: 50s - loss: 2.1388 - categorical_accuracy: 0.1667; Current Batch = 207\n",
            " 5/67 [=>............................] - ETA: 52s - loss: 2.3239 - categorical_accuracy: 0.1333; Current Batch = 208\n",
            " 6/67 [=>............................] - ETA: 46s - loss: 2.2845 - categorical_accuracy: 0.1667; Current Batch = 209\n",
            " 7/67 [==>...........................] - ETA: 42s - loss: 2.2180 - categorical_accuracy: 0.1429; Current Batch = 210\n",
            " 8/67 [==>...........................] - ETA: 38s - loss: 2.1350 - categorical_accuracy: 0.1250; Current Batch = 211\n",
            " 9/67 [===>..........................] - ETA: 35s - loss: 2.1693 - categorical_accuracy: 0.1111; Current Batch = 212\n",
            "10/67 [===>..........................] - ETA: 34s - loss: 2.1993 - categorical_accuracy: 0.1000; Current Batch = 213\n",
            "11/67 [===>..........................] - ETA: 33s - loss: 2.2182 - categorical_accuracy: 0.0909; Current Batch = 214\n",
            "12/67 [====>.........................] - ETA: 32s - loss: 2.2182 - categorical_accuracy: 0.1111; Current Batch = 215\n",
            "13/67 [====>.........................] - ETA: 31s - loss: 2.2117 - categorical_accuracy: 0.1026; Current Batch = 216\n",
            "14/67 [=====>........................] - ETA: 30s - loss: 2.2224 - categorical_accuracy: 0.0952; Current Batch = 217\n",
            "15/67 [=====>........................] - ETA: 28s - loss: 2.2372 - categorical_accuracy: 0.0889; Current Batch = 218\n",
            "16/67 [======>.......................] - ETA: 28s - loss: 2.2567 - categorical_accuracy: 0.1042; Current Batch = 219\n",
            "17/67 [======>.......................] - ETA: 27s - loss: 2.2336 - categorical_accuracy: 0.0980; Current Batch = 220\n",
            "18/67 [=======>......................] - ETA: 26s - loss: 2.2416 - categorical_accuracy: 0.0926; Num of batches = 221\n",
            "; Current Batch = 0\n",
            "19/67 [=======>......................] - ETA: 25s - loss: 2.1859 - categorical_accuracy: 0.1228; Current Batch = 1\n",
            "20/67 [=======>......................] - ETA: 24s - loss: 2.2067 - categorical_accuracy: 0.1167; Current Batch = 2\n",
            "21/67 [========>.....................] - ETA: 24s - loss: 2.1474 - categorical_accuracy: 0.1429; Current Batch = 3\n",
            "22/67 [========>.....................] - ETA: 23s - loss: 2.1569 - categorical_accuracy: 0.1364; Current Batch = 4\n",
            "23/67 [=========>....................] - ETA: 23s - loss: 2.1799 - categorical_accuracy: 0.1304; Current Batch = 5\n",
            "24/67 [=========>....................] - ETA: 22s - loss: 2.1764 - categorical_accuracy: 0.1250; Current Batch = 6\n",
            "25/67 [==========>...................] - ETA: 22s - loss: 2.1497 - categorical_accuracy: 0.1200; Current Batch = 7\n",
            "26/67 [==========>...................] - ETA: 21s - loss: 2.1282 - categorical_accuracy: 0.1282; Current Batch = 8\n",
            "27/67 [===========>..................] - ETA: 20s - loss: 2.0920 - categorical_accuracy: 0.1481; Current Batch = 9\n",
            "28/67 [===========>..................] - ETA: 20s - loss: 2.0927 - categorical_accuracy: 0.1429; Current Batch = 10\n",
            "29/67 [===========>..................] - ETA: 19s - loss: 2.0671 - categorical_accuracy: 0.1379; Current Batch = 11\n",
            "30/67 [============>.................] - ETA: 19s - loss: 2.0575 - categorical_accuracy: 0.1444; Current Batch = 12\n",
            "31/67 [============>.................] - ETA: 19s - loss: 2.0234 - categorical_accuracy: 0.1613; Current Batch = 13\n",
            "32/67 [=============>................] - ETA: 18s - loss: 2.0022 - categorical_accuracy: 0.1771; Current Batch = 14\n",
            "33/67 [=============>................] - ETA: 18s - loss: 2.0060 - categorical_accuracy: 0.1818; Current Batch = 15\n",
            "34/67 [==============>...............] - ETA: 18s - loss: 1.9873 - categorical_accuracy: 0.1765; Current Batch = 16\n",
            "35/67 [==============>...............] - ETA: 17s - loss: 1.9912 - categorical_accuracy: 0.1810; Current Batch = 17\n",
            "36/67 [===============>..............] - ETA: 17s - loss: 1.9874 - categorical_accuracy: 0.1852; Current Batch = 18\n",
            "37/67 [===============>..............] - ETA: 16s - loss: 1.9709 - categorical_accuracy: 0.1892; Current Batch = 19\n",
            "38/67 [================>.............] - ETA: 15s - loss: 1.9880 - categorical_accuracy: 0.1930; Current Batch = 20\n",
            "39/67 [================>.............] - ETA: 15s - loss: 2.0021 - categorical_accuracy: 0.1880; Current Batch = 21\n",
            "40/67 [================>.............] - ETA: 14s - loss: 1.9698 - categorical_accuracy: 0.2000; Current Batch = 22\n",
            "41/67 [=================>............] - ETA: 14s - loss: 2.0054 - categorical_accuracy: 0.1951; Current Batch = 23\n",
            "42/67 [=================>............] - ETA: 13s - loss: 2.0230 - categorical_accuracy: 0.1905; Current Batch = 24\n",
            "43/67 [==================>...........] - ETA: 13s - loss: 2.0288 - categorical_accuracy: 0.1860; Current Batch = 25\n",
            "44/67 [==================>...........] - ETA: 12s - loss: 2.0303 - categorical_accuracy: 0.1894; Current Batch = 26\n",
            "45/67 [===================>..........] - ETA: 11s - loss: 2.0167 - categorical_accuracy: 0.1852; Current Batch = 27\n",
            "46/67 [===================>..........] - ETA: 11s - loss: 2.0079 - categorical_accuracy: 0.1884; Current Batch = 28\n",
            "47/67 [====================>.........] - ETA: 10s - loss: 1.9954 - categorical_accuracy: 0.1986; Current Batch = 29\n",
            "48/67 [====================>.........] - ETA: 10s - loss: 2.0017 - categorical_accuracy: 0.1944; Current Batch = 30\n",
            "49/67 [====================>.........] - ETA: 9s - loss: 1.9773 - categorical_accuracy: 0.2041 ; Current Batch = 31\n",
            "50/67 [=====================>........] - ETA: 9s - loss: 1.9601 - categorical_accuracy: 0.2067; Current Batch = 32\n",
            "51/67 [=====================>........] - ETA: 8s - loss: 1.9558 - categorical_accuracy: 0.2026; Current Batch = 33\n",
            "52/67 [======================>.......] - ETA: 8s - loss: 1.9479 - categorical_accuracy: 0.2051; Current Batch = 34\n",
            "53/67 [======================>.......] - ETA: 7s - loss: 1.9505 - categorical_accuracy: 0.2013; Current Batch = 35\n",
            "54/67 [=======================>......] - ETA: 6s - loss: 1.9484 - categorical_accuracy: 0.2099; Current Batch = 36\n",
            "55/67 [=======================>......] - ETA: 6s - loss: 1.9396 - categorical_accuracy: 0.2121; Current Batch = 37\n",
            "56/67 [========================>.....] - ETA: 5s - loss: 1.9331 - categorical_accuracy: 0.2083; Current Batch = 38\n",
            "57/67 [========================>.....] - ETA: 5s - loss: 1.9338 - categorical_accuracy: 0.2105; Current Batch = 39\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.9259 - categorical_accuracy: 0.2069; Current Batch = 40\n",
            "59/67 [=========================>....] - ETA: 4s - loss: 1.9363 - categorical_accuracy: 0.2090; Current Batch = 41\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.9294 - categorical_accuracy: 0.2111; Current Batch = 42\n",
            "61/67 [==========================>...] - ETA: 3s - loss: 1.9207 - categorical_accuracy: 0.2077; Current Batch = 43\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.9219 - categorical_accuracy: 0.2097; Current Batch = 44\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.9045 - categorical_accuracy: 0.2222; Current Batch = 45\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.8952 - categorical_accuracy: 0.2240; Current Batch = 46\n",
            "65/67 [============================>.] - ETA: 1s - loss: 1.9051 - categorical_accuracy: 0.2205; Current Batch = 47\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.8995 - categorical_accuracy: 0.2222; Current Batch = 48\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.9000 - categorical_accuracy: 0.2189; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "\n",
            "Epoch 2: saving model to model_init_2023-08-3109_43_59.510610/model-00002-1.89999-0.21891-3.38679-0.24000.h5\n",
            "67/67 [==============================] - 58s 875ms/step - loss: 1.9000 - categorical_accuracy: 0.2189 - val_loss: 3.3868 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
            "Epoch 3/25\n",
            "; Current Batch = 49\n",
            " 1/67 [..............................] - ETA: 3s - loss: 2.3309 - categorical_accuracy: 0.3333; Current Batch = 50\n",
            " 2/67 [..............................] - ETA: 1:04 - loss: 1.9075 - categorical_accuracy: 0.3333; Current Batch = 51\n",
            " 3/67 [>.............................] - ETA: 1:18 - loss: 2.0708 - categorical_accuracy: 0.3333; Current Batch = 52\n",
            " 4/67 [>.............................] - ETA: 1:06 - loss: 2.0550 - categorical_accuracy: 0.2500; Current Batch = 53\n",
            " 5/67 [=>............................] - ETA: 55s - loss: 2.1573 - categorical_accuracy: 0.2000 ; Current Batch = 54\n",
            " 6/67 [=>............................] - ETA: 47s - loss: 2.2461 - categorical_accuracy: 0.2222; Current Batch = 55\n",
            " 7/67 [==>...........................] - ETA: 45s - loss: 2.0704 - categorical_accuracy: 0.2857; Current Batch = 56\n",
            " 8/67 [==>...........................] - ETA: 43s - loss: 2.0018 - categorical_accuracy: 0.2917; Current Batch = 57\n",
            " 9/67 [===>..........................] - ETA: 40s - loss: 1.9319 - categorical_accuracy: 0.3333; Current Batch = 58\n",
            "10/67 [===>..........................] - ETA: 38s - loss: 1.8797 - categorical_accuracy: 0.3667; Current Batch = 59\n",
            "11/67 [===>..........................] - ETA: 35s - loss: 1.8227 - categorical_accuracy: 0.3939; Current Batch = 60\n",
            "12/67 [====>.........................] - ETA: 34s - loss: 1.8696 - categorical_accuracy: 0.3611; Current Batch = 61\n",
            "13/67 [====>.........................] - ETA: 34s - loss: 1.7834 - categorical_accuracy: 0.4103; Current Batch = 62\n",
            "14/67 [=====>........................] - ETA: 32s - loss: 1.7553 - categorical_accuracy: 0.4048; Current Batch = 63\n",
            "15/67 [=====>........................] - ETA: 31s - loss: 1.7590 - categorical_accuracy: 0.4000; Current Batch = 64\n",
            "16/67 [======>.......................] - ETA: 29s - loss: 1.7135 - categorical_accuracy: 0.4167; Current Batch = 65\n",
            "17/67 [======>.......................] - ETA: 28s - loss: 1.6877 - categorical_accuracy: 0.4118; Current Batch = 66\n",
            "18/67 [=======>......................] - ETA: 27s - loss: 1.6450 - categorical_accuracy: 0.4074; Current Batch = 67\n",
            "19/67 [=======>......................] - ETA: 27s - loss: 1.5962 - categorical_accuracy: 0.4211; Current Batch = 68\n",
            "20/67 [=======>......................] - ETA: 26s - loss: 1.6330 - categorical_accuracy: 0.4167; Current Batch = 69\n",
            "21/67 [========>.....................] - ETA: 26s - loss: 1.6164 - categorical_accuracy: 0.4286; Current Batch = 70\n",
            "22/67 [========>.....................] - ETA: 25s - loss: 1.6167 - categorical_accuracy: 0.4242; Current Batch = 71\n",
            "23/67 [=========>....................] - ETA: 25s - loss: 1.5743 - categorical_accuracy: 0.4493; Current Batch = 72\n",
            "24/67 [=========>....................] - ETA: 25s - loss: 1.5976 - categorical_accuracy: 0.4306; Current Batch = 73\n",
            "25/67 [==========>...................] - ETA: 24s - loss: 1.6033 - categorical_accuracy: 0.4267; Current Batch = 74\n",
            "26/67 [==========>...................] - ETA: 23s - loss: 1.5962 - categorical_accuracy: 0.4231; Current Batch = 75\n",
            "27/67 [===========>..................] - ETA: 23s - loss: 1.6059 - categorical_accuracy: 0.4074; Current Batch = 76\n",
            "28/67 [===========>..................] - ETA: 22s - loss: 1.6320 - categorical_accuracy: 0.3929; Current Batch = 77\n",
            "29/67 [===========>..................] - ETA: 22s - loss: 1.6526 - categorical_accuracy: 0.3908; Current Batch = 78\n",
            "30/67 [============>.................] - ETA: 22s - loss: 1.6671 - categorical_accuracy: 0.3889; Current Batch = 79\n",
            "31/67 [============>.................] - ETA: 21s - loss: 1.6326 - categorical_accuracy: 0.4086; Current Batch = 80\n",
            "32/67 [=============>................] - ETA: 20s - loss: 1.6077 - categorical_accuracy: 0.4167; Current Batch = 81\n",
            "33/67 [=============>................] - ETA: 19s - loss: 1.6071 - categorical_accuracy: 0.4242; Current Batch = 82\n",
            "34/67 [==============>...............] - ETA: 19s - loss: 1.6222 - categorical_accuracy: 0.4118; Current Batch = 83\n",
            "35/67 [==============>...............] - ETA: 18s - loss: 1.6416 - categorical_accuracy: 0.4000; Current Batch = 84\n",
            "36/67 [===============>..............] - ETA: 17s - loss: 1.6431 - categorical_accuracy: 0.3981; Current Batch = 85\n",
            "37/67 [===============>..............] - ETA: 16s - loss: 1.6572 - categorical_accuracy: 0.3874; Current Batch = 86\n",
            "38/67 [================>.............] - ETA: 16s - loss: 1.6749 - categorical_accuracy: 0.3772; Current Batch = 87\n",
            "39/67 [================>.............] - ETA: 15s - loss: 1.7234 - categorical_accuracy: 0.3675; Current Batch = 88\n",
            "40/67 [================>.............] - ETA: 15s - loss: 1.7361 - categorical_accuracy: 0.3583; Current Batch = 89\n",
            "41/67 [=================>............] - ETA: 14s - loss: 1.7355 - categorical_accuracy: 0.3577; Current Batch = 90\n",
            "42/67 [=================>............] - ETA: 14s - loss: 1.7243 - categorical_accuracy: 0.3651; Current Batch = 91\n",
            "43/67 [==================>...........] - ETA: 13s - loss: 1.7581 - categorical_accuracy: 0.3566; Current Batch = 92\n",
            "44/67 [==================>...........] - ETA: 12s - loss: 1.7491 - categorical_accuracy: 0.3561; Current Batch = 93\n",
            "45/67 [===================>..........] - ETA: 12s - loss: 1.7459 - categorical_accuracy: 0.3556; Current Batch = 94\n",
            "46/67 [===================>..........] - ETA: 11s - loss: 1.7404 - categorical_accuracy: 0.3623; Current Batch = 95\n",
            "47/67 [====================>.........] - ETA: 11s - loss: 1.7453 - categorical_accuracy: 0.3546; Current Batch = 96\n",
            "48/67 [====================>.........] - ETA: 10s - loss: 1.7488 - categorical_accuracy: 0.3472; Current Batch = 97\n",
            "49/67 [====================>.........] - ETA: 9s - loss: 1.7602 - categorical_accuracy: 0.3401 ; Current Batch = 98\n",
            "50/67 [=====================>........] - ETA: 9s - loss: 1.7665 - categorical_accuracy: 0.3333; Current Batch = 99\n",
            "51/67 [=====================>........] - ETA: 8s - loss: 1.7859 - categorical_accuracy: 0.3268; Current Batch = 100\n",
            "52/67 [======================>.......] - ETA: 8s - loss: 1.7999 - categorical_accuracy: 0.3205; Current Batch = 101\n",
            "53/67 [======================>.......] - ETA: 7s - loss: 1.8021 - categorical_accuracy: 0.3145; Current Batch = 102\n",
            "54/67 [=======================>......] - ETA: 7s - loss: 1.8264 - categorical_accuracy: 0.3086; Current Batch = 103\n",
            "55/67 [=======================>......] - ETA: 6s - loss: 1.8310 - categorical_accuracy: 0.3030; Current Batch = 104\n",
            "56/67 [========================>.....] - ETA: 6s - loss: 1.8351 - categorical_accuracy: 0.2976; Current Batch = 105\n",
            "57/67 [========================>.....] - ETA: 5s - loss: 1.8291 - categorical_accuracy: 0.2982; Current Batch = 106\n",
            "58/67 [========================>.....] - ETA: 5s - loss: 1.8298 - categorical_accuracy: 0.2931; Current Batch = 107\n",
            "59/67 [=========================>....] - ETA: 4s - loss: 1.8318 - categorical_accuracy: 0.2881; Current Batch = 108\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.8196 - categorical_accuracy: 0.2944; Current Batch = 109\n",
            "61/67 [==========================>...] - ETA: 3s - loss: 1.8195 - categorical_accuracy: 0.2951; Current Batch = 110\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.8250 - categorical_accuracy: 0.2903; Current Batch = 111\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.8186 - categorical_accuracy: 0.2910; Current Batch = 112\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.8299 - categorical_accuracy: 0.2865; Current Batch = 113\n",
            "65/67 [============================>.] - ETA: 1s - loss: 1.8478 - categorical_accuracy: 0.2821; Current Batch = 114\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.8643 - categorical_accuracy: 0.2778; Current Batch = 115\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.8711 - categorical_accuracy: 0.2736; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "\n",
            "Epoch 3: saving model to model_init_2023-08-3109_43_59.510610/model-00003-1.87113-0.27363-12.30732-0.20000.h5\n",
            "67/67 [==============================] - 57s 867ms/step - loss: 1.8711 - categorical_accuracy: 0.2736 - val_loss: 12.3073 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
            "Epoch 4/25\n",
            "; Current Batch = 116\n",
            " 1/67 [..............................] - ETA: 3s - loss: 1.5924 - categorical_accuracy: 0.0000e+00; Current Batch = 117\n",
            " 2/67 [..............................] - ETA: 28s - loss: 1.4792 - categorical_accuracy: 0.1667   ; Current Batch = 118\n",
            " 3/67 [>.............................] - ETA: 27s - loss: 1.7001 - categorical_accuracy: 0.2222; Current Batch = 119\n",
            " 4/67 [>.............................] - ETA: 27s - loss: 1.6274 - categorical_accuracy: 0.3333; Current Batch = 120\n",
            " 5/67 [=>............................] - ETA: 31s - loss: 1.8207 - categorical_accuracy: 0.2667; Current Batch = 121\n",
            " 6/67 [=>............................] - ETA: 28s - loss: 1.9230 - categorical_accuracy: 0.3333; Current Batch = 122\n",
            " 7/67 [==>...........................] - ETA: 26s - loss: 1.9136 - categorical_accuracy: 0.2857; Current Batch = 123\n",
            " 8/67 [==>...........................] - ETA: 26s - loss: 1.8803 - categorical_accuracy: 0.2500; Current Batch = 124\n",
            " 9/67 [===>..........................] - ETA: 26s - loss: 1.8981 - categorical_accuracy: 0.2222; Current Batch = 125\n",
            "10/67 [===>..........................] - ETA: 27s - loss: 1.8101 - categorical_accuracy: 0.2667; Current Batch = 126\n",
            "11/67 [===>..........................] - ETA: 26s - loss: 1.7730 - categorical_accuracy: 0.2727; Current Batch = 127\n",
            "12/67 [====>.........................] - ETA: 26s - loss: 1.7855 - categorical_accuracy: 0.2500; Current Batch = 128\n",
            "13/67 [====>.........................] - ETA: 26s - loss: 1.7877 - categorical_accuracy: 0.2308; Current Batch = 129\n",
            "14/67 [=====>........................] - ETA: 27s - loss: 1.7532 - categorical_accuracy: 0.2143; Current Batch = 130\n",
            "15/67 [=====>........................] - ETA: 26s - loss: 1.7270 - categorical_accuracy: 0.2222; Current Batch = 131\n",
            "16/67 [======>.......................] - ETA: 25s - loss: 1.7484 - categorical_accuracy: 0.2292; Current Batch = 132\n",
            "17/67 [======>.......................] - ETA: 24s - loss: 1.7509 - categorical_accuracy: 0.2353; Current Batch = 133\n",
            "18/67 [=======>......................] - ETA: 23s - loss: 1.7771 - categorical_accuracy: 0.2222; Current Batch = 134\n",
            "19/67 [=======>......................] - ETA: 22s - loss: 1.7440 - categorical_accuracy: 0.2456; Current Batch = 135\n",
            "20/67 [=======>......................] - ETA: 22s - loss: 1.7937 - categorical_accuracy: 0.2333; Current Batch = 136\n",
            "21/67 [========>.....................] - ETA: 21s - loss: 1.7787 - categorical_accuracy: 0.2540; Current Batch = 137\n",
            "22/67 [========>.....................] - ETA: 20s - loss: 1.7667 - categorical_accuracy: 0.2576; Current Batch = 138\n",
            "23/67 [=========>....................] - ETA: 20s - loss: 1.7794 - categorical_accuracy: 0.2609; Current Batch = 139\n",
            "24/67 [=========>....................] - ETA: 20s - loss: 1.7638 - categorical_accuracy: 0.2500; Current Batch = 140\n",
            "25/67 [==========>...................] - ETA: 19s - loss: 1.8171 - categorical_accuracy: 0.2400; Current Batch = 141\n",
            "26/67 [==========>...................] - ETA: 20s - loss: 1.8234 - categorical_accuracy: 0.2308; Current Batch = 142\n",
            "27/67 [===========>..................] - ETA: 19s - loss: 1.8262 - categorical_accuracy: 0.2222; Current Batch = 143\n",
            "28/67 [===========>..................] - ETA: 19s - loss: 1.7966 - categorical_accuracy: 0.2381; Current Batch = 144\n",
            "29/67 [===========>..................] - ETA: 18s - loss: 1.7996 - categorical_accuracy: 0.2414; Current Batch = 145\n",
            "30/67 [============>.................] - ETA: 18s - loss: 1.8471 - categorical_accuracy: 0.2333; Current Batch = 146\n",
            "31/67 [============>.................] - ETA: 18s - loss: 1.8439 - categorical_accuracy: 0.2366; Current Batch = 147\n",
            "32/67 [=============>................] - ETA: 17s - loss: 1.8509 - categorical_accuracy: 0.2292; Current Batch = 148\n",
            "33/67 [=============>................] - ETA: 16s - loss: 1.8558 - categorical_accuracy: 0.2323; Current Batch = 149\n",
            "34/67 [==============>...............] - ETA: 16s - loss: 1.8366 - categorical_accuracy: 0.2451; Current Batch = 150\n",
            "35/67 [==============>...............] - ETA: 15s - loss: 1.8278 - categorical_accuracy: 0.2381; Current Batch = 151\n",
            "36/67 [===============>..............] - ETA: 15s - loss: 1.7995 - categorical_accuracy: 0.2500; Current Batch = 152\n",
            "37/67 [===============>..............] - ETA: 14s - loss: 1.7986 - categorical_accuracy: 0.2523; Current Batch = 153\n",
            "38/67 [================>.............] - ETA: 14s - loss: 1.7912 - categorical_accuracy: 0.2632; Current Batch = 154\n",
            "39/67 [================>.............] - ETA: 13s - loss: 1.8002 - categorical_accuracy: 0.2564; Current Batch = 155\n",
            "40/67 [================>.............] - ETA: 12s - loss: 1.7822 - categorical_accuracy: 0.2583; Current Batch = 156\n",
            "41/67 [=================>............] - ETA: 12s - loss: 1.7746 - categorical_accuracy: 0.2602; Current Batch = 157\n",
            "42/67 [=================>............] - ETA: 11s - loss: 1.7761 - categorical_accuracy: 0.2540; Current Batch = 158\n",
            "43/67 [==================>...........] - ETA: 11s - loss: 1.7831 - categorical_accuracy: 0.2558; Current Batch = 159\n",
            "44/67 [==================>...........] - ETA: 10s - loss: 1.7792 - categorical_accuracy: 0.2576; Current Batch = 160\n",
            "45/67 [===================>..........] - ETA: 10s - loss: 1.7803 - categorical_accuracy: 0.2667; Current Batch = 161\n",
            "46/67 [===================>..........] - ETA: 9s - loss: 1.7700 - categorical_accuracy: 0.2754 ; Current Batch = 162\n",
            "47/67 [====================>.........] - ETA: 9s - loss: 1.7749 - categorical_accuracy: 0.2695; Current Batch = 163\n",
            "48/67 [====================>.........] - ETA: 8s - loss: 1.7708 - categorical_accuracy: 0.2708; Current Batch = 164\n",
            "49/67 [====================>.........] - ETA: 8s - loss: 1.7638 - categorical_accuracy: 0.2653; Current Batch = 165\n",
            "50/67 [=====================>........] - ETA: 8s - loss: 1.7560 - categorical_accuracy: 0.2667; Current Batch = 166\n",
            "51/67 [=====================>........] - ETA: 7s - loss: 1.7543 - categorical_accuracy: 0.2614; Current Batch = 167\n",
            "52/67 [======================>.......] - ETA: 7s - loss: 1.7592 - categorical_accuracy: 0.2564; Current Batch = 168\n",
            "53/67 [======================>.......] - ETA: 6s - loss: 1.7648 - categorical_accuracy: 0.2579; Current Batch = 169\n",
            "54/67 [=======================>......] - ETA: 6s - loss: 1.7692 - categorical_accuracy: 0.2531; Current Batch = 170\n",
            "55/67 [=======================>......] - ETA: 5s - loss: 1.7801 - categorical_accuracy: 0.2545; Current Batch = 171\n",
            "56/67 [========================>.....] - ETA: 5s - loss: 1.7845 - categorical_accuracy: 0.2560; Current Batch = 172\n",
            "57/67 [========================>.....] - ETA: 4s - loss: 1.7912 - categorical_accuracy: 0.2573; Current Batch = 173\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.7974 - categorical_accuracy: 0.2529; Current Batch = 174\n",
            "59/67 [=========================>....] - ETA: 3s - loss: 1.7834 - categorical_accuracy: 0.2599; Current Batch = 175\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.7791 - categorical_accuracy: 0.2556; Current Batch = 176\n",
            "61/67 [==========================>...] - ETA: 2s - loss: 1.7659 - categorical_accuracy: 0.2623; Current Batch = 177\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.7531 - categorical_accuracy: 0.2688; Current Batch = 178\n",
            "63/67 [===========================>..] - ETA: 1s - loss: 1.7496 - categorical_accuracy: 0.2698; Current Batch = 179\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.7517 - categorical_accuracy: 0.2708; Current Batch = 180\n",
            "65/67 [============================>.] - ETA: 0s - loss: 1.7751 - categorical_accuracy: 0.2718; Current Batch = 181\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.7853 - categorical_accuracy: 0.2727; Current Batch = 182\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.7880 - categorical_accuracy: 0.2687; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "\n",
            "Epoch 4: saving model to model_init_2023-08-3109_43_59.510610/model-00004-1.78795-0.26866-1.64582-0.30000.h5\n",
            "67/67 [==============================] - 53s 807ms/step - loss: 1.7880 - categorical_accuracy: 0.2687 - val_loss: 1.6458 - val_categorical_accuracy: 0.3000 - lr: 0.0010\n",
            "Epoch 5/25\n",
            "; Current Batch = 183\n",
            " 1/67 [..............................] - ETA: 3s - loss: 1.2376 - categorical_accuracy: 0.6667; Current Batch = 184\n",
            " 2/67 [..............................] - ETA: 49s - loss: 1.6934 - categorical_accuracy: 0.5000; Current Batch = 185\n",
            " 3/67 [>.............................] - ETA: 56s - loss: 1.7350 - categorical_accuracy: 0.3333; Current Batch = 186\n",
            " 4/67 [>.............................] - ETA: 45s - loss: 1.9065 - categorical_accuracy: 0.2500; Current Batch = 187\n",
            " 5/67 [=>............................] - ETA: 44s - loss: 1.9563 - categorical_accuracy: 0.2000; Current Batch = 188\n",
            " 6/67 [=>............................] - ETA: 43s - loss: 1.9816 - categorical_accuracy: 0.1667; Current Batch = 189\n",
            " 7/67 [==>...........................] - ETA: 40s - loss: 1.9177 - categorical_accuracy: 0.1905; Current Batch = 190\n",
            " 8/67 [==>...........................] - ETA: 38s - loss: 1.9820 - categorical_accuracy: 0.1667; Current Batch = 191\n",
            " 9/67 [===>..........................] - ETA: 35s - loss: 2.0443 - categorical_accuracy: 0.1481; Current Batch = 192\n",
            "10/67 [===>..........................] - ETA: 32s - loss: 2.0734 - categorical_accuracy: 0.1333; Current Batch = 193\n",
            "11/67 [===>..........................] - ETA: 30s - loss: 2.0333 - categorical_accuracy: 0.1212; Current Batch = 194\n",
            "12/67 [====>.........................] - ETA: 29s - loss: 2.0283 - categorical_accuracy: 0.1111; Current Batch = 195\n",
            "13/67 [====>.........................] - ETA: 27s - loss: 1.9905 - categorical_accuracy: 0.1026; Current Batch = 196\n",
            "14/67 [=====>........................] - ETA: 26s - loss: 1.9622 - categorical_accuracy: 0.1190; Current Batch = 197\n",
            "15/67 [=====>........................] - ETA: 25s - loss: 1.9223 - categorical_accuracy: 0.1333; Current Batch = 198\n",
            "16/67 [======>.......................] - ETA: 25s - loss: 1.9370 - categorical_accuracy: 0.1250; Current Batch = 199\n",
            "17/67 [======>.......................] - ETA: 24s - loss: 1.9063 - categorical_accuracy: 0.1569; Current Batch = 200\n",
            "18/67 [=======>......................] - ETA: 24s - loss: 1.8997 - categorical_accuracy: 0.1667; Current Batch = 201\n",
            "19/67 [=======>......................] - ETA: 23s - loss: 1.8839 - categorical_accuracy: 0.1754; Current Batch = 202\n",
            "20/67 [=======>......................] - ETA: 23s - loss: 1.8421 - categorical_accuracy: 0.1833; Current Batch = 203\n",
            "21/67 [========>.....................] - ETA: 22s - loss: 1.8538 - categorical_accuracy: 0.1905; Current Batch = 204\n",
            "22/67 [========>.....................] - ETA: 21s - loss: 1.8135 - categorical_accuracy: 0.2121; Current Batch = 205\n",
            "23/67 [=========>....................] - ETA: 20s - loss: 1.7940 - categorical_accuracy: 0.2319; Current Batch = 206\n",
            "24/67 [=========>....................] - ETA: 20s - loss: 1.7937 - categorical_accuracy: 0.2222; Current Batch = 207\n",
            "25/67 [==========>...................] - ETA: 20s - loss: 1.7894 - categorical_accuracy: 0.2133; Current Batch = 208\n",
            "26/67 [==========>...................] - ETA: 19s - loss: 1.7616 - categorical_accuracy: 0.2308; Current Batch = 209\n",
            "27/67 [===========>..................] - ETA: 18s - loss: 1.7374 - categorical_accuracy: 0.2346; Current Batch = 210\n",
            "28/67 [===========>..................] - ETA: 17s - loss: 1.7205 - categorical_accuracy: 0.2381; Current Batch = 211\n",
            "29/67 [===========>..................] - ETA: 17s - loss: 1.7166 - categorical_accuracy: 0.2414; Current Batch = 212\n",
            "30/67 [============>.................] - ETA: 17s - loss: 1.7426 - categorical_accuracy: 0.2333; Current Batch = 213\n",
            "31/67 [============>.................] - ETA: 17s - loss: 1.7423 - categorical_accuracy: 0.2258; Current Batch = 214\n",
            "32/67 [=============>................] - ETA: 17s - loss: 1.7569 - categorical_accuracy: 0.2188; Current Batch = 215\n",
            "33/67 [=============>................] - ETA: 16s - loss: 1.7389 - categorical_accuracy: 0.2323; Current Batch = 216\n",
            "34/67 [==============>...............] - ETA: 16s - loss: 1.7228 - categorical_accuracy: 0.2451; Current Batch = 217\n",
            "35/67 [==============>...............] - ETA: 15s - loss: 1.7144 - categorical_accuracy: 0.2571; Current Batch = 218\n",
            "36/67 [===============>..............] - ETA: 15s - loss: 1.7070 - categorical_accuracy: 0.2593; Current Batch = 219\n",
            "37/67 [===============>..............] - ETA: 14s - loss: 1.7004 - categorical_accuracy: 0.2613; Current Batch = 220\n",
            "38/67 [================>.............] - ETA: 14s - loss: 1.7019 - categorical_accuracy: 0.2544; Num of batches = 221\n",
            "; Current Batch = 0\n",
            "39/67 [================>.............] - ETA: 13s - loss: 1.6984 - categorical_accuracy: 0.2564; Current Batch = 1\n",
            "40/67 [================>.............] - ETA: 13s - loss: 1.6870 - categorical_accuracy: 0.2667; Current Batch = 2\n",
            "41/67 [=================>............] - ETA: 12s - loss: 1.7109 - categorical_accuracy: 0.2602; Current Batch = 3\n",
            "42/67 [=================>............] - ETA: 12s - loss: 1.7061 - categorical_accuracy: 0.2619; Current Batch = 4\n",
            "43/67 [==================>...........] - ETA: 11s - loss: 1.7134 - categorical_accuracy: 0.2558; Current Batch = 5\n",
            "44/67 [==================>...........] - ETA: 11s - loss: 1.7065 - categorical_accuracy: 0.2500; Current Batch = 6\n",
            "45/67 [===================>..........] - ETA: 10s - loss: 1.6925 - categorical_accuracy: 0.2593; Current Batch = 7\n",
            "46/67 [===================>..........] - ETA: 9s - loss: 1.6942 - categorical_accuracy: 0.2536 ; Current Batch = 8\n",
            "47/67 [====================>.........] - ETA: 9s - loss: 1.6899 - categorical_accuracy: 0.2553; Current Batch = 9\n",
            "48/67 [====================>.........] - ETA: 9s - loss: 1.6883 - categorical_accuracy: 0.2569; Current Batch = 10\n",
            "49/67 [====================>.........] - ETA: 8s - loss: 1.6968 - categorical_accuracy: 0.2517; Current Batch = 11\n",
            "50/67 [=====================>........] - ETA: 8s - loss: 1.6997 - categorical_accuracy: 0.2533; Current Batch = 12\n",
            "51/67 [=====================>........] - ETA: 7s - loss: 1.6961 - categorical_accuracy: 0.2549; Current Batch = 13\n",
            "52/67 [======================>.......] - ETA: 7s - loss: 1.7156 - categorical_accuracy: 0.2500; Current Batch = 14\n",
            "53/67 [======================>.......] - ETA: 6s - loss: 1.7067 - categorical_accuracy: 0.2579; Current Batch = 15\n",
            "54/67 [=======================>......] - ETA: 6s - loss: 1.7240 - categorical_accuracy: 0.2593; Current Batch = 16\n",
            "55/67 [=======================>......] - ETA: 5s - loss: 1.7339 - categorical_accuracy: 0.2545; Current Batch = 17\n",
            "56/67 [========================>.....] - ETA: 5s - loss: 1.7277 - categorical_accuracy: 0.2560; Current Batch = 18\n",
            "57/67 [========================>.....] - ETA: 4s - loss: 1.7185 - categorical_accuracy: 0.2632; Current Batch = 19\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.7208 - categorical_accuracy: 0.2644; Current Batch = 20\n",
            "59/67 [=========================>....] - ETA: 3s - loss: 1.7075 - categorical_accuracy: 0.2655; Current Batch = 21\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.6975 - categorical_accuracy: 0.2722; Current Batch = 22\n",
            "61/67 [==========================>...] - ETA: 2s - loss: 1.6921 - categorical_accuracy: 0.2787; Current Batch = 23\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.6938 - categorical_accuracy: 0.2796; Current Batch = 24\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.7016 - categorical_accuracy: 0.2751; Current Batch = 25\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.7027 - categorical_accuracy: 0.2760; Current Batch = 26\n",
            "65/67 [============================>.] - ETA: 0s - loss: 1.6973 - categorical_accuracy: 0.2769; Current Batch = 27\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.6996 - categorical_accuracy: 0.2778; Current Batch = 28\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.7046 - categorical_accuracy: 0.2736; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "\n",
            "Epoch 5: saving model to model_init_2023-08-3109_43_59.510610/model-00005-1.70463-0.27363-1.57668-0.38000.h5\n",
            "67/67 [==============================] - 54s 814ms/step - loss: 1.7046 - categorical_accuracy: 0.2736 - val_loss: 1.5767 - val_categorical_accuracy: 0.3800 - lr: 0.0010\n",
            "Epoch 6/25\n",
            "; Current Batch = 29\n",
            " 1/67 [..............................] - ETA: 3s - loss: 1.5054 - categorical_accuracy: 0.0000e+00; Current Batch = 30\n",
            " 2/67 [..............................] - ETA: 22s - loss: 1.2754 - categorical_accuracy: 0.3333   ; Current Batch = 31\n",
            " 3/67 [>.............................] - ETA: 26s - loss: 1.3626 - categorical_accuracy: 0.2222; Current Batch = 32\n",
            " 4/67 [>.............................] - ETA: 30s - loss: 1.2184 - categorical_accuracy: 0.4167; Current Batch = 33\n",
            " 5/67 [=>............................] - ETA: 33s - loss: 1.1868 - categorical_accuracy: 0.4667; Current Batch = 34\n",
            " 6/67 [=>............................] - ETA: 35s - loss: 1.4208 - categorical_accuracy: 0.3889; Current Batch = 35\n",
            " 7/67 [==>...........................] - ETA: 35s - loss: 1.3318 - categorical_accuracy: 0.4762; Current Batch = 36\n",
            " 8/67 [==>...........................] - ETA: 33s - loss: 1.3973 - categorical_accuracy: 0.4583; Current Batch = 37\n",
            " 9/67 [===>..........................] - ETA: 34s - loss: 1.4499 - categorical_accuracy: 0.4074; Current Batch = 38\n",
            "10/67 [===>..........................] - ETA: 34s - loss: 1.4628 - categorical_accuracy: 0.4000; Current Batch = 39\n",
            "11/67 [===>..........................] - ETA: 32s - loss: 1.5073 - categorical_accuracy: 0.3939; Current Batch = 40\n",
            "12/67 [====>.........................] - ETA: 31s - loss: 1.5599 - categorical_accuracy: 0.3611; Current Batch = 41\n",
            "13/67 [====>.........................] - ETA: 31s - loss: 1.5740 - categorical_accuracy: 0.3333; Current Batch = 42\n",
            "14/67 [=====>........................] - ETA: 30s - loss: 1.5826 - categorical_accuracy: 0.3333; Current Batch = 43\n",
            "15/67 [=====>........................] - ETA: 29s - loss: 1.5770 - categorical_accuracy: 0.3333; Current Batch = 44\n",
            "16/67 [======>.......................] - ETA: 28s - loss: 1.6133 - categorical_accuracy: 0.3125; Current Batch = 45\n",
            "17/67 [======>.......................] - ETA: 27s - loss: 1.6156 - categorical_accuracy: 0.3137; Current Batch = 46\n",
            "18/67 [=======>......................] - ETA: 26s - loss: 1.6049 - categorical_accuracy: 0.2963; Current Batch = 47\n",
            "19/67 [=======>......................] - ETA: 25s - loss: 1.6185 - categorical_accuracy: 0.2807; Current Batch = 48\n",
            "20/67 [=======>......................] - ETA: 25s - loss: 1.5700 - categorical_accuracy: 0.3167; Current Batch = 49\n",
            "21/67 [========>.....................] - ETA: 24s - loss: 1.5562 - categorical_accuracy: 0.3175; Current Batch = 50\n",
            "22/67 [========>.....................] - ETA: 23s - loss: 1.5805 - categorical_accuracy: 0.3030; Current Batch = 51\n",
            "23/67 [=========>....................] - ETA: 23s - loss: 1.5988 - categorical_accuracy: 0.3043; Current Batch = 52\n",
            "24/67 [=========>....................] - ETA: 22s - loss: 1.6126 - categorical_accuracy: 0.2917; Current Batch = 53\n",
            "25/67 [==========>...................] - ETA: 22s - loss: 1.6117 - categorical_accuracy: 0.2800; Current Batch = 54\n",
            "26/67 [==========>...................] - ETA: 21s - loss: 1.5979 - categorical_accuracy: 0.2821; Current Batch = 55\n",
            "27/67 [===========>..................] - ETA: 20s - loss: 1.5869 - categorical_accuracy: 0.2840; Current Batch = 56\n",
            "28/67 [===========>..................] - ETA: 20s - loss: 1.5641 - categorical_accuracy: 0.2976; Current Batch = 57\n",
            "29/67 [===========>..................] - ETA: 19s - loss: 1.5697 - categorical_accuracy: 0.2874; Current Batch = 58\n",
            "30/67 [============>.................] - ETA: 19s - loss: 1.5816 - categorical_accuracy: 0.2778; Current Batch = 59\n",
            "31/67 [============>.................] - ETA: 19s - loss: 1.5622 - categorical_accuracy: 0.2903; Current Batch = 60\n",
            "32/67 [=============>................] - ETA: 18s - loss: 1.5428 - categorical_accuracy: 0.2917; Current Batch = 61\n",
            "33/67 [=============>................] - ETA: 18s - loss: 1.5524 - categorical_accuracy: 0.3030; Current Batch = 62\n",
            "34/67 [==============>...............] - ETA: 17s - loss: 1.5531 - categorical_accuracy: 0.2941; Current Batch = 63\n",
            "35/67 [==============>...............] - ETA: 17s - loss: 1.5834 - categorical_accuracy: 0.2857; Current Batch = 64\n",
            "36/67 [===============>..............] - ETA: 16s - loss: 1.5566 - categorical_accuracy: 0.3056; Current Batch = 65\n",
            "37/67 [===============>..............] - ETA: 16s - loss: 1.5680 - categorical_accuracy: 0.3153; Current Batch = 66\n",
            "38/67 [================>.............] - ETA: 15s - loss: 1.5788 - categorical_accuracy: 0.3070; Current Batch = 67\n",
            "39/67 [================>.............] - ETA: 14s - loss: 1.5901 - categorical_accuracy: 0.3077; Current Batch = 68\n",
            "40/67 [================>.............] - ETA: 14s - loss: 1.6161 - categorical_accuracy: 0.3000; Current Batch = 69\n",
            "41/67 [=================>............] - ETA: 13s - loss: 1.6132 - categorical_accuracy: 0.3008; Current Batch = 70\n",
            "42/67 [=================>............] - ETA: 13s - loss: 1.5968 - categorical_accuracy: 0.3016; Current Batch = 71\n",
            "43/67 [==================>...........] - ETA: 12s - loss: 1.5863 - categorical_accuracy: 0.3023; Current Batch = 72\n",
            "44/67 [==================>...........] - ETA: 12s - loss: 1.5909 - categorical_accuracy: 0.2955; Current Batch = 73\n",
            "45/67 [===================>..........] - ETA: 11s - loss: 1.5919 - categorical_accuracy: 0.2963; Current Batch = 74\n",
            "46/67 [===================>..........] - ETA: 11s - loss: 1.6267 - categorical_accuracy: 0.2899; Current Batch = 75\n",
            "47/67 [====================>.........] - ETA: 10s - loss: 1.6315 - categorical_accuracy: 0.2908; Current Batch = 76\n",
            "48/67 [====================>.........] - ETA: 9s - loss: 1.6347 - categorical_accuracy: 0.2917 ; Current Batch = 77\n",
            "49/67 [====================>.........] - ETA: 9s - loss: 1.6384 - categorical_accuracy: 0.2857; Current Batch = 78\n",
            "50/67 [=====================>........] - ETA: 8s - loss: 1.6248 - categorical_accuracy: 0.2933; Current Batch = 79\n",
            "51/67 [=====================>........] - ETA: 8s - loss: 1.6214 - categorical_accuracy: 0.2941; Current Batch = 80\n",
            "52/67 [======================>.......] - ETA: 7s - loss: 1.6206 - categorical_accuracy: 0.2949; Current Batch = 81\n",
            "53/67 [======================>.......] - ETA: 7s - loss: 1.6243 - categorical_accuracy: 0.2956; Current Batch = 82\n",
            "54/67 [=======================>......] - ETA: 6s - loss: 1.6282 - categorical_accuracy: 0.2963; Current Batch = 83\n",
            "55/67 [=======================>......] - ETA: 6s - loss: 1.6193 - categorical_accuracy: 0.3030; Current Batch = 84\n",
            "56/67 [========================>.....] - ETA: 5s - loss: 1.6203 - categorical_accuracy: 0.3036; Current Batch = 85\n",
            "57/67 [========================>.....] - ETA: 5s - loss: 1.6161 - categorical_accuracy: 0.3099; Current Batch = 86\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.6119 - categorical_accuracy: 0.3103; Current Batch = 87\n",
            "59/67 [=========================>....] - ETA: 4s - loss: 1.6177 - categorical_accuracy: 0.3051; Current Batch = 88\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.6142 - categorical_accuracy: 0.3111; Current Batch = 89\n",
            "61/67 [==========================>...] - ETA: 3s - loss: 1.6097 - categorical_accuracy: 0.3060; Current Batch = 90\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.5906 - categorical_accuracy: 0.3172; Current Batch = 91\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.5919 - categorical_accuracy: 0.3122; Current Batch = 92\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.5855 - categorical_accuracy: 0.3125; Current Batch = 93\n",
            "65/67 [============================>.] - ETA: 1s - loss: 1.5686 - categorical_accuracy: 0.3231; Current Batch = 94\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.5653 - categorical_accuracy: 0.3232; Current Batch = 95\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.5579 - categorical_accuracy: 0.3234; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "\n",
            "Epoch 6: saving model to model_init_2023-08-3109_43_59.510610/model-00006-1.55794-0.32338-3.75961-0.27000.h5\n",
            "67/67 [==============================] - 57s 868ms/step - loss: 1.5579 - categorical_accuracy: 0.3234 - val_loss: 3.7596 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
            "Epoch 7/25\n",
            "; Current Batch = 96\n",
            " 1/67 [..............................] - ETA: 3s - loss: 0.9665 - categorical_accuracy: 0.6667; Current Batch = 97\n",
            " 2/67 [..............................] - ETA: 47s - loss: 1.2656 - categorical_accuracy: 0.6667; Current Batch = 98\n",
            " 3/67 [>.............................] - ETA: 45s - loss: 1.5388 - categorical_accuracy: 0.4444; Current Batch = 99\n",
            " 4/67 [>.............................] - ETA: 44s - loss: 1.6546 - categorical_accuracy: 0.4167; Current Batch = 100\n",
            " 5/67 [=>............................] - ETA: 43s - loss: 1.6871 - categorical_accuracy: 0.3333; Current Batch = 101\n",
            " 6/67 [=>............................] - ETA: 40s - loss: 1.5708 - categorical_accuracy: 0.3333; Current Batch = 102\n",
            " 7/67 [==>...........................] - ETA: 38s - loss: 1.4369 - categorical_accuracy: 0.4286; Current Batch = 103\n",
            " 8/67 [==>...........................] - ETA: 35s - loss: 1.5124 - categorical_accuracy: 0.4167; Current Batch = 104\n",
            " 9/67 [===>..........................] - ETA: 35s - loss: 1.6397 - categorical_accuracy: 0.3704; Current Batch = 105\n",
            "10/67 [===>..........................] - ETA: 34s - loss: 1.6488 - categorical_accuracy: 0.3667; Current Batch = 106\n",
            "11/67 [===>..........................] - ETA: 31s - loss: 1.6568 - categorical_accuracy: 0.3636; Current Batch = 107\n",
            "12/67 [====>.........................] - ETA: 30s - loss: 1.6276 - categorical_accuracy: 0.3611; Current Batch = 108\n",
            "13/67 [====>.........................] - ETA: 30s - loss: 1.6287 - categorical_accuracy: 0.3590; Current Batch = 109\n",
            "14/67 [=====>........................] - ETA: 28s - loss: 1.6401 - categorical_accuracy: 0.3571; Current Batch = 110\n",
            "15/67 [=====>........................] - ETA: 27s - loss: 1.5894 - categorical_accuracy: 0.3778; Current Batch = 111\n",
            "16/67 [======>.......................] - ETA: 26s - loss: 1.6032 - categorical_accuracy: 0.3542; Current Batch = 112\n",
            "17/67 [======>.......................] - ETA: 25s - loss: 1.5875 - categorical_accuracy: 0.3725; Current Batch = 113\n",
            "18/67 [=======>......................] - ETA: 24s - loss: 1.6268 - categorical_accuracy: 0.3519; Current Batch = 114\n",
            "19/67 [=======>......................] - ETA: 24s - loss: 1.6102 - categorical_accuracy: 0.3509; Current Batch = 115\n",
            "20/67 [=======>......................] - ETA: 23s - loss: 1.5889 - categorical_accuracy: 0.3500; Current Batch = 116\n",
            "21/67 [========>.....................] - ETA: 22s - loss: 1.5930 - categorical_accuracy: 0.3333; Current Batch = 117\n",
            "22/67 [========>.....................] - ETA: 22s - loss: 1.6554 - categorical_accuracy: 0.3182; Current Batch = 118\n",
            "23/67 [=========>....................] - ETA: 21s - loss: 1.6587 - categorical_accuracy: 0.3188; Current Batch = 119\n",
            "24/67 [=========>....................] - ETA: 20s - loss: 1.6840 - categorical_accuracy: 0.3056; Current Batch = 120\n",
            "25/67 [==========>...................] - ETA: 20s - loss: 1.6648 - categorical_accuracy: 0.3200; Current Batch = 121\n",
            "26/67 [==========>...................] - ETA: 19s - loss: 1.6999 - categorical_accuracy: 0.3077; Current Batch = 122\n",
            "27/67 [===========>..................] - ETA: 19s - loss: 1.7103 - categorical_accuracy: 0.2963; Current Batch = 123\n",
            "28/67 [===========>..................] - ETA: 18s - loss: 1.6849 - categorical_accuracy: 0.3095; Current Batch = 124\n",
            "29/67 [===========>..................] - ETA: 18s - loss: 1.6622 - categorical_accuracy: 0.3218; Current Batch = 125\n",
            "30/67 [============>.................] - ETA: 17s - loss: 1.6572 - categorical_accuracy: 0.3111; Current Batch = 126\n",
            "31/67 [============>.................] - ETA: 17s - loss: 1.6572 - categorical_accuracy: 0.3118; Current Batch = 127\n",
            "32/67 [=============>................] - ETA: 16s - loss: 1.6414 - categorical_accuracy: 0.3125; Current Batch = 128\n",
            "33/67 [=============>................] - ETA: 16s - loss: 1.6548 - categorical_accuracy: 0.3030; Current Batch = 129\n",
            "34/67 [==============>...............] - ETA: 16s - loss: 1.6618 - categorical_accuracy: 0.3039; Current Batch = 130\n",
            "35/67 [==============>...............] - ETA: 16s - loss: 1.6889 - categorical_accuracy: 0.2952; Current Batch = 131\n",
            "36/67 [===============>..............] - ETA: 15s - loss: 1.7027 - categorical_accuracy: 0.2870; Current Batch = 132\n",
            "37/67 [===============>..............] - ETA: 15s - loss: 1.6805 - categorical_accuracy: 0.2883; Current Batch = 133\n",
            "38/67 [================>.............] - ETA: 14s - loss: 1.6861 - categorical_accuracy: 0.2895; Current Batch = 134\n",
            "39/67 [================>.............] - ETA: 14s - loss: 1.7079 - categorical_accuracy: 0.2821; Current Batch = 135\n",
            "40/67 [================>.............] - ETA: 13s - loss: 1.7188 - categorical_accuracy: 0.2833; Current Batch = 136\n",
            "41/67 [=================>............] - ETA: 12s - loss: 1.7297 - categorical_accuracy: 0.2764; Current Batch = 137\n",
            "42/67 [=================>............] - ETA: 12s - loss: 1.7330 - categorical_accuracy: 0.2698; Current Batch = 138\n",
            "43/67 [==================>...........] - ETA: 11s - loss: 1.7409 - categorical_accuracy: 0.2636; Current Batch = 139\n",
            "44/67 [==================>...........] - ETA: 11s - loss: 1.7259 - categorical_accuracy: 0.2727; Current Batch = 140\n",
            "45/67 [===================>..........] - ETA: 10s - loss: 1.7226 - categorical_accuracy: 0.2741; Current Batch = 141\n",
            "46/67 [===================>..........] - ETA: 10s - loss: 1.7171 - categorical_accuracy: 0.2754; Current Batch = 142\n",
            "47/67 [====================>.........] - ETA: 9s - loss: 1.7209 - categorical_accuracy: 0.2695 ; Current Batch = 143\n",
            "48/67 [====================>.........] - ETA: 9s - loss: 1.7241 - categorical_accuracy: 0.2708; Current Batch = 144\n",
            "49/67 [====================>.........] - ETA: 8s - loss: 1.7460 - categorical_accuracy: 0.2653; Current Batch = 145\n",
            "50/67 [=====================>........] - ETA: 8s - loss: 1.7573 - categorical_accuracy: 0.2667; Current Batch = 146\n",
            "51/67 [=====================>........] - ETA: 7s - loss: 1.7756 - categorical_accuracy: 0.2614; Current Batch = 147\n",
            "52/67 [======================>.......] - ETA: 7s - loss: 1.7706 - categorical_accuracy: 0.2628; Current Batch = 148\n",
            "53/67 [======================>.......] - ETA: 6s - loss: 1.7737 - categorical_accuracy: 0.2579; Current Batch = 149\n",
            "54/67 [=======================>......] - ETA: 6s - loss: 1.7617 - categorical_accuracy: 0.2593; Current Batch = 150\n",
            "55/67 [=======================>......] - ETA: 5s - loss: 1.7588 - categorical_accuracy: 0.2545; Current Batch = 151\n",
            "56/67 [========================>.....] - ETA: 5s - loss: 1.7692 - categorical_accuracy: 0.2500; Current Batch = 152\n",
            "57/67 [========================>.....] - ETA: 4s - loss: 1.7738 - categorical_accuracy: 0.2515; Current Batch = 153\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.7774 - categorical_accuracy: 0.2471; Current Batch = 154\n",
            "59/67 [=========================>....] - ETA: 4s - loss: 1.7628 - categorical_accuracy: 0.2542; Current Batch = 155\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.7712 - categorical_accuracy: 0.2500; Current Batch = 156\n",
            "61/67 [==========================>...] - ETA: 3s - loss: 1.7667 - categorical_accuracy: 0.2568; Current Batch = 157\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.7610 - categorical_accuracy: 0.2581; Current Batch = 158\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.7593 - categorical_accuracy: 0.2593; Current Batch = 159\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.7540 - categorical_accuracy: 0.2604; Current Batch = 160\n",
            "65/67 [============================>.] - ETA: 1s - loss: 1.7630 - categorical_accuracy: 0.2564; Current Batch = 161\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.7702 - categorical_accuracy: 0.2576; Current Batch = 162\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.7692 - categorical_accuracy: 0.2587; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "\n",
            "Epoch 7: saving model to model_init_2023-08-3109_43_59.510610/model-00007-1.76919-0.25871-4.85844-0.26000.h5\n",
            "67/67 [==============================] - 54s 822ms/step - loss: 1.7692 - categorical_accuracy: 0.2587 - val_loss: 4.8584 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n",
            "Epoch 8/25\n",
            "; Current Batch = 163\n",
            " 1/67 [..............................] - ETA: 3s - loss: 1.7921 - categorical_accuracy: 0.3333; Current Batch = 164\n",
            " 2/67 [..............................] - ETA: 45s - loss: 1.5852 - categorical_accuracy: 0.3333; Current Batch = 165\n",
            " 3/67 [>.............................] - ETA: 35s - loss: 1.6512 - categorical_accuracy: 0.2222; Current Batch = 166\n",
            " 4/67 [>.............................] - ETA: 38s - loss: 1.5377 - categorical_accuracy: 0.3333; Current Batch = 167\n",
            " 5/67 [=>............................] - ETA: 35s - loss: 1.7535 - categorical_accuracy: 0.2667; Current Batch = 168\n",
            " 6/67 [=>............................] - ETA: 36s - loss: 1.7628 - categorical_accuracy: 0.3333; Current Batch = 169\n",
            " 7/67 [==>...........................] - ETA: 37s - loss: 1.7917 - categorical_accuracy: 0.3333; Current Batch = 170\n",
            " 8/67 [==>...........................] - ETA: 34s - loss: 1.8565 - categorical_accuracy: 0.2917; Current Batch = 171\n",
            " 9/67 [===>..........................] - ETA: 35s - loss: 1.8284 - categorical_accuracy: 0.2963; Current Batch = 172\n",
            "10/67 [===>..........................] - ETA: 33s - loss: 1.8142 - categorical_accuracy: 0.3000; Current Batch = 173\n",
            "11/67 [===>..........................] - ETA: 31s - loss: 1.7474 - categorical_accuracy: 0.3030; Current Batch = 174\n",
            "12/67 [====>.........................] - ETA: 29s - loss: 1.6921 - categorical_accuracy: 0.3333; Current Batch = 175\n",
            "13/67 [====>.........................] - ETA: 28s - loss: 1.6688 - categorical_accuracy: 0.3590; Current Batch = 176\n",
            "14/67 [=====>........................] - ETA: 27s - loss: 1.7432 - categorical_accuracy: 0.3333; Current Batch = 177\n",
            "15/67 [=====>........................] - ETA: 26s - loss: 1.7548 - categorical_accuracy: 0.3111; Current Batch = 178\n",
            "16/67 [======>.......................] - ETA: 26s - loss: 1.7075 - categorical_accuracy: 0.3333; Current Batch = 179\n",
            "17/67 [======>.......................] - ETA: 26s - loss: 1.6905 - categorical_accuracy: 0.3333; Current Batch = 180\n",
            "18/67 [=======>......................] - ETA: 25s - loss: 1.6944 - categorical_accuracy: 0.3333; Current Batch = 181\n",
            "19/67 [=======>......................] - ETA: 24s - loss: 1.6761 - categorical_accuracy: 0.3333; Current Batch = 182\n",
            "20/67 [=======>......................] - ETA: 23s - loss: 1.6780 - categorical_accuracy: 0.3167; Current Batch = 183\n",
            "21/67 [========>.....................] - ETA: 23s - loss: 1.6755 - categorical_accuracy: 0.3016; Current Batch = 184\n",
            "22/67 [========>.....................] - ETA: 22s - loss: 1.6938 - categorical_accuracy: 0.2879; Current Batch = 185\n",
            "23/67 [=========>....................] - ETA: 22s - loss: 1.6721 - categorical_accuracy: 0.3043; Current Batch = 186\n",
            "24/67 [=========>....................] - ETA: 21s - loss: 1.7179 - categorical_accuracy: 0.2917; Current Batch = 187\n",
            "25/67 [==========>...................] - ETA: 20s - loss: 1.7105 - categorical_accuracy: 0.2933; Current Batch = 188\n",
            "26/67 [==========>...................] - ETA: 20s - loss: 1.7115 - categorical_accuracy: 0.2949; Current Batch = 189\n",
            "27/67 [===========>..................] - ETA: 19s - loss: 1.7162 - categorical_accuracy: 0.2963; Current Batch = 190\n",
            "28/67 [===========>..................] - ETA: 19s - loss: 1.6932 - categorical_accuracy: 0.3095; Current Batch = 191\n",
            "29/67 [===========>..................] - ETA: 18s - loss: 1.6698 - categorical_accuracy: 0.3218; Current Batch = 192\n",
            "30/67 [============>.................] - ETA: 18s - loss: 1.6782 - categorical_accuracy: 0.3111; Current Batch = 193\n",
            "31/67 [============>.................] - ETA: 17s - loss: 1.6729 - categorical_accuracy: 0.3118; Current Batch = 194\n",
            "32/67 [=============>................] - ETA: 17s - loss: 1.6631 - categorical_accuracy: 0.3125; Current Batch = 195\n",
            "33/67 [=============>................] - ETA: 17s - loss: 1.6544 - categorical_accuracy: 0.3131; Current Batch = 196\n",
            "34/67 [==============>...............] - ETA: 16s - loss: 1.6351 - categorical_accuracy: 0.3235; Current Batch = 197\n",
            "35/67 [==============>...............] - ETA: 16s - loss: 1.6232 - categorical_accuracy: 0.3238; Current Batch = 198\n",
            "36/67 [===============>..............] - ETA: 16s - loss: 1.6213 - categorical_accuracy: 0.3241; Current Batch = 199\n",
            "37/67 [===============>..............] - ETA: 15s - loss: 1.5992 - categorical_accuracy: 0.3423; Current Batch = 200\n",
            "38/67 [================>.............] - ETA: 15s - loss: 1.5720 - categorical_accuracy: 0.3596; Current Batch = 201\n",
            "39/67 [================>.............] - ETA: 14s - loss: 1.5845 - categorical_accuracy: 0.3504; Current Batch = 202\n",
            "40/67 [================>.............] - ETA: 14s - loss: 1.5937 - categorical_accuracy: 0.3583; Current Batch = 203\n",
            "41/67 [=================>............] - ETA: 13s - loss: 1.6021 - categorical_accuracy: 0.3496; Current Batch = 204\n",
            "42/67 [=================>............] - ETA: 12s - loss: 1.6175 - categorical_accuracy: 0.3492; Current Batch = 205\n",
            "43/67 [==================>...........] - ETA: 12s - loss: 1.6194 - categorical_accuracy: 0.3488; Current Batch = 206\n",
            "44/67 [==================>...........] - ETA: 11s - loss: 1.6343 - categorical_accuracy: 0.3409; Current Batch = 207\n",
            "45/67 [===================>..........] - ETA: 11s - loss: 1.6184 - categorical_accuracy: 0.3481; Current Batch = 208\n",
            "46/67 [===================>..........] - ETA: 10s - loss: 1.6232 - categorical_accuracy: 0.3406; Current Batch = 209\n",
            "47/67 [====================>.........] - ETA: 10s - loss: 1.6423 - categorical_accuracy: 0.3333; Current Batch = 210\n",
            "48/67 [====================>.........] - ETA: 9s - loss: 1.6457 - categorical_accuracy: 0.3264 ; Current Batch = 211\n",
            "49/67 [====================>.........] - ETA: 9s - loss: 1.6476 - categorical_accuracy: 0.3265; Current Batch = 212\n",
            "50/67 [=====================>........] - ETA: 8s - loss: 1.6444 - categorical_accuracy: 0.3267; Current Batch = 213\n",
            "51/67 [=====================>........] - ETA: 8s - loss: 1.6407 - categorical_accuracy: 0.3268; Current Batch = 214\n",
            "52/67 [======================>.......] - ETA: 7s - loss: 1.6343 - categorical_accuracy: 0.3269; Current Batch = 215\n",
            "53/67 [======================>.......] - ETA: 7s - loss: 1.6419 - categorical_accuracy: 0.3208; Current Batch = 216\n",
            "54/67 [=======================>......] - ETA: 6s - loss: 1.6429 - categorical_accuracy: 0.3272; Current Batch = 217\n",
            "55/67 [=======================>......] - ETA: 6s - loss: 1.6361 - categorical_accuracy: 0.3273; Current Batch = 218\n",
            "56/67 [========================>.....] - ETA: 5s - loss: 1.6331 - categorical_accuracy: 0.3274; Current Batch = 219\n",
            "57/67 [========================>.....] - ETA: 5s - loss: 1.6496 - categorical_accuracy: 0.3275; Current Batch = 220\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.6477 - categorical_accuracy: 0.3276; Num of batches = 221\n",
            "; Current Batch = 0\n",
            "59/67 [=========================>....] - ETA: 4s - loss: 1.6485 - categorical_accuracy: 0.3277; Current Batch = 1\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.6463 - categorical_accuracy: 0.3278; Current Batch = 2\n",
            "61/67 [==========================>...] - ETA: 3s - loss: 1.6499 - categorical_accuracy: 0.3279; Current Batch = 3\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.6634 - categorical_accuracy: 0.3226; Current Batch = 4\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.6635 - categorical_accuracy: 0.3228; Current Batch = 5\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.6731 - categorical_accuracy: 0.3177; Current Batch = 6\n",
            "65/67 [============================>.] - ETA: 1s - loss: 1.6766 - categorical_accuracy: 0.3128; Current Batch = 7\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.6693 - categorical_accuracy: 0.3182; Current Batch = 8\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.6662 - categorical_accuracy: 0.3134; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "\n",
            "Epoch 8: saving model to model_init_2023-08-3109_43_59.510610/model-00008-1.66618-0.31343-2.55280-0.28000.h5\n",
            "67/67 [==============================] - 55s 827ms/step - loss: 1.6662 - categorical_accuracy: 0.3134 - val_loss: 2.5528 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
            "Epoch 9/25\n",
            "; Current Batch = 9\n",
            " 1/67 [..............................] - ETA: 3s - loss: 1.5054 - categorical_accuracy: 0.3333; Current Batch = 10\n",
            " 2/67 [..............................] - ETA: 45s - loss: 1.2528 - categorical_accuracy: 0.5000; Current Batch = 11\n",
            " 3/67 [>.............................] - ETA: 38s - loss: 1.4197 - categorical_accuracy: 0.4444; Current Batch = 12\n",
            " 4/67 [>.............................] - ETA: 32s - loss: 1.5458 - categorical_accuracy: 0.3333; Current Batch = 13\n",
            " 5/67 [=>............................] - ETA: 35s - loss: 1.5875 - categorical_accuracy: 0.3333; Current Batch = 14\n",
            " 6/67 [=>............................] - ETA: 36s - loss: 1.4932 - categorical_accuracy: 0.3333; Current Batch = 15\n",
            " 7/67 [==>...........................] - ETA: 34s - loss: 1.4676 - categorical_accuracy: 0.3333; Current Batch = 16\n",
            " 8/67 [==>...........................] - ETA: 32s - loss: 1.4483 - categorical_accuracy: 0.3333; Current Batch = 17\n",
            " 9/67 [===>..........................] - ETA: 31s - loss: 1.4703 - categorical_accuracy: 0.3333; Current Batch = 18\n",
            "10/67 [===>..........................] - ETA: 33s - loss: 1.5736 - categorical_accuracy: 0.3000; Current Batch = 19\n",
            "11/67 [===>..........................] - ETA: 32s - loss: 1.5453 - categorical_accuracy: 0.3030; Current Batch = 20\n",
            "12/67 [====>.........................] - ETA: 32s - loss: 1.4905 - categorical_accuracy: 0.3611; Current Batch = 21\n",
            "13/67 [====>.........................] - ETA: 31s - loss: 1.5384 - categorical_accuracy: 0.3333; Current Batch = 22\n",
            "14/67 [=====>........................] - ETA: 31s - loss: 1.5168 - categorical_accuracy: 0.3333; Current Batch = 23\n",
            "15/67 [=====>........................] - ETA: 30s - loss: 1.5024 - categorical_accuracy: 0.3333; Current Batch = 24\n",
            "16/67 [======>.......................] - ETA: 29s - loss: 1.5147 - categorical_accuracy: 0.3125; Current Batch = 25\n",
            "17/67 [======>.......................] - ETA: 29s - loss: 1.5064 - categorical_accuracy: 0.3137; Current Batch = 26\n",
            "18/67 [=======>......................] - ETA: 28s - loss: 1.5040 - categorical_accuracy: 0.3148; Current Batch = 27\n",
            "19/67 [=======>......................] - ETA: 26s - loss: 1.4809 - categorical_accuracy: 0.3158; Current Batch = 28\n",
            "20/67 [=======>......................] - ETA: 26s - loss: 1.5077 - categorical_accuracy: 0.3167; Current Batch = 29\n",
            "21/67 [========>.....................] - ETA: 25s - loss: 1.4962 - categorical_accuracy: 0.3175; Current Batch = 30\n",
            "22/67 [========>.....................] - ETA: 24s - loss: 1.5234 - categorical_accuracy: 0.3030; Current Batch = 31\n",
            "23/67 [=========>....................] - ETA: 23s - loss: 1.5173 - categorical_accuracy: 0.3043; Current Batch = 32\n",
            "24/67 [=========>....................] - ETA: 22s - loss: 1.5280 - categorical_accuracy: 0.3056; Current Batch = 33\n",
            "25/67 [==========>...................] - ETA: 21s - loss: 1.5013 - categorical_accuracy: 0.3333; Current Batch = 34\n",
            "26/67 [==========>...................] - ETA: 21s - loss: 1.4889 - categorical_accuracy: 0.3333; Current Batch = 35\n",
            "27/67 [===========>..................] - ETA: 20s - loss: 1.4811 - categorical_accuracy: 0.3457; Current Batch = 36\n",
            "28/67 [===========>..................] - ETA: 20s - loss: 1.4919 - categorical_accuracy: 0.3452; Current Batch = 37\n",
            "29/67 [===========>..................] - ETA: 19s - loss: 1.4830 - categorical_accuracy: 0.3563; Current Batch = 38\n",
            "30/67 [============>.................] - ETA: 18s - loss: 1.4835 - categorical_accuracy: 0.3556; Current Batch = 39\n",
            "31/67 [============>.................] - ETA: 18s - loss: 1.4826 - categorical_accuracy: 0.3548; Current Batch = 40\n",
            "32/67 [=============>................] - ETA: 18s - loss: 1.4800 - categorical_accuracy: 0.3542; Current Batch = 41\n",
            "33/67 [=============>................] - ETA: 17s - loss: 1.5045 - categorical_accuracy: 0.3434; Current Batch = 42\n",
            "34/67 [==============>...............] - ETA: 16s - loss: 1.5235 - categorical_accuracy: 0.3333; Current Batch = 43\n",
            "35/67 [==============>...............] - ETA: 16s - loss: 1.5127 - categorical_accuracy: 0.3333; Current Batch = 44\n",
            "36/67 [===============>..............] - ETA: 16s - loss: 1.4986 - categorical_accuracy: 0.3426; Current Batch = 45\n",
            "37/67 [===============>..............] - ETA: 15s - loss: 1.5009 - categorical_accuracy: 0.3333; Current Batch = 46\n",
            "38/67 [================>.............] - ETA: 15s - loss: 1.5060 - categorical_accuracy: 0.3246; Current Batch = 47\n",
            "39/67 [================>.............] - ETA: 14s - loss: 1.5031 - categorical_accuracy: 0.3162; Current Batch = 48\n",
            "40/67 [================>.............] - ETA: 14s - loss: 1.4892 - categorical_accuracy: 0.3250; Current Batch = 49\n",
            "41/67 [=================>............] - ETA: 13s - loss: 1.4973 - categorical_accuracy: 0.3252; Current Batch = 50\n",
            "42/67 [=================>............] - ETA: 13s - loss: 1.4996 - categorical_accuracy: 0.3175; Current Batch = 51\n",
            "43/67 [==================>...........] - ETA: 12s - loss: 1.4897 - categorical_accuracy: 0.3101; Current Batch = 52\n",
            "44/67 [==================>...........] - ETA: 11s - loss: 1.4815 - categorical_accuracy: 0.3182; Current Batch = 53\n",
            "45/67 [===================>..........] - ETA: 11s - loss: 1.4947 - categorical_accuracy: 0.3111; Current Batch = 54\n",
            "46/67 [===================>..........] - ETA: 10s - loss: 1.5031 - categorical_accuracy: 0.3116; Current Batch = 55\n",
            "47/67 [====================>.........] - ETA: 10s - loss: 1.4971 - categorical_accuracy: 0.3191; Current Batch = 56\n",
            "48/67 [====================>.........] - ETA: 9s - loss: 1.4848 - categorical_accuracy: 0.3264 ; Current Batch = 57\n",
            "49/67 [====================>.........] - ETA: 9s - loss: 1.5089 - categorical_accuracy: 0.3197; Current Batch = 58\n",
            "50/67 [=====================>........] - ETA: 8s - loss: 1.5110 - categorical_accuracy: 0.3267; Current Batch = 59\n",
            "51/67 [=====================>........] - ETA: 8s - loss: 1.5061 - categorical_accuracy: 0.3203; Current Batch = 60\n",
            "52/67 [======================>.......] - ETA: 7s - loss: 1.5165 - categorical_accuracy: 0.3205; Current Batch = 61\n",
            "53/67 [======================>.......] - ETA: 7s - loss: 1.5169 - categorical_accuracy: 0.3208; Current Batch = 62\n",
            "54/67 [=======================>......] - ETA: 6s - loss: 1.5165 - categorical_accuracy: 0.3148; Current Batch = 63\n",
            "55/67 [=======================>......] - ETA: 6s - loss: 1.5101 - categorical_accuracy: 0.3152; Current Batch = 64\n",
            "56/67 [========================>.....] - ETA: 5s - loss: 1.5189 - categorical_accuracy: 0.3095; Current Batch = 65\n",
            "57/67 [========================>.....] - ETA: 5s - loss: 1.5148 - categorical_accuracy: 0.3158; Current Batch = 66\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.5082 - categorical_accuracy: 0.3218; Current Batch = 67\n",
            "59/67 [=========================>....] - ETA: 4s - loss: 1.5068 - categorical_accuracy: 0.3220; Current Batch = 68\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.4899 - categorical_accuracy: 0.3333; Current Batch = 69\n",
            "61/67 [==========================>...] - ETA: 3s - loss: 1.4877 - categorical_accuracy: 0.3333; Current Batch = 70\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.4966 - categorical_accuracy: 0.3280; Current Batch = 71\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.4899 - categorical_accuracy: 0.3333; Current Batch = 72\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.4938 - categorical_accuracy: 0.3333; Current Batch = 73\n",
            "65/67 [============================>.] - ETA: 1s - loss: 1.4885 - categorical_accuracy: 0.3385; Current Batch = 74\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.4947 - categorical_accuracy: 0.3333; Current Batch = 75\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.5089 - categorical_accuracy: 0.3333; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "\n",
            "Epoch 9: saving model to model_init_2023-08-3109_43_59.510610/model-00009-1.50887-0.33333-1.69189-0.39000.h5\n",
            "\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "67/67 [==============================] - 77s 1s/step - loss: 1.5089 - categorical_accuracy: 0.3333 - val_loss: 1.6919 - val_categorical_accuracy: 0.3900 - lr: 0.0010\n",
            "Epoch 10/25\n",
            "; Current Batch = 76\n",
            " 1/67 [..............................] - ETA: 3s - loss: 2.8837 - categorical_accuracy: 0.0000e+00; Current Batch = 77\n",
            " 2/67 [..............................] - ETA: 21s - loss: 1.9973 - categorical_accuracy: 0.3333   ; Current Batch = 78\n",
            " 3/67 [>.............................] - ETA: 21s - loss: 1.9350 - categorical_accuracy: 0.2222; Current Batch = 79\n",
            " 4/67 [>.............................] - ETA: 24s - loss: 1.7357 - categorical_accuracy: 0.2500; Current Batch = 80\n",
            " 5/67 [=>............................] - ETA: 28s - loss: 1.6048 - categorical_accuracy: 0.3333; Current Batch = 81\n",
            " 6/67 [=>............................] - ETA: 32s - loss: 1.6212 - categorical_accuracy: 0.2778; Current Batch = 82\n",
            " 7/67 [==>...........................] - ETA: 34s - loss: 1.6466 - categorical_accuracy: 0.2381; Current Batch = 83\n",
            " 8/67 [==>...........................] - ETA: 32s - loss: 1.5020 - categorical_accuracy: 0.3333; Current Batch = 84\n",
            " 9/67 [===>..........................] - ETA: 31s - loss: 1.5437 - categorical_accuracy: 0.3333; Current Batch = 85\n",
            "10/67 [===>..........................] - ETA: 32s - loss: 1.5246 - categorical_accuracy: 0.3333; Current Batch = 86\n",
            "11/67 [===>..........................] - ETA: 30s - loss: 1.5134 - categorical_accuracy: 0.3333; Current Batch = 87\n",
            "12/67 [====>.........................] - ETA: 29s - loss: 1.5247 - categorical_accuracy: 0.3333; Current Batch = 88\n",
            "13/67 [====>.........................] - ETA: 28s - loss: 1.6232 - categorical_accuracy: 0.3077; Current Batch = 89\n",
            "14/67 [=====>........................] - ETA: 28s - loss: 1.5815 - categorical_accuracy: 0.3571; Current Batch = 90\n",
            "15/67 [=====>........................] - ETA: 29s - loss: 1.5815 - categorical_accuracy: 0.3778; Current Batch = 91\n",
            "16/67 [======>.......................] - ETA: 28s - loss: 1.5642 - categorical_accuracy: 0.3958; Current Batch = 92\n",
            "17/67 [======>.......................] - ETA: 29s - loss: 1.5805 - categorical_accuracy: 0.3725; Current Batch = 93\n",
            "18/67 [=======>......................] - ETA: 28s - loss: 1.5603 - categorical_accuracy: 0.3704; Current Batch = 94\n",
            "19/67 [=======>......................] - ETA: 28s - loss: 1.5919 - categorical_accuracy: 0.3684; Current Batch = 95\n",
            "20/67 [=======>......................] - ETA: 27s - loss: 1.6043 - categorical_accuracy: 0.3500; Current Batch = 96\n",
            "21/67 [========>.....................] - ETA: 27s - loss: 1.6047 - categorical_accuracy: 0.3492; Current Batch = 97\n",
            "22/67 [========>.....................] - ETA: 26s - loss: 1.6132 - categorical_accuracy: 0.3485; Current Batch = 98\n",
            "23/67 [=========>....................] - ETA: 26s - loss: 1.6260 - categorical_accuracy: 0.3478; Current Batch = 99\n",
            "24/67 [=========>....................] - ETA: 25s - loss: 1.6204 - categorical_accuracy: 0.3472; Current Batch = 100\n",
            "25/67 [==========>...................] - ETA: 24s - loss: 1.6250 - categorical_accuracy: 0.3467; Current Batch = 101\n",
            "26/67 [==========>...................] - ETA: 23s - loss: 1.6691 - categorical_accuracy: 0.3333; Current Batch = 102\n",
            "27/67 [===========>..................] - ETA: 23s - loss: 1.6926 - categorical_accuracy: 0.3333; Current Batch = 103\n",
            "28/67 [===========>..................] - ETA: 23s - loss: 1.6911 - categorical_accuracy: 0.3214; Current Batch = 104\n",
            "29/67 [===========>..................] - ETA: 22s - loss: 1.6907 - categorical_accuracy: 0.3103; Current Batch = 105\n",
            "30/67 [============>.................] - ETA: 21s - loss: 1.6725 - categorical_accuracy: 0.3222; Current Batch = 106\n",
            "31/67 [============>.................] - ETA: 20s - loss: 1.6860 - categorical_accuracy: 0.3118; Current Batch = 107\n",
            "32/67 [=============>................] - ETA: 19s - loss: 1.6729 - categorical_accuracy: 0.3125; Current Batch = 108\n",
            "33/67 [=============>................] - ETA: 18s - loss: 1.6668 - categorical_accuracy: 0.3030; Current Batch = 109\n",
            "34/67 [==============>...............] - ETA: 18s - loss: 1.6509 - categorical_accuracy: 0.3137; Current Batch = 110\n",
            "35/67 [==============>...............] - ETA: 17s - loss: 1.6368 - categorical_accuracy: 0.3143; Current Batch = 111\n",
            "36/67 [===============>..............] - ETA: 16s - loss: 1.6361 - categorical_accuracy: 0.3148; Current Batch = 112\n",
            "37/67 [===============>..............] - ETA: 16s - loss: 1.6366 - categorical_accuracy: 0.3153; Current Batch = 113\n",
            "38/67 [================>.............] - ETA: 15s - loss: 1.6267 - categorical_accuracy: 0.3158; Current Batch = 114\n",
            "39/67 [================>.............] - ETA: 15s - loss: 1.6199 - categorical_accuracy: 0.3248; Current Batch = 115\n",
            "40/67 [================>.............] - ETA: 14s - loss: 1.6084 - categorical_accuracy: 0.3250; Current Batch = 116\n",
            "41/67 [=================>............] - ETA: 14s - loss: 1.5902 - categorical_accuracy: 0.3252; Current Batch = 117\n",
            "42/67 [=================>............] - ETA: 13s - loss: 1.6164 - categorical_accuracy: 0.3175; Current Batch = 118\n",
            "43/67 [==================>...........] - ETA: 13s - loss: 1.6242 - categorical_accuracy: 0.3178; Current Batch = 119\n",
            "44/67 [==================>...........] - ETA: 12s - loss: 1.6212 - categorical_accuracy: 0.3182; Current Batch = 120\n",
            "45/67 [===================>..........] - ETA: 11s - loss: 1.6129 - categorical_accuracy: 0.3185; Current Batch = 121\n",
            "46/67 [===================>..........] - ETA: 11s - loss: 1.6195 - categorical_accuracy: 0.3188; Current Batch = 122\n",
            "47/67 [====================>.........] - ETA: 10s - loss: 1.6063 - categorical_accuracy: 0.3262; Current Batch = 123\n",
            "48/67 [====================>.........] - ETA: 10s - loss: 1.6245 - categorical_accuracy: 0.3194; Current Batch = 124\n",
            "49/67 [====================>.........] - ETA: 9s - loss: 1.6065 - categorical_accuracy: 0.3265 ; Current Batch = 125\n",
            "50/67 [=====================>........] - ETA: 9s - loss: 1.6040 - categorical_accuracy: 0.3267; Current Batch = 126\n",
            "51/67 [=====================>........] - ETA: 8s - loss: 1.5983 - categorical_accuracy: 0.3333; Current Batch = 127\n",
            "52/67 [======================>.......] - ETA: 8s - loss: 1.5968 - categorical_accuracy: 0.3333; Current Batch = 128\n",
            "53/67 [======================>.......] - ETA: 7s - loss: 1.5969 - categorical_accuracy: 0.3333; Current Batch = 129\n",
            "54/67 [=======================>......] - ETA: 7s - loss: 1.5941 - categorical_accuracy: 0.3333; Current Batch = 130\n",
            "55/67 [=======================>......] - ETA: 6s - loss: 1.6110 - categorical_accuracy: 0.3273; Current Batch = 131\n",
            "56/67 [========================>.....] - ETA: 5s - loss: 1.6066 - categorical_accuracy: 0.3274; Current Batch = 132\n",
            "57/67 [========================>.....] - ETA: 5s - loss: 1.5971 - categorical_accuracy: 0.3392; Current Batch = 133\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.5935 - categorical_accuracy: 0.3391; Current Batch = 134\n",
            "59/67 [=========================>....] - ETA: 4s - loss: 1.5914 - categorical_accuracy: 0.3390; Current Batch = 135\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.6045 - categorical_accuracy: 0.3333; Current Batch = 136\n",
            "61/67 [==========================>...] - ETA: 3s - loss: 1.5951 - categorical_accuracy: 0.3388; Current Batch = 137\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.5989 - categorical_accuracy: 0.3387; Current Batch = 138\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.5974 - categorical_accuracy: 0.3439; Current Batch = 139\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.5886 - categorical_accuracy: 0.3438; Current Batch = 140\n",
            "65/67 [============================>.] - ETA: 1s - loss: 1.5893 - categorical_accuracy: 0.3436; Current Batch = 141\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.5847 - categorical_accuracy: 0.3485; Current Batch = 142\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.5940 - categorical_accuracy: 0.3483; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "\n",
            "Epoch 10: saving model to model_init_2023-08-3109_43_59.510610/model-00010-1.59397-0.34826-1.45111-0.42000.h5\n",
            "67/67 [==============================] - 76s 1s/step - loss: 1.5940 - categorical_accuracy: 0.3483 - val_loss: 1.4511 - val_categorical_accuracy: 0.4200 - lr: 2.0000e-04\n",
            "Epoch 11/25\n",
            "; Current Batch = 143\n",
            " 1/67 [..............................] - ETA: 3s - loss: 1.4530 - categorical_accuracy: 0.6667; Current Batch = 144\n",
            " 2/67 [..............................] - ETA: 24s - loss: 1.9041 - categorical_accuracy: 0.3333; Current Batch = 145\n",
            " 3/67 [>.............................] - ETA: 28s - loss: 1.8102 - categorical_accuracy: 0.4444; Current Batch = 146\n",
            " 4/67 [>.............................] - ETA: 30s - loss: 1.7665 - categorical_accuracy: 0.4167; Current Batch = 147\n",
            " 5/67 [=>............................] - ETA: 33s - loss: 1.8259 - categorical_accuracy: 0.4000; Current Batch = 148\n",
            " 6/67 [=>............................] - ETA: 30s - loss: 1.7056 - categorical_accuracy: 0.4444; Current Batch = 149\n",
            " 7/67 [==>...........................] - ETA: 30s - loss: 1.7084 - categorical_accuracy: 0.3810; Current Batch = 150\n",
            " 8/67 [==>...........................] - ETA: 27s - loss: 1.6641 - categorical_accuracy: 0.3750; Current Batch = 151\n",
            " 9/67 [===>..........................] - ETA: 29s - loss: 1.6782 - categorical_accuracy: 0.3704; Current Batch = 152\n",
            "10/67 [===>..........................] - ETA: 28s - loss: 1.6982 - categorical_accuracy: 0.3667; Current Batch = 153\n",
            "11/67 [===>..........................] - ETA: 29s - loss: 1.6980 - categorical_accuracy: 0.3333; Current Batch = 154\n",
            "12/67 [====>.........................] - ETA: 29s - loss: 1.6952 - categorical_accuracy: 0.3333; Current Batch = 155\n",
            "13/67 [====>.........................] - ETA: 28s - loss: 1.7051 - categorical_accuracy: 0.3333; Current Batch = 156\n",
            "14/67 [=====>........................] - ETA: 28s - loss: 1.6745 - categorical_accuracy: 0.3333; Current Batch = 157\n",
            "15/67 [=====>........................] - ETA: 27s - loss: 1.7220 - categorical_accuracy: 0.3111; Current Batch = 158\n",
            "16/67 [======>.......................] - ETA: 26s - loss: 1.6664 - categorical_accuracy: 0.3542; Current Batch = 159\n",
            "17/67 [======>.......................] - ETA: 26s - loss: 1.6957 - categorical_accuracy: 0.3333; Current Batch = 160\n",
            "18/67 [=======>......................] - ETA: 25s - loss: 1.7193 - categorical_accuracy: 0.3333; Current Batch = 161\n",
            "19/67 [=======>......................] - ETA: 25s - loss: 1.7172 - categorical_accuracy: 0.3509; Current Batch = 162\n",
            "20/67 [=======>......................] - ETA: 24s - loss: 1.7151 - categorical_accuracy: 0.3500; Current Batch = 163\n",
            "21/67 [========>.....................] - ETA: 24s - loss: 1.6928 - categorical_accuracy: 0.3492; Current Batch = 164\n",
            "22/67 [========>.....................] - ETA: 25s - loss: 1.6946 - categorical_accuracy: 0.3636; Current Batch = 165\n",
            "23/67 [=========>....................] - ETA: 25s - loss: 1.6830 - categorical_accuracy: 0.3478; Current Batch = 166\n",
            "24/67 [=========>....................] - ETA: 25s - loss: 1.6825 - categorical_accuracy: 0.3333; Current Batch = 167\n",
            "25/67 [==========>...................] - ETA: 24s - loss: 1.6933 - categorical_accuracy: 0.3333; Current Batch = 168\n",
            "26/67 [==========>...................] - ETA: 23s - loss: 1.6678 - categorical_accuracy: 0.3462; Current Batch = 169\n",
            "27/67 [===========>..................] - ETA: 23s - loss: 1.6379 - categorical_accuracy: 0.3580; Current Batch = 170\n",
            "28/67 [===========>..................] - ETA: 22s - loss: 1.6574 - categorical_accuracy: 0.3452; Current Batch = 171\n",
            "29/67 [===========>..................] - ETA: 21s - loss: 1.6597 - categorical_accuracy: 0.3448; Current Batch = 172\n",
            "30/67 [============>.................] - ETA: 21s - loss: 1.6501 - categorical_accuracy: 0.3444; Current Batch = 173\n",
            "31/67 [============>.................] - ETA: 20s - loss: 1.6422 - categorical_accuracy: 0.3441; Current Batch = 174\n",
            "32/67 [=============>................] - ETA: 20s - loss: 1.6361 - categorical_accuracy: 0.3438; Current Batch = 175\n",
            "33/67 [=============>................] - ETA: 19s - loss: 1.6379 - categorical_accuracy: 0.3434; Current Batch = 176\n",
            "34/67 [==============>...............] - ETA: 18s - loss: 1.6482 - categorical_accuracy: 0.3333; Current Batch = 177\n",
            "35/67 [==============>...............] - ETA: 18s - loss: 1.6530 - categorical_accuracy: 0.3333; Current Batch = 178\n",
            "36/67 [===============>..............] - ETA: 17s - loss: 1.6487 - categorical_accuracy: 0.3426; Current Batch = 179\n",
            "37/67 [===============>..............] - ETA: 17s - loss: 1.6431 - categorical_accuracy: 0.3423; Current Batch = 180\n",
            "38/67 [================>.............] - ETA: 16s - loss: 1.6537 - categorical_accuracy: 0.3421; Current Batch = 181\n",
            "39/67 [================>.............] - ETA: 16s - loss: 1.6497 - categorical_accuracy: 0.3504; Current Batch = 182\n",
            "40/67 [================>.............] - ETA: 15s - loss: 1.6451 - categorical_accuracy: 0.3583; Current Batch = 183\n",
            "41/67 [=================>............] - ETA: 14s - loss: 1.6251 - categorical_accuracy: 0.3659; Current Batch = 184\n",
            "42/67 [=================>............] - ETA: 14s - loss: 1.6176 - categorical_accuracy: 0.3651; Current Batch = 185\n",
            "43/67 [==================>...........] - ETA: 13s - loss: 1.6177 - categorical_accuracy: 0.3643; Current Batch = 186\n",
            "44/67 [==================>...........] - ETA: 13s - loss: 1.6244 - categorical_accuracy: 0.3561; Current Batch = 187\n",
            "45/67 [===================>..........] - ETA: 12s - loss: 1.6185 - categorical_accuracy: 0.3556; Current Batch = 188\n",
            "46/67 [===================>..........] - ETA: 12s - loss: 1.6121 - categorical_accuracy: 0.3551; Current Batch = 189\n",
            "47/67 [====================>.........] - ETA: 11s - loss: 1.6093 - categorical_accuracy: 0.3546; Current Batch = 190\n",
            "48/67 [====================>.........] - ETA: 10s - loss: 1.6221 - categorical_accuracy: 0.3472; Current Batch = 191\n",
            "49/67 [====================>.........] - ETA: 10s - loss: 1.6213 - categorical_accuracy: 0.3469; Current Batch = 192\n",
            "50/67 [=====================>........] - ETA: 9s - loss: 1.6145 - categorical_accuracy: 0.3467 ; Current Batch = 193\n",
            "51/67 [=====================>........] - ETA: 9s - loss: 1.6206 - categorical_accuracy: 0.3399; Current Batch = 194\n",
            "52/67 [======================>.......] - ETA: 8s - loss: 1.6165 - categorical_accuracy: 0.3397; Current Batch = 195\n",
            "53/67 [======================>.......] - ETA: 7s - loss: 1.6145 - categorical_accuracy: 0.3333; Current Batch = 196\n",
            "54/67 [=======================>......] - ETA: 7s - loss: 1.6122 - categorical_accuracy: 0.3333; Current Batch = 197\n",
            "55/67 [=======================>......] - ETA: 6s - loss: 1.6095 - categorical_accuracy: 0.3333; Current Batch = 198\n",
            "56/67 [========================>.....] - ETA: 6s - loss: 1.5971 - categorical_accuracy: 0.3393; Current Batch = 199\n",
            "57/67 [========================>.....] - ETA: 5s - loss: 1.5916 - categorical_accuracy: 0.3392; Current Batch = 200\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.5931 - categorical_accuracy: 0.3333; Current Batch = 201\n",
            "59/67 [=========================>....] - ETA: 4s - loss: 1.5975 - categorical_accuracy: 0.3277; Current Batch = 202\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.5862 - categorical_accuracy: 0.3333; Current Batch = 203\n",
            "61/67 [==========================>...] - ETA: 3s - loss: 1.5856 - categorical_accuracy: 0.3333; Current Batch = 204\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.5762 - categorical_accuracy: 0.3387; Current Batch = 205\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.5896 - categorical_accuracy: 0.3333; Current Batch = 206\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.5944 - categorical_accuracy: 0.3333; Current Batch = 207\n",
            "65/67 [============================>.] - ETA: 1s - loss: 1.5823 - categorical_accuracy: 0.3333; Current Batch = 208\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.5856 - categorical_accuracy: 0.3283; Current Batch = 209\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.5778 - categorical_accuracy: 0.3333; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "\n",
            "Epoch 11: saving model to model_init_2023-08-3109_43_59.510610/model-00011-1.57776-0.33333-1.57045-0.41000.h5\n",
            "67/67 [==============================] - 57s 866ms/step - loss: 1.5778 - categorical_accuracy: 0.3333 - val_loss: 1.5705 - val_categorical_accuracy: 0.4100 - lr: 2.0000e-04\n",
            "Epoch 12/25\n",
            "; Current Batch = 210\n",
            " 1/67 [..............................] - ETA: 4s - loss: 1.8903 - categorical_accuracy: 0.0000e+00; Current Batch = 211\n",
            " 2/67 [..............................] - ETA: 34s - loss: 1.5027 - categorical_accuracy: 0.1667   ; Current Batch = 212\n",
            " 3/67 [>.............................] - ETA: 27s - loss: 1.6725 - categorical_accuracy: 0.1111; Current Batch = 213\n",
            " 4/67 [>.............................] - ETA: 29s - loss: 1.7114 - categorical_accuracy: 0.0833; Current Batch = 214\n",
            " 5/67 [=>............................] - ETA: 29s - loss: 1.6544 - categorical_accuracy: 0.1333; Current Batch = 215\n",
            " 6/67 [=>............................] - ETA: 32s - loss: 1.8035 - categorical_accuracy: 0.1111; Current Batch = 216\n",
            " 7/67 [==>...........................] - ETA: 31s - loss: 1.8223 - categorical_accuracy: 0.0952; Current Batch = 217\n",
            " 8/67 [==>...........................] - ETA: 30s - loss: 1.8046 - categorical_accuracy: 0.1250; Current Batch = 218\n",
            " 9/67 [===>..........................] - ETA: 28s - loss: 1.8462 - categorical_accuracy: 0.1481; Current Batch = 219\n",
            "10/67 [===>..........................] - ETA: 28s - loss: 1.7919 - categorical_accuracy: 0.1667; Current Batch = 220\n",
            "11/67 [===>..........................] - ETA: 27s - loss: 1.8013 - categorical_accuracy: 0.1515; Num of batches = 221\n",
            "; Current Batch = 0\n",
            "12/67 [====>.........................] - ETA: 26s - loss: 1.8144 - categorical_accuracy: 0.1389; Current Batch = 1\n",
            "13/67 [====>.........................] - ETA: 26s - loss: 1.7917 - categorical_accuracy: 0.1538; Current Batch = 2\n",
            "14/67 [=====>........................] - ETA: 25s - loss: 1.7629 - categorical_accuracy: 0.1667; Current Batch = 3\n",
            "15/67 [=====>........................] - ETA: 24s - loss: 1.7026 - categorical_accuracy: 0.2000; Current Batch = 4\n",
            "16/67 [======>.......................] - ETA: 24s - loss: 1.6739 - categorical_accuracy: 0.2292; Current Batch = 5\n",
            "17/67 [======>.......................] - ETA: 25s - loss: 1.6362 - categorical_accuracy: 0.2549; Current Batch = 6\n",
            "18/67 [=======>......................] - ETA: 25s - loss: 1.6555 - categorical_accuracy: 0.2593; Current Batch = 7\n",
            "19/67 [=======>......................] - ETA: 24s - loss: 1.6451 - categorical_accuracy: 0.2632; Current Batch = 8\n",
            "20/67 [=======>......................] - ETA: 25s - loss: 1.6679 - categorical_accuracy: 0.2500; Current Batch = 9\n",
            "21/67 [========>.....................] - ETA: 24s - loss: 1.6729 - categorical_accuracy: 0.2381; Current Batch = 10\n",
            "22/67 [========>.....................] - ETA: 23s - loss: 1.6687 - categorical_accuracy: 0.2576; Current Batch = 11\n",
            "23/67 [=========>....................] - ETA: 22s - loss: 1.6879 - categorical_accuracy: 0.2464; Current Batch = 12\n",
            "24/67 [=========>....................] - ETA: 22s - loss: 1.7057 - categorical_accuracy: 0.2361; Current Batch = 13\n",
            "25/67 [==========>...................] - ETA: 21s - loss: 1.7082 - categorical_accuracy: 0.2400; Current Batch = 14\n",
            "26/67 [==========>...................] - ETA: 20s - loss: 1.6937 - categorical_accuracy: 0.2308; Current Batch = 15\n",
            "27/67 [===========>..................] - ETA: 20s - loss: 1.6582 - categorical_accuracy: 0.2593; Current Batch = 16\n",
            "28/67 [===========>..................] - ETA: 19s - loss: 1.6748 - categorical_accuracy: 0.2500; Current Batch = 17\n",
            "29/67 [===========>..................] - ETA: 19s - loss: 1.6714 - categorical_accuracy: 0.2529; Current Batch = 18\n",
            "30/67 [============>.................] - ETA: 18s - loss: 1.6735 - categorical_accuracy: 0.2556; Current Batch = 19\n",
            "31/67 [============>.................] - ETA: 18s - loss: 1.6672 - categorical_accuracy: 0.2581; Current Batch = 20\n",
            "32/67 [=============>................] - ETA: 17s - loss: 1.6742 - categorical_accuracy: 0.2604; Current Batch = 21\n",
            "33/67 [=============>................] - ETA: 17s - loss: 1.6766 - categorical_accuracy: 0.2525; Current Batch = 22\n",
            "34/67 [==============>...............] - ETA: 16s - loss: 1.6694 - categorical_accuracy: 0.2647; Current Batch = 23\n",
            "35/67 [==============>...............] - ETA: 15s - loss: 1.6668 - categorical_accuracy: 0.2667; Current Batch = 24\n",
            "36/67 [===============>..............] - ETA: 15s - loss: 1.6625 - categorical_accuracy: 0.2593; Current Batch = 25\n",
            "37/67 [===============>..............] - ETA: 14s - loss: 1.6570 - categorical_accuracy: 0.2613; Current Batch = 26\n",
            "38/67 [================>.............] - ETA: 14s - loss: 1.6469 - categorical_accuracy: 0.2719; Current Batch = 27\n",
            "39/67 [================>.............] - ETA: 13s - loss: 1.6374 - categorical_accuracy: 0.2735; Current Batch = 28\n",
            "40/67 [================>.............] - ETA: 13s - loss: 1.6311 - categorical_accuracy: 0.2833; Current Batch = 29\n",
            "41/67 [=================>............] - ETA: 12s - loss: 1.6308 - categorical_accuracy: 0.2764; Current Batch = 30\n",
            "42/67 [=================>............] - ETA: 12s - loss: 1.6411 - categorical_accuracy: 0.2778; Current Batch = 31\n",
            "43/67 [==================>...........] - ETA: 11s - loss: 1.6429 - categorical_accuracy: 0.2791; Current Batch = 32\n",
            "44/67 [==================>...........] - ETA: 11s - loss: 1.6372 - categorical_accuracy: 0.2803; Current Batch = 33\n",
            "45/67 [===================>..........] - ETA: 10s - loss: 1.6382 - categorical_accuracy: 0.2815; Current Batch = 34\n",
            "46/67 [===================>..........] - ETA: 10s - loss: 1.6500 - categorical_accuracy: 0.2754; Current Batch = 35\n",
            "47/67 [====================>.........] - ETA: 9s - loss: 1.6485 - categorical_accuracy: 0.2766 ; Current Batch = 36\n",
            "48/67 [====================>.........] - ETA: 9s - loss: 1.6465 - categorical_accuracy: 0.2778; Current Batch = 37\n",
            "49/67 [====================>.........] - ETA: 8s - loss: 1.6384 - categorical_accuracy: 0.2857; Current Batch = 38\n",
            "50/67 [=====================>........] - ETA: 8s - loss: 1.6415 - categorical_accuracy: 0.2867; Current Batch = 39\n",
            "51/67 [=====================>........] - ETA: 7s - loss: 1.6563 - categorical_accuracy: 0.2876; Current Batch = 40\n",
            "52/67 [======================>.......] - ETA: 7s - loss: 1.6518 - categorical_accuracy: 0.2885; Current Batch = 41\n",
            "53/67 [======================>.......] - ETA: 6s - loss: 1.6599 - categorical_accuracy: 0.2830; Current Batch = 42\n",
            "54/67 [=======================>......] - ETA: 6s - loss: 1.6667 - categorical_accuracy: 0.2778; Current Batch = 43\n",
            "55/67 [=======================>......] - ETA: 5s - loss: 1.6694 - categorical_accuracy: 0.2727; Current Batch = 44\n",
            "56/67 [========================>.....] - ETA: 5s - loss: 1.6546 - categorical_accuracy: 0.2857; Current Batch = 45\n",
            "57/67 [========================>.....] - ETA: 5s - loss: 1.6512 - categorical_accuracy: 0.2924; Current Batch = 46\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.6639 - categorical_accuracy: 0.2931; Current Batch = 47\n",
            "59/67 [=========================>....] - ETA: 4s - loss: 1.6719 - categorical_accuracy: 0.2881; Current Batch = 48\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.6668 - categorical_accuracy: 0.2889; Current Batch = 49\n",
            "61/67 [==========================>...] - ETA: 3s - loss: 1.6673 - categorical_accuracy: 0.2896; Current Batch = 50\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.6584 - categorical_accuracy: 0.2903; Current Batch = 51\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.6550 - categorical_accuracy: 0.2910; Current Batch = 52\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.6625 - categorical_accuracy: 0.2865; Current Batch = 53\n",
            "65/67 [============================>.] - ETA: 0s - loss: 1.6591 - categorical_accuracy: 0.2923; Current Batch = 54\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.6558 - categorical_accuracy: 0.2929; Current Batch = 55\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.6514 - categorical_accuracy: 0.2935; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "\n",
            "Epoch 12: saving model to model_init_2023-08-3109_43_59.510610/model-00012-1.65144-0.29353-1.28626-0.46000.h5\n",
            "67/67 [==============================] - 55s 825ms/step - loss: 1.6514 - categorical_accuracy: 0.2935 - val_loss: 1.2863 - val_categorical_accuracy: 0.4600 - lr: 2.0000e-04\n",
            "Epoch 13/25\n",
            "; Current Batch = 56\n",
            " 1/67 [..............................] - ETA: 4s - loss: 1.2974 - categorical_accuracy: 0.3333; Current Batch = 57\n",
            " 2/67 [..............................] - ETA: 36s - loss: 1.6593 - categorical_accuracy: 0.5000; Current Batch = 58\n",
            " 3/67 [>.............................] - ETA: 29s - loss: 1.3846 - categorical_accuracy: 0.5556; Current Batch = 59\n",
            " 4/67 [>.............................] - ETA: 29s - loss: 1.2600 - categorical_accuracy: 0.5833; Current Batch = 60\n",
            " 5/67 [=>............................] - ETA: 27s - loss: 1.1143 - categorical_accuracy: 0.6667; Current Batch = 61\n",
            " 6/67 [=>............................] - ETA: 28s - loss: 1.0946 - categorical_accuracy: 0.6667; Current Batch = 62\n",
            " 7/67 [==>...........................] - ETA: 26s - loss: 1.1510 - categorical_accuracy: 0.6190; Current Batch = 63\n",
            " 8/67 [==>...........................] - ETA: 26s - loss: 1.1830 - categorical_accuracy: 0.5833; Current Batch = 64\n",
            " 9/67 [===>..........................] - ETA: 25s - loss: 1.2362 - categorical_accuracy: 0.5556; Current Batch = 65\n",
            "10/67 [===>..........................] - ETA: 26s - loss: 1.2431 - categorical_accuracy: 0.5667; Current Batch = 66\n",
            "11/67 [===>..........................] - ETA: 25s - loss: 1.2104 - categorical_accuracy: 0.5758; Current Batch = 67\n",
            "12/67 [====>.........................] - ETA: 26s - loss: 1.2608 - categorical_accuracy: 0.5556; Current Batch = 68\n",
            "13/67 [====>.........................] - ETA: 27s - loss: 1.2560 - categorical_accuracy: 0.5385; Current Batch = 69\n",
            "14/67 [=====>........................] - ETA: 27s - loss: 1.3718 - categorical_accuracy: 0.5238; Current Batch = 70\n",
            "15/67 [=====>........................] - ETA: 26s - loss: 1.4102 - categorical_accuracy: 0.4889; Current Batch = 71\n",
            "16/67 [======>.......................] - ETA: 26s - loss: 1.4622 - categorical_accuracy: 0.4583; Current Batch = 72\n",
            "17/67 [======>.......................] - ETA: 25s - loss: 1.4524 - categorical_accuracy: 0.4510; Current Batch = 73\n",
            "18/67 [=======>......................] - ETA: 26s - loss: 1.4802 - categorical_accuracy: 0.4259; Current Batch = 74\n",
            "19/67 [=======>......................] - ETA: 25s - loss: 1.4605 - categorical_accuracy: 0.4386; Current Batch = 75\n",
            "20/67 [=======>......................] - ETA: 25s - loss: 1.4797 - categorical_accuracy: 0.4333; Current Batch = 76\n",
            "21/67 [========>.....................] - ETA: 24s - loss: 1.5209 - categorical_accuracy: 0.4127; Current Batch = 77\n",
            "22/67 [========>.....................] - ETA: 24s - loss: 1.5198 - categorical_accuracy: 0.4242; Current Batch = 78\n",
            "23/67 [=========>....................] - ETA: 24s - loss: 1.4944 - categorical_accuracy: 0.4348; Current Batch = 79\n",
            "24/67 [=========>....................] - ETA: 23s - loss: 1.4609 - categorical_accuracy: 0.4583; Current Batch = 80\n",
            "25/67 [==========>...................] - ETA: 23s - loss: 1.4346 - categorical_accuracy: 0.4800; Current Batch = 81\n",
            "26/67 [==========>...................] - ETA: 22s - loss: 1.4465 - categorical_accuracy: 0.4615; Current Batch = 82\n",
            "27/67 [===========>..................] - ETA: 22s - loss: 1.4499 - categorical_accuracy: 0.4568; Current Batch = 83\n",
            "28/67 [===========>..................] - ETA: 21s - loss: 1.4707 - categorical_accuracy: 0.4405; Current Batch = 84\n",
            "29/67 [===========>..................] - ETA: 21s - loss: 1.4593 - categorical_accuracy: 0.4368; Current Batch = 85\n",
            "30/67 [============>.................] - ETA: 20s - loss: 1.4424 - categorical_accuracy: 0.4444; Current Batch = 86\n",
            "31/67 [============>.................] - ETA: 20s - loss: 1.4468 - categorical_accuracy: 0.4409; Current Batch = 87\n",
            "32/67 [=============>................] - ETA: 19s - loss: 1.4420 - categorical_accuracy: 0.4479; Current Batch = 88\n",
            "33/67 [=============>................] - ETA: 18s - loss: 1.4360 - categorical_accuracy: 0.4444; Current Batch = 89\n",
            "34/67 [==============>...............] - ETA: 18s - loss: 1.4523 - categorical_accuracy: 0.4314; Current Batch = 90\n",
            "35/67 [==============>...............] - ETA: 17s - loss: 1.4420 - categorical_accuracy: 0.4286; Current Batch = 91\n",
            "36/67 [===============>..............] - ETA: 16s - loss: 1.4483 - categorical_accuracy: 0.4259; Current Batch = 92\n",
            "37/67 [===============>..............] - ETA: 16s - loss: 1.4480 - categorical_accuracy: 0.4234; Current Batch = 93\n",
            "38/67 [================>.............] - ETA: 15s - loss: 1.4487 - categorical_accuracy: 0.4211; Current Batch = 94\n",
            "39/67 [================>.............] - ETA: 15s - loss: 1.4353 - categorical_accuracy: 0.4274; Current Batch = 95\n",
            "40/67 [================>.............] - ETA: 14s - loss: 1.4556 - categorical_accuracy: 0.4167; Current Batch = 96\n",
            "41/67 [=================>............] - ETA: 14s - loss: 1.4509 - categorical_accuracy: 0.4228; Current Batch = 97\n",
            "42/67 [=================>............] - ETA: 13s - loss: 1.4621 - categorical_accuracy: 0.4286; Current Batch = 98\n",
            "43/67 [==================>...........] - ETA: 13s - loss: 1.4656 - categorical_accuracy: 0.4186; Current Batch = 99\n",
            "44/67 [==================>...........] - ETA: 12s - loss: 1.4653 - categorical_accuracy: 0.4242; Current Batch = 100\n",
            "45/67 [===================>..........] - ETA: 12s - loss: 1.4735 - categorical_accuracy: 0.4222; Current Batch = 101\n",
            "46/67 [===================>..........] - ETA: 11s - loss: 1.4678 - categorical_accuracy: 0.4275; Current Batch = 102\n",
            "47/67 [====================>.........] - ETA: 11s - loss: 1.4673 - categorical_accuracy: 0.4255; Current Batch = 103\n",
            "48/67 [====================>.........] - ETA: 10s - loss: 1.4824 - categorical_accuracy: 0.4236; Current Batch = 104\n",
            "49/67 [====================>.........] - ETA: 10s - loss: 1.4809 - categorical_accuracy: 0.4218; Current Batch = 105\n",
            "50/67 [=====================>........] - ETA: 9s - loss: 1.4738 - categorical_accuracy: 0.4200 ; Current Batch = 106\n",
            "51/67 [=====================>........] - ETA: 8s - loss: 1.4748 - categorical_accuracy: 0.4183; Current Batch = 107\n",
            "52/67 [======================>.......] - ETA: 8s - loss: 1.4800 - categorical_accuracy: 0.4103; Current Batch = 108\n",
            "53/67 [======================>.......] - ETA: 7s - loss: 1.4832 - categorical_accuracy: 0.4088; Current Batch = 109\n",
            "54/67 [=======================>......] - ETA: 7s - loss: 1.4757 - categorical_accuracy: 0.4074; Current Batch = 110\n",
            "55/67 [=======================>......] - ETA: 6s - loss: 1.4683 - categorical_accuracy: 0.4121; Current Batch = 111\n",
            "56/67 [========================>.....] - ETA: 6s - loss: 1.4855 - categorical_accuracy: 0.4048; Current Batch = 112\n",
            "57/67 [========================>.....] - ETA: 5s - loss: 1.4830 - categorical_accuracy: 0.4035; Current Batch = 113\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.4790 - categorical_accuracy: 0.4023; Current Batch = 114\n",
            "59/67 [=========================>....] - ETA: 4s - loss: 1.4814 - categorical_accuracy: 0.4068; Current Batch = 115\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.4871 - categorical_accuracy: 0.4056; Current Batch = 116\n",
            "61/67 [==========================>...] - ETA: 3s - loss: 1.4957 - categorical_accuracy: 0.4044; Current Batch = 117\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.5108 - categorical_accuracy: 0.3978; Current Batch = 118\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.5071 - categorical_accuracy: 0.3915; Current Batch = 119\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.4995 - categorical_accuracy: 0.3906; Current Batch = 120\n",
            "65/67 [============================>.] - ETA: 1s - loss: 1.4889 - categorical_accuracy: 0.3949; Current Batch = 121\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.4903 - categorical_accuracy: 0.3939; Current Batch = 122\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.4855 - categorical_accuracy: 0.3980; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "\n",
            "Epoch 13: saving model to model_init_2023-08-3109_43_59.510610/model-00013-1.48548-0.39801-1.33332-0.46000.h5\n",
            "67/67 [==============================] - 58s 883ms/step - loss: 1.4855 - categorical_accuracy: 0.3980 - val_loss: 1.3333 - val_categorical_accuracy: 0.4600 - lr: 2.0000e-04\n",
            "Epoch 14/25\n",
            "; Current Batch = 123\n",
            " 1/67 [..............................] - ETA: 4s - loss: 1.1039 - categorical_accuracy: 0.6667; Current Batch = 124\n",
            " 2/67 [..............................] - ETA: 34s - loss: 1.3974 - categorical_accuracy: 0.3333; Current Batch = 125\n",
            " 3/67 [>.............................] - ETA: 33s - loss: 1.4278 - categorical_accuracy: 0.3333; Current Batch = 126\n",
            " 4/67 [>.............................] - ETA: 33s - loss: 1.4114 - categorical_accuracy: 0.3333; Current Batch = 127\n",
            " 5/67 [=>............................] - ETA: 29s - loss: 1.2693 - categorical_accuracy: 0.4667; Current Batch = 128\n",
            " 6/67 [=>............................] - ETA: 29s - loss: 1.2482 - categorical_accuracy: 0.4444; Current Batch = 129\n",
            " 7/67 [==>...........................] - ETA: 31s - loss: 1.2665 - categorical_accuracy: 0.4762; Current Batch = 130\n",
            " 8/67 [==>...........................] - ETA: 32s - loss: 1.2477 - categorical_accuracy: 0.5000; Current Batch = 131\n",
            " 9/67 [===>..........................] - ETA: 31s - loss: 1.2946 - categorical_accuracy: 0.4815; Current Batch = 132\n",
            "10/67 [===>..........................] - ETA: 29s - loss: 1.2519 - categorical_accuracy: 0.4667; Current Batch = 133\n",
            "11/67 [===>..........................] - ETA: 28s - loss: 1.2723 - categorical_accuracy: 0.4545; Current Batch = 134\n",
            "12/67 [====>.........................] - ETA: 28s - loss: 1.2760 - categorical_accuracy: 0.4722; Current Batch = 135\n",
            "13/67 [====>.........................] - ETA: 28s - loss: 1.3130 - categorical_accuracy: 0.4359; Current Batch = 136\n",
            "14/67 [=====>........................] - ETA: 28s - loss: 1.3168 - categorical_accuracy: 0.4286; Current Batch = 137\n",
            "15/67 [=====>........................] - ETA: 29s - loss: 1.3646 - categorical_accuracy: 0.4000; Current Batch = 138\n",
            "16/67 [======>.......................] - ETA: 30s - loss: 1.3842 - categorical_accuracy: 0.4167; Current Batch = 139\n",
            "17/67 [======>.......................] - ETA: 29s - loss: 1.3979 - categorical_accuracy: 0.4314; Current Batch = 140\n",
            "18/67 [=======>......................] - ETA: 30s - loss: 1.4451 - categorical_accuracy: 0.4074; Current Batch = 141\n",
            "19/67 [=======>......................] - ETA: 28s - loss: 1.4601 - categorical_accuracy: 0.4211; Current Batch = 142\n",
            "20/67 [=======>......................] - ETA: 27s - loss: 1.4461 - categorical_accuracy: 0.4000; Current Batch = 143\n",
            "21/67 [========>.....................] - ETA: 26s - loss: 1.4515 - categorical_accuracy: 0.3968; Current Batch = 144\n",
            "22/67 [========>.....................] - ETA: 26s - loss: 1.4573 - categorical_accuracy: 0.3939; Current Batch = 145\n",
            "23/67 [=========>....................] - ETA: 24s - loss: 1.4681 - categorical_accuracy: 0.3913; Current Batch = 146\n",
            "24/67 [=========>....................] - ETA: 24s - loss: 1.4398 - categorical_accuracy: 0.4167; Current Batch = 147\n",
            "25/67 [==========>...................] - ETA: 23s - loss: 1.4144 - categorical_accuracy: 0.4267; Current Batch = 148\n",
            "26/67 [==========>...................] - ETA: 22s - loss: 1.4146 - categorical_accuracy: 0.4359; Current Batch = 149\n",
            "27/67 [===========>..................] - ETA: 22s - loss: 1.4150 - categorical_accuracy: 0.4321; Current Batch = 150\n",
            "28/67 [===========>..................] - ETA: 21s - loss: 1.4054 - categorical_accuracy: 0.4405; Current Batch = 151\n",
            "29/67 [===========>..................] - ETA: 21s - loss: 1.3853 - categorical_accuracy: 0.4483; Current Batch = 152\n",
            "30/67 [============>.................] - ETA: 20s - loss: 1.3727 - categorical_accuracy: 0.4556; Current Batch = 153\n",
            "31/67 [============>.................] - ETA: 20s - loss: 1.4000 - categorical_accuracy: 0.4409; Current Batch = 154\n",
            "32/67 [=============>................] - ETA: 19s - loss: 1.3846 - categorical_accuracy: 0.4479; Current Batch = 155\n",
            "33/67 [=============>................] - ETA: 18s - loss: 1.3648 - categorical_accuracy: 0.4545; Current Batch = 156\n",
            "34/67 [==============>...............] - ETA: 18s - loss: 1.3746 - categorical_accuracy: 0.4510; Current Batch = 157\n",
            "35/67 [==============>...............] - ETA: 17s - loss: 1.3882 - categorical_accuracy: 0.4381; Current Batch = 158\n",
            "36/67 [===============>..............] - ETA: 16s - loss: 1.4071 - categorical_accuracy: 0.4352; Current Batch = 159\n",
            "37/67 [===============>..............] - ETA: 16s - loss: 1.4076 - categorical_accuracy: 0.4324; Current Batch = 160\n",
            "38/67 [================>.............] - ETA: 15s - loss: 1.4195 - categorical_accuracy: 0.4298; Current Batch = 161\n",
            "39/67 [================>.............] - ETA: 15s - loss: 1.4334 - categorical_accuracy: 0.4274; Current Batch = 162\n",
            "40/67 [================>.............] - ETA: 14s - loss: 1.4244 - categorical_accuracy: 0.4250; Current Batch = 163\n",
            "41/67 [=================>............] - ETA: 14s - loss: 1.4244 - categorical_accuracy: 0.4228; Current Batch = 164\n",
            "42/67 [=================>............] - ETA: 14s - loss: 1.4275 - categorical_accuracy: 0.4127; Current Batch = 165\n",
            "43/67 [==================>...........] - ETA: 13s - loss: 1.4291 - categorical_accuracy: 0.4031; Current Batch = 166\n",
            "44/67 [==================>...........] - ETA: 12s - loss: 1.4505 - categorical_accuracy: 0.3939; Current Batch = 167\n",
            "45/67 [===================>..........] - ETA: 12s - loss: 1.4498 - categorical_accuracy: 0.3852; Current Batch = 168\n",
            "46/67 [===================>..........] - ETA: 11s - loss: 1.4673 - categorical_accuracy: 0.3768; Current Batch = 169\n",
            "47/67 [====================>.........] - ETA: 11s - loss: 1.4724 - categorical_accuracy: 0.3830; Current Batch = 170\n",
            "48/67 [====================>.........] - ETA: 10s - loss: 1.4643 - categorical_accuracy: 0.3889; Current Batch = 171\n",
            "49/67 [====================>.........] - ETA: 9s - loss: 1.4507 - categorical_accuracy: 0.4014 ; Current Batch = 172\n",
            "50/67 [=====================>........] - ETA: 9s - loss: 1.4747 - categorical_accuracy: 0.3933; Current Batch = 173\n",
            "51/67 [=====================>........] - ETA: 8s - loss: 1.4673 - categorical_accuracy: 0.3987; Current Batch = 174\n",
            "52/67 [======================>.......] - ETA: 8s - loss: 1.4716 - categorical_accuracy: 0.3910; Current Batch = 175\n",
            "53/67 [======================>.......] - ETA: 7s - loss: 1.4573 - categorical_accuracy: 0.4025; Current Batch = 176\n",
            "54/67 [=======================>......] - ETA: 7s - loss: 1.4496 - categorical_accuracy: 0.4074; Current Batch = 177\n",
            "55/67 [=======================>......] - ETA: 6s - loss: 1.4337 - categorical_accuracy: 0.4182; Current Batch = 178\n",
            "56/67 [========================>.....] - ETA: 5s - loss: 1.4328 - categorical_accuracy: 0.4107; Current Batch = 179\n",
            "57/67 [========================>.....] - ETA: 5s - loss: 1.4327 - categorical_accuracy: 0.4094; Current Batch = 180\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.4441 - categorical_accuracy: 0.4080; Current Batch = 181\n",
            "59/67 [=========================>....] - ETA: 4s - loss: 1.4401 - categorical_accuracy: 0.4068; Current Batch = 182\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.4506 - categorical_accuracy: 0.4000; Current Batch = 183\n",
            "61/67 [==========================>...] - ETA: 3s - loss: 1.4547 - categorical_accuracy: 0.3989; Current Batch = 184\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.4447 - categorical_accuracy: 0.4032; Current Batch = 185\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.4491 - categorical_accuracy: 0.4021; Current Batch = 186\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.4399 - categorical_accuracy: 0.4062; Current Batch = 187\n",
            "65/67 [============================>.] - ETA: 1s - loss: 1.4420 - categorical_accuracy: 0.4051; Current Batch = 188\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.4513 - categorical_accuracy: 0.4040; Current Batch = 189\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.4480 - categorical_accuracy: 0.4030; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "\n",
            "Epoch 14: saving model to model_init_2023-08-3109_43_59.510610/model-00014-1.44800-0.40299-1.34576-0.47000.h5\n",
            "67/67 [==============================] - 56s 854ms/step - loss: 1.4480 - categorical_accuracy: 0.4030 - val_loss: 1.3458 - val_categorical_accuracy: 0.4700 - lr: 2.0000e-04\n",
            "Epoch 15/25\n",
            "; Current Batch = 190\n",
            " 1/67 [..............................] - ETA: 4s - loss: 1.2190 - categorical_accuracy: 0.6667; Current Batch = 191\n",
            " 2/67 [..............................] - ETA: 22s - loss: 1.3118 - categorical_accuracy: 0.5000; Current Batch = 192\n",
            " 3/67 [>.............................] - ETA: 33s - loss: 1.5508 - categorical_accuracy: 0.4444; Current Batch = 193\n",
            " 4/67 [>.............................] - ETA: 28s - loss: 1.4500 - categorical_accuracy: 0.4167; Current Batch = 194\n",
            " 5/67 [=>............................] - ETA: 29s - loss: 1.4233 - categorical_accuracy: 0.3333; Current Batch = 195\n",
            " 6/67 [=>............................] - ETA: 27s - loss: 1.4047 - categorical_accuracy: 0.3333; Current Batch = 196\n",
            " 7/67 [==>...........................] - ETA: 25s - loss: 1.4630 - categorical_accuracy: 0.3333; Current Batch = 197\n",
            " 8/67 [==>...........................] - ETA: 26s - loss: 1.4546 - categorical_accuracy: 0.3333; Current Batch = 198\n",
            " 9/67 [===>..........................] - ETA: 26s - loss: 1.5536 - categorical_accuracy: 0.3704; Current Batch = 199\n",
            "10/67 [===>..........................] - ETA: 24s - loss: 1.5091 - categorical_accuracy: 0.4000; Current Batch = 200\n",
            "11/67 [===>..........................] - ETA: 23s - loss: 1.6171 - categorical_accuracy: 0.3939; Current Batch = 201\n",
            "12/67 [====>.........................] - ETA: 24s - loss: 1.6683 - categorical_accuracy: 0.3611; Current Batch = 202\n",
            "13/67 [====>.........................] - ETA: 25s - loss: 1.7021 - categorical_accuracy: 0.3333; Current Batch = 203\n",
            "14/67 [=====>........................] - ETA: 24s - loss: 1.7181 - categorical_accuracy: 0.3095; Current Batch = 204\n",
            "15/67 [=====>........................] - ETA: 24s - loss: 1.6976 - categorical_accuracy: 0.3111; Current Batch = 205\n",
            "16/67 [======>.......................] - ETA: 24s - loss: 1.6750 - categorical_accuracy: 0.3333; Current Batch = 206\n",
            "17/67 [======>.......................] - ETA: 23s - loss: 1.6895 - categorical_accuracy: 0.3333; Current Batch = 207\n",
            "18/67 [=======>......................] - ETA: 23s - loss: 1.7016 - categorical_accuracy: 0.3148; Current Batch = 208\n",
            "19/67 [=======>......................] - ETA: 22s - loss: 1.7362 - categorical_accuracy: 0.2982; Current Batch = 209\n",
            "20/67 [=======>......................] - ETA: 23s - loss: 1.7088 - categorical_accuracy: 0.3167; Current Batch = 210\n",
            "21/67 [========>.....................] - ETA: 22s - loss: 1.6684 - categorical_accuracy: 0.3492; Current Batch = 211\n",
            "22/67 [========>.....................] - ETA: 22s - loss: 1.6642 - categorical_accuracy: 0.3485; Current Batch = 212\n",
            "23/67 [=========>....................] - ETA: 21s - loss: 1.6902 - categorical_accuracy: 0.3333; Current Batch = 213\n",
            "24/67 [=========>....................] - ETA: 21s - loss: 1.6621 - categorical_accuracy: 0.3472; Current Batch = 214\n",
            "25/67 [==========>...................] - ETA: 20s - loss: 1.6734 - categorical_accuracy: 0.3333; Current Batch = 215\n",
            "26/67 [==========>...................] - ETA: 20s - loss: 1.6770 - categorical_accuracy: 0.3333; Current Batch = 216\n",
            "27/67 [===========>..................] - ETA: 19s - loss: 1.6684 - categorical_accuracy: 0.3333; Current Batch = 217\n",
            "28/67 [===========>..................] - ETA: 19s - loss: 1.6654 - categorical_accuracy: 0.3214; Current Batch = 218\n",
            "29/67 [===========>..................] - ETA: 18s - loss: 1.6496 - categorical_accuracy: 0.3333; Current Batch = 219\n",
            "30/67 [============>.................] - ETA: 18s - loss: 1.6418 - categorical_accuracy: 0.3333; Current Batch = 220\n",
            "31/67 [============>.................] - ETA: 17s - loss: 1.6308 - categorical_accuracy: 0.3441; Num of batches = 221\n",
            "; Current Batch = 0\n",
            "32/67 [=============>................] - ETA: 17s - loss: 1.6347 - categorical_accuracy: 0.3438; Current Batch = 1\n",
            "33/67 [=============>................] - ETA: 17s - loss: 1.6228 - categorical_accuracy: 0.3434; Current Batch = 2\n",
            "34/67 [==============>...............] - ETA: 16s - loss: 1.6122 - categorical_accuracy: 0.3529; Current Batch = 3\n",
            "35/67 [==============>...............] - ETA: 16s - loss: 1.6031 - categorical_accuracy: 0.3524; Current Batch = 4\n",
            "36/67 [===============>..............] - ETA: 15s - loss: 1.5943 - categorical_accuracy: 0.3519; Current Batch = 5\n",
            "37/67 [===============>..............] - ETA: 15s - loss: 1.5903 - categorical_accuracy: 0.3514; Current Batch = 6\n",
            "38/67 [================>.............] - ETA: 14s - loss: 1.5904 - categorical_accuracy: 0.3596; Current Batch = 7\n",
            "39/67 [================>.............] - ETA: 14s - loss: 1.6102 - categorical_accuracy: 0.3504; Current Batch = 8\n",
            "40/67 [================>.............] - ETA: 13s - loss: 1.6080 - categorical_accuracy: 0.3500; Current Batch = 9\n",
            "41/67 [=================>............] - ETA: 13s - loss: 1.6189 - categorical_accuracy: 0.3496; Current Batch = 10\n",
            "42/67 [=================>............] - ETA: 12s - loss: 1.6128 - categorical_accuracy: 0.3492; Current Batch = 11\n",
            "43/67 [==================>...........] - ETA: 12s - loss: 1.6259 - categorical_accuracy: 0.3488; Current Batch = 12\n",
            "44/67 [==================>...........] - ETA: 11s - loss: 1.6083 - categorical_accuracy: 0.3561; Current Batch = 13\n",
            "45/67 [===================>..........] - ETA: 11s - loss: 1.6016 - categorical_accuracy: 0.3556; Current Batch = 14\n",
            "46/67 [===================>..........] - ETA: 10s - loss: 1.5869 - categorical_accuracy: 0.3623; Current Batch = 15\n",
            "47/67 [====================>.........] - ETA: 10s - loss: 1.5938 - categorical_accuracy: 0.3617; Current Batch = 16\n",
            "48/67 [====================>.........] - ETA: 9s - loss: 1.5854 - categorical_accuracy: 0.3681 ; Current Batch = 17\n",
            "49/67 [====================>.........] - ETA: 9s - loss: 1.5899 - categorical_accuracy: 0.3673; Current Batch = 18\n",
            "50/67 [=====================>........] - ETA: 8s - loss: 1.5804 - categorical_accuracy: 0.3667; Current Batch = 19\n",
            "51/67 [=====================>........] - ETA: 8s - loss: 1.5694 - categorical_accuracy: 0.3660; Current Batch = 20\n",
            "52/67 [======================>.......] - ETA: 7s - loss: 1.5596 - categorical_accuracy: 0.3718; Current Batch = 21\n",
            "53/67 [======================>.......] - ETA: 7s - loss: 1.5619 - categorical_accuracy: 0.3774; Current Batch = 22\n",
            "54/67 [=======================>......] - ETA: 6s - loss: 1.5575 - categorical_accuracy: 0.3827; Current Batch = 23\n",
            "55/67 [=======================>......] - ETA: 6s - loss: 1.5513 - categorical_accuracy: 0.3818; Current Batch = 24\n",
            "56/67 [========================>.....] - ETA: 5s - loss: 1.5617 - categorical_accuracy: 0.3750; Current Batch = 25\n",
            "57/67 [========================>.....] - ETA: 5s - loss: 1.5559 - categorical_accuracy: 0.3743; Current Batch = 26\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.5584 - categorical_accuracy: 0.3736; Current Batch = 27\n",
            "59/67 [=========================>....] - ETA: 3s - loss: 1.5525 - categorical_accuracy: 0.3729; Current Batch = 28\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.5458 - categorical_accuracy: 0.3778; Current Batch = 29\n",
            "61/67 [==========================>...] - ETA: 2s - loss: 1.5343 - categorical_accuracy: 0.3825; Current Batch = 30\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.5287 - categorical_accuracy: 0.3871; Current Batch = 31\n",
            "63/67 [===========================>..] - ETA: 1s - loss: 1.5299 - categorical_accuracy: 0.3862; Current Batch = 32\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.5316 - categorical_accuracy: 0.3854; Current Batch = 33\n",
            "65/67 [============================>.] - ETA: 0s - loss: 1.5403 - categorical_accuracy: 0.3795; Current Batch = 34\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.5412 - categorical_accuracy: 0.3737; Current Batch = 35\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.5340 - categorical_accuracy: 0.3831; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "\n",
            "Epoch 15: saving model to model_init_2023-08-3109_43_59.510610/model-00015-1.53398-0.38308-1.45489-0.42000.h5\n",
            "67/67 [==============================] - 55s 825ms/step - loss: 1.5340 - categorical_accuracy: 0.3831 - val_loss: 1.4549 - val_categorical_accuracy: 0.4200 - lr: 2.0000e-04\n",
            "Epoch 16/25\n",
            "; Current Batch = 36\n",
            " 1/67 [..............................] - ETA: 4s - loss: 1.9646 - categorical_accuracy: 0.0000e+00; Current Batch = 37\n",
            " 2/67 [..............................] - ETA: 35s - loss: 1.4884 - categorical_accuracy: 0.3333   ; Current Batch = 38\n",
            " 3/67 [>.............................] - ETA: 34s - loss: 1.4554 - categorical_accuracy: 0.3333; Current Batch = 39\n",
            " 4/67 [>.............................] - ETA: 37s - loss: 1.4762 - categorical_accuracy: 0.2500; Current Batch = 40\n",
            " 5/67 [=>............................] - ETA: 32s - loss: 1.5250 - categorical_accuracy: 0.2000; Current Batch = 41\n",
            " 6/67 [=>............................] - ETA: 31s - loss: 1.4068 - categorical_accuracy: 0.2778; Current Batch = 42\n",
            " 7/67 [==>...........................] - ETA: 32s - loss: 1.3563 - categorical_accuracy: 0.3333; Current Batch = 43\n",
            " 8/67 [==>...........................] - ETA: 31s - loss: 1.3940 - categorical_accuracy: 0.2917; Current Batch = 44\n",
            " 9/67 [===>..........................] - ETA: 32s - loss: 1.3630 - categorical_accuracy: 0.3333; Current Batch = 45\n",
            "10/67 [===>..........................] - ETA: 31s - loss: 1.4412 - categorical_accuracy: 0.3333; Current Batch = 46\n",
            "11/67 [===>..........................] - ETA: 30s - loss: 1.4604 - categorical_accuracy: 0.3636; Current Batch = 47\n",
            "12/67 [====>.........................] - ETA: 29s - loss: 1.4879 - categorical_accuracy: 0.3333; Current Batch = 48\n",
            "13/67 [====>.........................] - ETA: 29s - loss: 1.4561 - categorical_accuracy: 0.3590; Current Batch = 49\n",
            "14/67 [=====>........................] - ETA: 29s - loss: 1.4758 - categorical_accuracy: 0.3571; Current Batch = 50\n",
            "15/67 [=====>........................] - ETA: 29s - loss: 1.4630 - categorical_accuracy: 0.3556; Current Batch = 51\n",
            "16/67 [======>.......................] - ETA: 28s - loss: 1.4382 - categorical_accuracy: 0.3750; Current Batch = 52\n",
            "17/67 [======>.......................] - ETA: 28s - loss: 1.4220 - categorical_accuracy: 0.3922; Current Batch = 53\n",
            "18/67 [=======>......................] - ETA: 27s - loss: 1.4261 - categorical_accuracy: 0.3704; Current Batch = 54\n",
            "19/67 [=======>......................] - ETA: 26s - loss: 1.4290 - categorical_accuracy: 0.3684; Current Batch = 55\n",
            "20/67 [=======>......................] - ETA: 25s - loss: 1.4011 - categorical_accuracy: 0.3833; Current Batch = 56\n",
            "21/67 [========>.....................] - ETA: 24s - loss: 1.4123 - categorical_accuracy: 0.3810; Current Batch = 57\n",
            "22/67 [========>.....................] - ETA: 24s - loss: 1.4204 - categorical_accuracy: 0.3788; Current Batch = 58\n",
            "23/67 [=========>....................] - ETA: 23s - loss: 1.4319 - categorical_accuracy: 0.3623; Current Batch = 59\n",
            "24/67 [=========>....................] - ETA: 22s - loss: 1.4597 - categorical_accuracy: 0.3611; Current Batch = 60\n",
            "25/67 [==========>...................] - ETA: 22s - loss: 1.4700 - categorical_accuracy: 0.3600; Current Batch = 61\n",
            "26/67 [==========>...................] - ETA: 21s - loss: 1.4589 - categorical_accuracy: 0.3462; Current Batch = 62\n",
            "27/67 [===========>..................] - ETA: 20s - loss: 1.4668 - categorical_accuracy: 0.3457; Current Batch = 63\n",
            "28/67 [===========>..................] - ETA: 20s - loss: 1.4465 - categorical_accuracy: 0.3571; Current Batch = 64\n",
            "29/67 [===========>..................] - ETA: 19s - loss: 1.4201 - categorical_accuracy: 0.3793; Current Batch = 65\n",
            "30/67 [============>.................] - ETA: 18s - loss: 1.4339 - categorical_accuracy: 0.3667; Current Batch = 66\n",
            "31/67 [============>.................] - ETA: 18s - loss: 1.4430 - categorical_accuracy: 0.3548; Current Batch = 67\n",
            "32/67 [=============>................] - ETA: 17s - loss: 1.4308 - categorical_accuracy: 0.3646; Current Batch = 68\n",
            "33/67 [=============>................] - ETA: 16s - loss: 1.3977 - categorical_accuracy: 0.3838; Current Batch = 69\n",
            "34/67 [==============>...............] - ETA: 16s - loss: 1.3923 - categorical_accuracy: 0.3725; Current Batch = 70\n",
            "35/67 [==============>...............] - ETA: 15s - loss: 1.3946 - categorical_accuracy: 0.3714; Current Batch = 71\n",
            "36/67 [===============>..............] - ETA: 15s - loss: 1.3971 - categorical_accuracy: 0.3704; Current Batch = 72\n",
            "37/67 [===============>..............] - ETA: 15s - loss: 1.4072 - categorical_accuracy: 0.3604; Current Batch = 73\n",
            "38/67 [================>.............] - ETA: 14s - loss: 1.4103 - categorical_accuracy: 0.3596; Current Batch = 74\n",
            "39/67 [================>.............] - ETA: 14s - loss: 1.4088 - categorical_accuracy: 0.3590; Current Batch = 75\n",
            "40/67 [================>.............] - ETA: 13s - loss: 1.3965 - categorical_accuracy: 0.3667; Current Batch = 76\n",
            "41/67 [=================>............] - ETA: 13s - loss: 1.3952 - categorical_accuracy: 0.3659; Current Batch = 77\n",
            "42/67 [=================>............] - ETA: 12s - loss: 1.3921 - categorical_accuracy: 0.3651; Current Batch = 78\n",
            "43/67 [==================>...........] - ETA: 12s - loss: 1.3997 - categorical_accuracy: 0.3566; Current Batch = 79\n",
            "44/67 [==================>...........] - ETA: 12s - loss: 1.4073 - categorical_accuracy: 0.3485; Current Batch = 80\n",
            "45/67 [===================>..........] - ETA: 11s - loss: 1.3980 - categorical_accuracy: 0.3556; Current Batch = 81\n",
            "46/67 [===================>..........] - ETA: 11s - loss: 1.3889 - categorical_accuracy: 0.3623; Current Batch = 82\n",
            "47/67 [====================>.........] - ETA: 10s - loss: 1.3742 - categorical_accuracy: 0.3688; Current Batch = 83\n",
            "48/67 [====================>.........] - ETA: 9s - loss: 1.3784 - categorical_accuracy: 0.3681 ; Current Batch = 84\n",
            "49/67 [====================>.........] - ETA: 9s - loss: 1.3656 - categorical_accuracy: 0.3810; Current Batch = 85\n",
            "50/67 [=====================>........] - ETA: 8s - loss: 1.3760 - categorical_accuracy: 0.3800; Current Batch = 86\n",
            "51/67 [=====================>........] - ETA: 8s - loss: 1.3698 - categorical_accuracy: 0.3856; Current Batch = 87\n",
            "52/67 [======================>.......] - ETA: 7s - loss: 1.3681 - categorical_accuracy: 0.3846; Current Batch = 88\n",
            "53/67 [======================>.......] - ETA: 7s - loss: 1.3666 - categorical_accuracy: 0.3836; Current Batch = 89\n",
            "54/67 [=======================>......] - ETA: 6s - loss: 1.3694 - categorical_accuracy: 0.3827; Current Batch = 90\n",
            "55/67 [=======================>......] - ETA: 6s - loss: 1.3769 - categorical_accuracy: 0.3758; Current Batch = 91\n",
            "56/67 [========================>.....] - ETA: 5s - loss: 1.3859 - categorical_accuracy: 0.3750; Current Batch = 92\n",
            "57/67 [========================>.....] - ETA: 5s - loss: 1.3879 - categorical_accuracy: 0.3743; Current Batch = 93\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.3978 - categorical_accuracy: 0.3736; Current Batch = 94\n",
            "59/67 [=========================>....] - ETA: 3s - loss: 1.3964 - categorical_accuracy: 0.3785; Current Batch = 95\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.3812 - categorical_accuracy: 0.3889; Current Batch = 96\n",
            "61/67 [==========================>...] - ETA: 2s - loss: 1.3840 - categorical_accuracy: 0.3880; Current Batch = 97\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.3763 - categorical_accuracy: 0.3925; Current Batch = 98\n",
            "63/67 [===========================>..] - ETA: 1s - loss: 1.3746 - categorical_accuracy: 0.3915; Current Batch = 99\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.3937 - categorical_accuracy: 0.3854; Current Batch = 100\n",
            "65/67 [============================>.] - ETA: 0s - loss: 1.4080 - categorical_accuracy: 0.3846; Current Batch = 101\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.4066 - categorical_accuracy: 0.3838; Current Batch = 102\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.4034 - categorical_accuracy: 0.3881; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "\n",
            "Epoch 16: saving model to model_init_2023-08-3109_43_59.510610/model-00016-1.40335-0.38806-1.58847-0.36000.h5\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "67/67 [==============================] - 55s 830ms/step - loss: 1.4034 - categorical_accuracy: 0.3881 - val_loss: 1.5885 - val_categorical_accuracy: 0.3600 - lr: 2.0000e-04\n",
            "Epoch 17/25\n",
            "; Current Batch = 103\n",
            " 1/67 [..............................] - ETA: 3s - loss: 1.2444 - categorical_accuracy: 0.3333; Current Batch = 104\n",
            " 2/67 [..............................] - ETA: 33s - loss: 0.9824 - categorical_accuracy: 0.6667; Current Batch = 105\n",
            " 3/67 [>.............................] - ETA: 39s - loss: 1.4453 - categorical_accuracy: 0.5556; Current Batch = 106\n",
            " 4/67 [>.............................] - ETA: 35s - loss: 1.3081 - categorical_accuracy: 0.5833; Current Batch = 107\n",
            " 5/67 [=>............................] - ETA: 41s - loss: 1.3970 - categorical_accuracy: 0.5333; Current Batch = 108\n",
            " 6/67 [=>............................] - ETA: 41s - loss: 1.2776 - categorical_accuracy: 0.6111; Current Batch = 109\n",
            " 7/67 [==>...........................] - ETA: 39s - loss: 1.2554 - categorical_accuracy: 0.6190; Current Batch = 110\n",
            " 8/67 [==>...........................] - ETA: 39s - loss: 1.1918 - categorical_accuracy: 0.6250; Current Batch = 111\n",
            " 9/67 [===>..........................] - ETA: 38s - loss: 1.2283 - categorical_accuracy: 0.5556; Current Batch = 112\n",
            "10/67 [===>..........................] - ETA: 35s - loss: 1.2305 - categorical_accuracy: 0.5667; Current Batch = 113\n",
            "11/67 [===>..........................] - ETA: 34s - loss: 1.3130 - categorical_accuracy: 0.5152; Current Batch = 114\n",
            "12/67 [====>.........................] - ETA: 35s - loss: 1.3277 - categorical_accuracy: 0.4722; Current Batch = 115\n",
            "13/67 [====>.........................] - ETA: 36s - loss: 1.3555 - categorical_accuracy: 0.4359; Current Batch = 116\n",
            "14/67 [=====>........................] - ETA: 35s - loss: 1.3004 - categorical_accuracy: 0.4524; Current Batch = 117\n",
            "15/67 [=====>........................] - ETA: 35s - loss: 1.3478 - categorical_accuracy: 0.4222; Current Batch = 118\n",
            "16/67 [======>.......................] - ETA: 33s - loss: 1.3602 - categorical_accuracy: 0.3958; Current Batch = 119\n",
            "17/67 [======>.......................] - ETA: 32s - loss: 1.3398 - categorical_accuracy: 0.4118; Current Batch = 120\n",
            "18/67 [=======>......................] - ETA: 32s - loss: 1.3859 - categorical_accuracy: 0.3889; Current Batch = 121\n",
            "19/67 [=======>......................] - ETA: 31s - loss: 1.3770 - categorical_accuracy: 0.3860; Current Batch = 122\n",
            "20/67 [=======>......................] - ETA: 30s - loss: 1.3535 - categorical_accuracy: 0.4000; Current Batch = 123\n",
            "21/67 [========>.....................] - ETA: 29s - loss: 1.3535 - categorical_accuracy: 0.3968; Current Batch = 124\n",
            "22/67 [========>.....................] - ETA: 28s - loss: 1.3580 - categorical_accuracy: 0.4091; Current Batch = 125\n",
            "23/67 [=========>....................] - ETA: 27s - loss: 1.4112 - categorical_accuracy: 0.3913; Current Batch = 126\n",
            "24/67 [=========>....................] - ETA: 26s - loss: 1.4159 - categorical_accuracy: 0.3889; Current Batch = 127\n",
            "25/67 [==========>...................] - ETA: 25s - loss: 1.4237 - categorical_accuracy: 0.3867; Current Batch = 128\n",
            "26/67 [==========>...................] - ETA: 25s - loss: 1.4430 - categorical_accuracy: 0.3846; Current Batch = 129\n",
            "27/67 [===========>..................] - ETA: 24s - loss: 1.4109 - categorical_accuracy: 0.4074; Current Batch = 130\n",
            "28/67 [===========>..................] - ETA: 23s - loss: 1.3865 - categorical_accuracy: 0.4167; Current Batch = 131\n",
            "29/67 [===========>..................] - ETA: 22s - loss: 1.4002 - categorical_accuracy: 0.4138; Current Batch = 132\n",
            "30/67 [============>.................] - ETA: 22s - loss: 1.4051 - categorical_accuracy: 0.4111; Current Batch = 133\n",
            "31/67 [============>.................] - ETA: 21s - loss: 1.3852 - categorical_accuracy: 0.4194; Current Batch = 134\n",
            "32/67 [=============>................] - ETA: 21s - loss: 1.3929 - categorical_accuracy: 0.4062; Current Batch = 135\n",
            "33/67 [=============>................] - ETA: 20s - loss: 1.3833 - categorical_accuracy: 0.4242; Current Batch = 136\n",
            "34/67 [==============>...............] - ETA: 19s - loss: 1.4074 - categorical_accuracy: 0.4314; Current Batch = 137\n",
            "35/67 [==============>...............] - ETA: 19s - loss: 1.4031 - categorical_accuracy: 0.4286; Current Batch = 138\n",
            "36/67 [===============>..............] - ETA: 18s - loss: 1.3943 - categorical_accuracy: 0.4259; Current Batch = 139\n",
            "37/67 [===============>..............] - ETA: 18s - loss: 1.3893 - categorical_accuracy: 0.4234; Current Batch = 140\n",
            "38/67 [================>.............] - ETA: 17s - loss: 1.3975 - categorical_accuracy: 0.4211; Current Batch = 141\n",
            "39/67 [================>.............] - ETA: 17s - loss: 1.4010 - categorical_accuracy: 0.4188; Current Batch = 142\n",
            "40/67 [================>.............] - ETA: 16s - loss: 1.4122 - categorical_accuracy: 0.4083; Current Batch = 143\n",
            "41/67 [=================>............] - ETA: 15s - loss: 1.4010 - categorical_accuracy: 0.4146; Current Batch = 144\n",
            "42/67 [=================>............] - ETA: 14s - loss: 1.4035 - categorical_accuracy: 0.4127; Current Batch = 145\n",
            "43/67 [==================>...........] - ETA: 14s - loss: 1.3954 - categorical_accuracy: 0.4186; Current Batch = 146\n",
            "44/67 [==================>...........] - ETA: 13s - loss: 1.3969 - categorical_accuracy: 0.4242; Current Batch = 147\n",
            "45/67 [===================>..........] - ETA: 12s - loss: 1.4048 - categorical_accuracy: 0.4222; Current Batch = 148\n",
            "46/67 [===================>..........] - ETA: 12s - loss: 1.4073 - categorical_accuracy: 0.4203; Current Batch = 149\n",
            "47/67 [====================>.........] - ETA: 11s - loss: 1.4132 - categorical_accuracy: 0.4255; Current Batch = 150\n",
            "48/67 [====================>.........] - ETA: 11s - loss: 1.4121 - categorical_accuracy: 0.4236; Current Batch = 151\n",
            "49/67 [====================>.........] - ETA: 10s - loss: 1.4236 - categorical_accuracy: 0.4218; Current Batch = 152\n",
            "50/67 [=====================>........] - ETA: 9s - loss: 1.4291 - categorical_accuracy: 0.4200 ; Current Batch = 153\n",
            "51/67 [=====================>........] - ETA: 9s - loss: 1.4256 - categorical_accuracy: 0.4248; Current Batch = 154\n",
            "52/67 [======================>.......] - ETA: 8s - loss: 1.4230 - categorical_accuracy: 0.4231; Current Batch = 155\n",
            "53/67 [======================>.......] - ETA: 7s - loss: 1.4231 - categorical_accuracy: 0.4214; Current Batch = 156\n",
            "54/67 [=======================>......] - ETA: 7s - loss: 1.4459 - categorical_accuracy: 0.4136; Current Batch = 157\n",
            "55/67 [=======================>......] - ETA: 6s - loss: 1.4481 - categorical_accuracy: 0.4121; Current Batch = 158\n",
            "56/67 [========================>.....] - ETA: 6s - loss: 1.4569 - categorical_accuracy: 0.4048; Current Batch = 159\n",
            "57/67 [========================>.....] - ETA: 5s - loss: 1.4485 - categorical_accuracy: 0.4035; Current Batch = 160\n",
            "58/67 [========================>.....] - ETA: 5s - loss: 1.4532 - categorical_accuracy: 0.3966; Current Batch = 161\n",
            "59/67 [=========================>....] - ETA: 4s - loss: 1.4655 - categorical_accuracy: 0.3898; Current Batch = 162\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.4779 - categorical_accuracy: 0.3833; Current Batch = 163\n",
            "61/67 [==========================>...] - ETA: 3s - loss: 1.4725 - categorical_accuracy: 0.3825; Current Batch = 164\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.4652 - categorical_accuracy: 0.3871; Current Batch = 165\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.4632 - categorical_accuracy: 0.3915; Current Batch = 166\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.4591 - categorical_accuracy: 0.3906; Current Batch = 167\n",
            "65/67 [============================>.] - ETA: 1s - loss: 1.4551 - categorical_accuracy: 0.3897; Current Batch = 168\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.4584 - categorical_accuracy: 0.3889; Current Batch = 169\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.4535 - categorical_accuracy: 0.3930; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "\n",
            "Epoch 17: saving model to model_init_2023-08-3109_43_59.510610/model-00017-1.45346-0.39303-1.50756-0.44000.h5\n",
            "67/67 [==============================] - 58s 884ms/step - loss: 1.4535 - categorical_accuracy: 0.3930 - val_loss: 1.5076 - val_categorical_accuracy: 0.4400 - lr: 4.0000e-05\n",
            "Epoch 18/25\n",
            "; Current Batch = 170\n",
            " 1/67 [..............................] - ETA: 4s - loss: 2.1387 - categorical_accuracy: 0.0000e+00; Current Batch = 171\n",
            " 2/67 [..............................] - ETA: 34s - loss: 2.0852 - categorical_accuracy: 0.3333   ; Current Batch = 172\n",
            " 3/67 [>.............................] - ETA: 33s - loss: 1.7969 - categorical_accuracy: 0.3333; Current Batch = 173\n",
            " 4/67 [>.............................] - ETA: 32s - loss: 1.5933 - categorical_accuracy: 0.4167; Current Batch = 174\n",
            " 5/67 [=>............................] - ETA: 28s - loss: 1.7051 - categorical_accuracy: 0.3333; Current Batch = 175\n",
            " 6/67 [=>............................] - ETA: 26s - loss: 1.8743 - categorical_accuracy: 0.3333; Current Batch = 176\n",
            " 7/67 [==>...........................] - ETA: 25s - loss: 1.9084 - categorical_accuracy: 0.2857; Current Batch = 177\n",
            " 8/67 [==>...........................] - ETA: 24s - loss: 1.9382 - categorical_accuracy: 0.2500; Current Batch = 178\n",
            " 9/67 [===>..........................] - ETA: 23s - loss: 1.8523 - categorical_accuracy: 0.2593; Current Batch = 179\n",
            "10/67 [===>..........................] - ETA: 22s - loss: 1.6949 - categorical_accuracy: 0.3333; Current Batch = 180\n",
            "11/67 [===>..........................] - ETA: 22s - loss: 1.7121 - categorical_accuracy: 0.3030; Current Batch = 181\n",
            "12/67 [====>.........................] - ETA: 22s - loss: 1.6243 - categorical_accuracy: 0.3611; Current Batch = 182\n",
            "13/67 [====>.........................] - ETA: 23s - loss: 1.5877 - categorical_accuracy: 0.3590; Current Batch = 183\n",
            "14/67 [=====>........................] - ETA: 24s - loss: 1.5785 - categorical_accuracy: 0.3333; Current Batch = 184\n",
            "15/67 [=====>........................] - ETA: 23s - loss: 1.5715 - categorical_accuracy: 0.3333; Current Batch = 185\n",
            "16/67 [======>.......................] - ETA: 23s - loss: 1.5333 - categorical_accuracy: 0.3542; Current Batch = 186\n",
            "17/67 [======>.......................] - ETA: 23s - loss: 1.5278 - categorical_accuracy: 0.3725; Current Batch = 187\n",
            "18/67 [=======>......................] - ETA: 23s - loss: 1.5537 - categorical_accuracy: 0.3519; Current Batch = 188\n",
            "19/67 [=======>......................] - ETA: 23s - loss: 1.5541 - categorical_accuracy: 0.3333; Current Batch = 189\n",
            "20/67 [=======>......................] - ETA: 23s - loss: 1.5689 - categorical_accuracy: 0.3167; Current Batch = 190\n",
            "21/67 [========>.....................] - ETA: 22s - loss: 1.5580 - categorical_accuracy: 0.3175; Current Batch = 191\n",
            "22/67 [========>.....................] - ETA: 21s - loss: 1.5594 - categorical_accuracy: 0.3333; Current Batch = 192\n",
            "23/67 [=========>....................] - ETA: 21s - loss: 1.5656 - categorical_accuracy: 0.3333; Current Batch = 193\n",
            "24/67 [=========>....................] - ETA: 21s - loss: 1.5647 - categorical_accuracy: 0.3333; Current Batch = 194\n",
            "25/67 [==========>...................] - ETA: 21s - loss: 1.5720 - categorical_accuracy: 0.3333; Current Batch = 195\n",
            "26/67 [==========>...................] - ETA: 20s - loss: 1.5617 - categorical_accuracy: 0.3333; Current Batch = 196\n",
            "27/67 [===========>..................] - ETA: 19s - loss: 1.5446 - categorical_accuracy: 0.3457; Current Batch = 197\n",
            "28/67 [===========>..................] - ETA: 19s - loss: 1.5616 - categorical_accuracy: 0.3452; Current Batch = 198\n",
            "29/67 [===========>..................] - ETA: 19s - loss: 1.5272 - categorical_accuracy: 0.3678; Current Batch = 199\n",
            "30/67 [============>.................] - ETA: 18s - loss: 1.5140 - categorical_accuracy: 0.3667; Current Batch = 200\n",
            "31/67 [============>.................] - ETA: 17s - loss: 1.4895 - categorical_accuracy: 0.3871; Current Batch = 201\n",
            "32/67 [=============>................] - ETA: 17s - loss: 1.4701 - categorical_accuracy: 0.3958; Current Batch = 202\n",
            "33/67 [=============>................] - ETA: 17s - loss: 1.4650 - categorical_accuracy: 0.3939; Current Batch = 203\n",
            "34/67 [==============>...............] - ETA: 16s - loss: 1.4564 - categorical_accuracy: 0.4020; Current Batch = 204\n",
            "35/67 [==============>...............] - ETA: 15s - loss: 1.4465 - categorical_accuracy: 0.4095; Current Batch = 205\n",
            "36/67 [===============>..............] - ETA: 15s - loss: 1.4530 - categorical_accuracy: 0.4074; Current Batch = 206\n",
            "37/67 [===============>..............] - ETA: 15s - loss: 1.4326 - categorical_accuracy: 0.4234; Current Batch = 207\n",
            "38/67 [================>.............] - ETA: 14s - loss: 1.4454 - categorical_accuracy: 0.4211; Current Batch = 208\n",
            "39/67 [================>.............] - ETA: 14s - loss: 1.4533 - categorical_accuracy: 0.4188; Current Batch = 209\n",
            "40/67 [================>.............] - ETA: 13s - loss: 1.4414 - categorical_accuracy: 0.4167; Current Batch = 210\n",
            "41/67 [=================>............] - ETA: 13s - loss: 1.4600 - categorical_accuracy: 0.4065; Current Batch = 211\n",
            "42/67 [=================>............] - ETA: 13s - loss: 1.4466 - categorical_accuracy: 0.4127; Current Batch = 212\n",
            "43/67 [==================>...........] - ETA: 12s - loss: 1.4277 - categorical_accuracy: 0.4264; Current Batch = 213\n",
            "44/67 [==================>...........] - ETA: 12s - loss: 1.4320 - categorical_accuracy: 0.4167; Current Batch = 214\n",
            "45/67 [===================>..........] - ETA: 11s - loss: 1.4403 - categorical_accuracy: 0.4148; Current Batch = 215\n",
            "46/67 [===================>..........] - ETA: 11s - loss: 1.4436 - categorical_accuracy: 0.4130; Current Batch = 216\n",
            "47/67 [====================>.........] - ETA: 10s - loss: 1.4540 - categorical_accuracy: 0.4113; Current Batch = 217\n",
            "48/67 [====================>.........] - ETA: 9s - loss: 1.4583 - categorical_accuracy: 0.4097 ; Current Batch = 218\n",
            "49/67 [====================>.........] - ETA: 9s - loss: 1.4631 - categorical_accuracy: 0.4014; Current Batch = 219\n",
            "50/67 [=====================>........] - ETA: 8s - loss: 1.4538 - categorical_accuracy: 0.4000; Current Batch = 220\n",
            "51/67 [=====================>........] - ETA: 8s - loss: 1.4591 - categorical_accuracy: 0.3922; Num of batches = 221\n",
            "; Current Batch = 0\n",
            "52/67 [======================>.......] - ETA: 7s - loss: 1.4609 - categorical_accuracy: 0.3846; Current Batch = 1\n",
            "53/67 [======================>.......] - ETA: 7s - loss: 1.4634 - categorical_accuracy: 0.3774; Current Batch = 2\n",
            "54/67 [=======================>......] - ETA: 6s - loss: 1.4668 - categorical_accuracy: 0.3765; Current Batch = 3\n",
            "55/67 [=======================>......] - ETA: 6s - loss: 1.4607 - categorical_accuracy: 0.3818; Current Batch = 4\n",
            "56/67 [========================>.....] - ETA: 5s - loss: 1.4520 - categorical_accuracy: 0.3869; Current Batch = 5\n",
            "57/67 [========================>.....] - ETA: 5s - loss: 1.4598 - categorical_accuracy: 0.3860; Current Batch = 6\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.4536 - categorical_accuracy: 0.3851; Current Batch = 7\n",
            "59/67 [=========================>....] - ETA: 4s - loss: 1.4585 - categorical_accuracy: 0.3785; Current Batch = 8\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.4522 - categorical_accuracy: 0.3778; Current Batch = 9\n",
            "61/67 [==========================>...] - ETA: 3s - loss: 1.4539 - categorical_accuracy: 0.3770; Current Batch = 10\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.4522 - categorical_accuracy: 0.3763; Current Batch = 11\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.4566 - categorical_accuracy: 0.3757; Current Batch = 12\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.4650 - categorical_accuracy: 0.3750; Current Batch = 13\n",
            "65/67 [============================>.] - ETA: 1s - loss: 1.4662 - categorical_accuracy: 0.3744; Current Batch = 14\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.4743 - categorical_accuracy: 0.3687; Current Batch = 15\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.4796 - categorical_accuracy: 0.3682; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "\n",
            "Epoch 18: saving model to model_init_2023-08-3109_43_59.510610/model-00018-1.47959-0.36816-1.40445-0.44000.h5\n",
            "67/67 [==============================] - 55s 828ms/step - loss: 1.4796 - categorical_accuracy: 0.3682 - val_loss: 1.4045 - val_categorical_accuracy: 0.4400 - lr: 4.0000e-05\n",
            "Epoch 19/25\n",
            "; Current Batch = 16\n",
            " 1/67 [..............................] - ETA: 4s - loss: 1.0588 - categorical_accuracy: 0.6667; Current Batch = 17\n",
            " 2/67 [..............................] - ETA: 35s - loss: 1.4436 - categorical_accuracy: 0.5000; Current Batch = 18\n",
            " 3/67 [>.............................] - ETA: 28s - loss: 1.3110 - categorical_accuracy: 0.4444; Current Batch = 19\n",
            " 4/67 [>.............................] - ETA: 28s - loss: 1.4640 - categorical_accuracy: 0.4167; Current Batch = 20\n",
            " 5/67 [=>............................] - ETA: 31s - loss: 1.5277 - categorical_accuracy: 0.4000; Current Batch = 21\n",
            " 6/67 [=>............................] - ETA: 31s - loss: 1.4922 - categorical_accuracy: 0.3889; Current Batch = 22\n",
            " 7/67 [==>...........................] - ETA: 29s - loss: 1.5447 - categorical_accuracy: 0.3810; Current Batch = 23\n",
            " 8/67 [==>...........................] - ETA: 27s - loss: 1.5264 - categorical_accuracy: 0.3333; Current Batch = 24\n",
            " 9/67 [===>..........................] - ETA: 28s - loss: 1.5550 - categorical_accuracy: 0.2963; Current Batch = 25\n",
            "10/67 [===>..........................] - ETA: 31s - loss: 1.5685 - categorical_accuracy: 0.3000; Current Batch = 26\n",
            "11/67 [===>..........................] - ETA: 30s - loss: 1.5489 - categorical_accuracy: 0.3030; Current Batch = 27\n",
            "12/67 [====>.........................] - ETA: 29s - loss: 1.5869 - categorical_accuracy: 0.3056; Current Batch = 28\n",
            "13/67 [====>.........................] - ETA: 28s - loss: 1.5736 - categorical_accuracy: 0.3077; Current Batch = 29\n",
            "14/67 [=====>........................] - ETA: 27s - loss: 1.5636 - categorical_accuracy: 0.2857; Current Batch = 30\n",
            "15/67 [=====>........................] - ETA: 28s - loss: 1.5529 - categorical_accuracy: 0.3111; Current Batch = 31\n",
            "16/67 [======>.......................] - ETA: 27s - loss: 1.5348 - categorical_accuracy: 0.3125; Current Batch = 32\n",
            "17/67 [======>.......................] - ETA: 26s - loss: 1.4968 - categorical_accuracy: 0.3333; Current Batch = 33\n",
            "18/67 [=======>......................] - ETA: 26s - loss: 1.4999 - categorical_accuracy: 0.3333; Current Batch = 34\n",
            "19/67 [=======>......................] - ETA: 26s - loss: 1.4646 - categorical_accuracy: 0.3684; Current Batch = 35\n",
            "20/67 [=======>......................] - ETA: 25s - loss: 1.4922 - categorical_accuracy: 0.3500; Current Batch = 36\n",
            "21/67 [========>.....................] - ETA: 25s - loss: 1.4947 - categorical_accuracy: 0.3492; Current Batch = 37\n",
            "22/67 [========>.....................] - ETA: 24s - loss: 1.4689 - categorical_accuracy: 0.3636; Current Batch = 38\n",
            "23/67 [=========>....................] - ETA: 23s - loss: 1.5188 - categorical_accuracy: 0.3478; Current Batch = 39\n",
            "24/67 [=========>....................] - ETA: 23s - loss: 1.5005 - categorical_accuracy: 0.3472; Current Batch = 40\n",
            "25/67 [==========>...................] - ETA: 22s - loss: 1.4842 - categorical_accuracy: 0.3467; Current Batch = 41\n",
            "26/67 [==========>...................] - ETA: 22s - loss: 1.4982 - categorical_accuracy: 0.3333; Current Batch = 42\n",
            "27/67 [===========>..................] - ETA: 21s - loss: 1.4782 - categorical_accuracy: 0.3457; Current Batch = 43\n",
            "28/67 [===========>..................] - ETA: 21s - loss: 1.4714 - categorical_accuracy: 0.3452; Current Batch = 44\n",
            "29/67 [===========>..................] - ETA: 21s - loss: 1.5190 - categorical_accuracy: 0.3333; Current Batch = 45\n",
            "30/67 [============>.................] - ETA: 20s - loss: 1.4994 - categorical_accuracy: 0.3444; Current Batch = 46\n",
            "31/67 [============>.................] - ETA: 19s - loss: 1.4988 - categorical_accuracy: 0.3441; Current Batch = 47\n",
            "32/67 [=============>................] - ETA: 19s - loss: 1.4981 - categorical_accuracy: 0.3438; Current Batch = 48\n",
            "33/67 [=============>................] - ETA: 18s - loss: 1.5162 - categorical_accuracy: 0.3434; Current Batch = 49\n",
            "34/67 [==============>...............] - ETA: 18s - loss: 1.5164 - categorical_accuracy: 0.3431; Current Batch = 50\n",
            "35/67 [==============>...............] - ETA: 17s - loss: 1.5082 - categorical_accuracy: 0.3333; Current Batch = 51\n",
            "36/67 [===============>..............] - ETA: 17s - loss: 1.4975 - categorical_accuracy: 0.3426; Current Batch = 52\n",
            "37/67 [===============>..............] - ETA: 16s - loss: 1.5184 - categorical_accuracy: 0.3333; Current Batch = 53\n",
            "38/67 [================>.............] - ETA: 16s - loss: 1.4989 - categorical_accuracy: 0.3421; Current Batch = 54\n",
            "39/67 [================>.............] - ETA: 15s - loss: 1.5069 - categorical_accuracy: 0.3419; Current Batch = 55\n",
            "40/67 [================>.............] - ETA: 15s - loss: 1.5033 - categorical_accuracy: 0.3417; Current Batch = 56\n",
            "41/67 [=================>............] - ETA: 14s - loss: 1.5252 - categorical_accuracy: 0.3333; Current Batch = 57\n",
            "42/67 [=================>............] - ETA: 13s - loss: 1.5166 - categorical_accuracy: 0.3333; Current Batch = 58\n",
            "43/67 [==================>...........] - ETA: 13s - loss: 1.5101 - categorical_accuracy: 0.3333; Current Batch = 59\n",
            "44/67 [==================>...........] - ETA: 12s - loss: 1.4995 - categorical_accuracy: 0.3409; Current Batch = 60\n",
            "45/67 [===================>..........] - ETA: 11s - loss: 1.4961 - categorical_accuracy: 0.3407; Current Batch = 61\n",
            "46/67 [===================>..........] - ETA: 11s - loss: 1.5047 - categorical_accuracy: 0.3406; Current Batch = 62\n",
            "47/67 [====================>.........] - ETA: 10s - loss: 1.4954 - categorical_accuracy: 0.3475; Current Batch = 63\n",
            "48/67 [====================>.........] - ETA: 10s - loss: 1.4907 - categorical_accuracy: 0.3472; Current Batch = 64\n",
            "49/67 [====================>.........] - ETA: 9s - loss: 1.4801 - categorical_accuracy: 0.3537 ; Current Batch = 65\n",
            "50/67 [=====================>........] - ETA: 9s - loss: 1.4736 - categorical_accuracy: 0.3533; Current Batch = 66\n",
            "51/67 [=====================>........] - ETA: 8s - loss: 1.4844 - categorical_accuracy: 0.3464; Current Batch = 67\n",
            "52/67 [======================>.......] - ETA: 8s - loss: 1.4930 - categorical_accuracy: 0.3397; Current Batch = 68\n",
            "53/67 [======================>.......] - ETA: 7s - loss: 1.5058 - categorical_accuracy: 0.3396; Current Batch = 69\n",
            "54/67 [=======================>......] - ETA: 7s - loss: 1.5001 - categorical_accuracy: 0.3395; Current Batch = 70\n",
            "55/67 [=======================>......] - ETA: 6s - loss: 1.4972 - categorical_accuracy: 0.3394; Current Batch = 71\n",
            "56/67 [========================>.....] - ETA: 5s - loss: 1.5168 - categorical_accuracy: 0.3333; Current Batch = 72\n",
            "57/67 [========================>.....] - ETA: 5s - loss: 1.5085 - categorical_accuracy: 0.3392; Current Batch = 73\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.5136 - categorical_accuracy: 0.3391; Current Batch = 74\n",
            "59/67 [=========================>....] - ETA: 4s - loss: 1.5122 - categorical_accuracy: 0.3390; Current Batch = 75\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.5225 - categorical_accuracy: 0.3333; Current Batch = 76\n",
            "61/67 [==========================>...] - ETA: 3s - loss: 1.5219 - categorical_accuracy: 0.3388; Current Batch = 77\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.5423 - categorical_accuracy: 0.3333; Current Batch = 78\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.5397 - categorical_accuracy: 0.3333; Current Batch = 79\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.5337 - categorical_accuracy: 0.3385; Current Batch = 80\n",
            "65/67 [============================>.] - ETA: 1s - loss: 1.5405 - categorical_accuracy: 0.3333; Current Batch = 81\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.5422 - categorical_accuracy: 0.3283; Current Batch = 82\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.5401 - categorical_accuracy: 0.3333; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "\n",
            "Epoch 19: saving model to model_init_2023-08-3109_43_59.510610/model-00019-1.54005-0.33333-1.44306-0.45000.h5\n",
            "67/67 [==============================] - 56s 846ms/step - loss: 1.5401 - categorical_accuracy: 0.3333 - val_loss: 1.4431 - val_categorical_accuracy: 0.4500 - lr: 4.0000e-05\n",
            "Epoch 20/25\n",
            "; Current Batch = 83\n",
            " 1/67 [..............................] - ETA: 4s - loss: 1.7277 - categorical_accuracy: 0.3333; Current Batch = 84\n",
            " 2/67 [..............................] - ETA: 33s - loss: 2.0147 - categorical_accuracy: 0.1667; Current Batch = 85\n",
            " 3/67 [>.............................] - ETA: 38s - loss: 1.9006 - categorical_accuracy: 0.2222; Current Batch = 86\n",
            " 4/67 [>.............................] - ETA: 32s - loss: 1.8497 - categorical_accuracy: 0.2500; Current Batch = 87\n",
            " 5/67 [=>............................] - ETA: 29s - loss: 1.9201 - categorical_accuracy: 0.2667; Current Batch = 88\n",
            " 6/67 [=>............................] - ETA: 29s - loss: 1.9071 - categorical_accuracy: 0.2778; Current Batch = 89\n",
            " 7/67 [==>...........................] - ETA: 29s - loss: 1.8422 - categorical_accuracy: 0.3333; Current Batch = 90\n",
            " 8/67 [==>...........................] - ETA: 27s - loss: 1.8219 - categorical_accuracy: 0.2917; Current Batch = 91\n",
            " 9/67 [===>..........................] - ETA: 27s - loss: 1.7821 - categorical_accuracy: 0.2963; Current Batch = 92\n",
            "10/67 [===>..........................] - ETA: 28s - loss: 1.8070 - categorical_accuracy: 0.3000; Current Batch = 93\n",
            "11/67 [===>..........................] - ETA: 31s - loss: 1.7291 - categorical_accuracy: 0.3333; Current Batch = 94\n",
            "12/67 [====>.........................] - ETA: 32s - loss: 1.7059 - categorical_accuracy: 0.3333; Current Batch = 95\n",
            "13/67 [====>.........................] - ETA: 33s - loss: 1.7276 - categorical_accuracy: 0.3077; Current Batch = 96\n",
            "14/67 [=====>........................] - ETA: 32s - loss: 1.7677 - categorical_accuracy: 0.2857; Current Batch = 97\n",
            "15/67 [=====>........................] - ETA: 31s - loss: 1.7659 - categorical_accuracy: 0.2889; Current Batch = 98\n",
            "16/67 [======>.......................] - ETA: 30s - loss: 1.7619 - categorical_accuracy: 0.2708; Current Batch = 99\n",
            "17/67 [======>.......................] - ETA: 28s - loss: 1.7347 - categorical_accuracy: 0.2745; Current Batch = 100\n",
            "18/67 [=======>......................] - ETA: 28s - loss: 1.7247 - categorical_accuracy: 0.2593; Current Batch = 101\n",
            "19/67 [=======>......................] - ETA: 27s - loss: 1.7486 - categorical_accuracy: 0.2456; Current Batch = 102\n",
            "20/67 [=======>......................] - ETA: 26s - loss: 1.7341 - categorical_accuracy: 0.2500; Current Batch = 103\n",
            "21/67 [========>.....................] - ETA: 25s - loss: 1.7152 - categorical_accuracy: 0.2540; Current Batch = 104\n",
            "22/67 [========>.....................] - ETA: 24s - loss: 1.6933 - categorical_accuracy: 0.2424; Current Batch = 105\n",
            "23/67 [=========>....................] - ETA: 23s - loss: 1.6788 - categorical_accuracy: 0.2464; Current Batch = 106\n",
            "24/67 [=========>....................] - ETA: 23s - loss: 1.6748 - categorical_accuracy: 0.2361; Current Batch = 107\n",
            "25/67 [==========>...................] - ETA: 22s - loss: 1.6734 - categorical_accuracy: 0.2400; Current Batch = 108\n",
            "26/67 [==========>...................] - ETA: 21s - loss: 1.6338 - categorical_accuracy: 0.2692; Current Batch = 109\n",
            "27/67 [===========>..................] - ETA: 21s - loss: 1.6196 - categorical_accuracy: 0.2716; Current Batch = 110\n",
            "28/67 [===========>..................] - ETA: 20s - loss: 1.6226 - categorical_accuracy: 0.2738; Current Batch = 111\n",
            "29/67 [===========>..................] - ETA: 20s - loss: 1.6500 - categorical_accuracy: 0.2644; Current Batch = 112\n",
            "30/67 [============>.................] - ETA: 19s - loss: 1.6454 - categorical_accuracy: 0.2667; Current Batch = 113\n",
            "31/67 [============>.................] - ETA: 19s - loss: 1.6577 - categorical_accuracy: 0.2581; Current Batch = 114\n",
            "32/67 [=============>................] - ETA: 18s - loss: 1.6708 - categorical_accuracy: 0.2500; Current Batch = 115\n",
            "33/67 [=============>................] - ETA: 17s - loss: 1.6668 - categorical_accuracy: 0.2525; Current Batch = 116\n",
            "34/67 [==============>...............] - ETA: 17s - loss: 1.6664 - categorical_accuracy: 0.2451; Current Batch = 117\n",
            "35/67 [==============>...............] - ETA: 16s - loss: 1.6748 - categorical_accuracy: 0.2381; Current Batch = 118\n",
            "36/67 [===============>..............] - ETA: 15s - loss: 1.6919 - categorical_accuracy: 0.2407; Current Batch = 119\n",
            "37/67 [===============>..............] - ETA: 15s - loss: 1.6792 - categorical_accuracy: 0.2432; Current Batch = 120\n",
            "38/67 [================>.............] - ETA: 15s - loss: 1.6877 - categorical_accuracy: 0.2456; Current Batch = 121\n",
            "39/67 [================>.............] - ETA: 14s - loss: 1.6723 - categorical_accuracy: 0.2564; Current Batch = 122\n",
            "40/67 [================>.............] - ETA: 14s - loss: 1.6664 - categorical_accuracy: 0.2500; Current Batch = 123\n",
            "41/67 [=================>............] - ETA: 13s - loss: 1.6664 - categorical_accuracy: 0.2520; Current Batch = 124\n",
            "42/67 [=================>............] - ETA: 13s - loss: 1.6730 - categorical_accuracy: 0.2460; Current Batch = 125\n",
            "43/67 [==================>...........] - ETA: 12s - loss: 1.6606 - categorical_accuracy: 0.2481; Current Batch = 126\n",
            "44/67 [==================>...........] - ETA: 12s - loss: 1.6434 - categorical_accuracy: 0.2500; Current Batch = 127\n",
            "45/67 [===================>..........] - ETA: 11s - loss: 1.6518 - categorical_accuracy: 0.2444; Current Batch = 128\n",
            "46/67 [===================>..........] - ETA: 10s - loss: 1.6336 - categorical_accuracy: 0.2536; Current Batch = 129\n",
            "47/67 [====================>.........] - ETA: 10s - loss: 1.6495 - categorical_accuracy: 0.2482; Current Batch = 130\n",
            "48/67 [====================>.........] - ETA: 9s - loss: 1.6447 - categorical_accuracy: 0.2500 ; Current Batch = 131\n",
            "49/67 [====================>.........] - ETA: 9s - loss: 1.6542 - categorical_accuracy: 0.2449; Current Batch = 132\n",
            "50/67 [=====================>........] - ETA: 8s - loss: 1.6490 - categorical_accuracy: 0.2533; Current Batch = 133\n",
            "51/67 [=====================>........] - ETA: 8s - loss: 1.6377 - categorical_accuracy: 0.2614; Current Batch = 134\n",
            "52/67 [======================>.......] - ETA: 7s - loss: 1.6210 - categorical_accuracy: 0.2756; Current Batch = 135\n",
            "53/67 [======================>.......] - ETA: 7s - loss: 1.6068 - categorical_accuracy: 0.2830; Current Batch = 136\n",
            "54/67 [=======================>......] - ETA: 6s - loss: 1.6076 - categorical_accuracy: 0.2778; Current Batch = 137\n",
            "55/67 [=======================>......] - ETA: 6s - loss: 1.6112 - categorical_accuracy: 0.2788; Current Batch = 138\n",
            "56/67 [========================>.....] - ETA: 5s - loss: 1.6059 - categorical_accuracy: 0.2798; Current Batch = 139\n",
            "57/67 [========================>.....] - ETA: 5s - loss: 1.5998 - categorical_accuracy: 0.2807; Current Batch = 140\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.5995 - categorical_accuracy: 0.2816; Current Batch = 141\n",
            "59/67 [=========================>....] - ETA: 4s - loss: 1.5951 - categorical_accuracy: 0.2881; Current Batch = 142\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.5908 - categorical_accuracy: 0.2944; Current Batch = 143\n",
            "61/67 [==========================>...] - ETA: 3s - loss: 1.5924 - categorical_accuracy: 0.2951; Current Batch = 144\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.5945 - categorical_accuracy: 0.2957; Current Batch = 145\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.5948 - categorical_accuracy: 0.3016; Current Batch = 146\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.5988 - categorical_accuracy: 0.2969; Current Batch = 147\n",
            "65/67 [============================>.] - ETA: 1s - loss: 1.5965 - categorical_accuracy: 0.2974; Current Batch = 148\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.5935 - categorical_accuracy: 0.3030; Current Batch = 149\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.5939 - categorical_accuracy: 0.3035; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "\n",
            "Epoch 20: saving model to model_init_2023-08-3109_43_59.510610/model-00020-1.59388-0.30348-1.54392-0.44000.h5\n",
            "\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "67/67 [==============================] - 57s 865ms/step - loss: 1.5939 - categorical_accuracy: 0.3035 - val_loss: 1.5439 - val_categorical_accuracy: 0.4400 - lr: 4.0000e-05\n",
            "Epoch 21/25\n",
            "; Current Batch = 150\n",
            " 1/67 [..............................] - ETA: 4s - loss: 2.6983 - categorical_accuracy: 0.0000e+00; Current Batch = 151\n",
            " 2/67 [..............................] - ETA: 34s - loss: 2.0234 - categorical_accuracy: 0.3333   ; Current Batch = 152\n",
            " 3/67 [>.............................] - ETA: 38s - loss: 1.8567 - categorical_accuracy: 0.3333; Current Batch = 153\n",
            " 4/67 [>.............................] - ETA: 36s - loss: 1.9219 - categorical_accuracy: 0.2500; Current Batch = 154\n",
            " 5/67 [=>............................] - ETA: 39s - loss: 1.8466 - categorical_accuracy: 0.2667; Current Batch = 155\n",
            " 6/67 [=>............................] - ETA: 40s - loss: 1.8177 - categorical_accuracy: 0.2778; Current Batch = 156\n",
            " 7/67 [==>...........................] - ETA: 40s - loss: 1.7268 - categorical_accuracy: 0.2857; Current Batch = 157\n",
            " 8/67 [==>...........................] - ETA: 39s - loss: 1.7928 - categorical_accuracy: 0.2500; Current Batch = 158\n",
            " 9/67 [===>..........................] - ETA: 39s - loss: 1.7523 - categorical_accuracy: 0.2222; Current Batch = 159\n",
            "10/67 [===>..........................] - ETA: 38s - loss: 1.7383 - categorical_accuracy: 0.2000; Current Batch = 160\n",
            "11/67 [===>..........................] - ETA: 35s - loss: 1.6202 - categorical_accuracy: 0.2727; Current Batch = 161\n",
            "12/67 [====>.........................] - ETA: 33s - loss: 1.7238 - categorical_accuracy: 0.2500; Current Batch = 162\n",
            "13/67 [====>.........................] - ETA: 33s - loss: 1.6709 - categorical_accuracy: 0.2821; Current Batch = 163\n",
            "14/67 [=====>........................] - ETA: 31s - loss: 1.6919 - categorical_accuracy: 0.2619; Current Batch = 164\n",
            "15/67 [=====>........................] - ETA: 30s - loss: 1.6586 - categorical_accuracy: 0.2667; Current Batch = 165\n",
            "16/67 [======>.......................] - ETA: 28s - loss: 1.6076 - categorical_accuracy: 0.2917; Current Batch = 166\n",
            "17/67 [======>.......................] - ETA: 27s - loss: 1.6363 - categorical_accuracy: 0.2941; Current Batch = 167\n",
            "18/67 [=======>......................] - ETA: 26s - loss: 1.6157 - categorical_accuracy: 0.3148; Current Batch = 168\n",
            "19/67 [=======>......................] - ETA: 26s - loss: 1.5879 - categorical_accuracy: 0.3333; Current Batch = 169\n",
            "20/67 [=======>......................] - ETA: 26s - loss: 1.6040 - categorical_accuracy: 0.3333; Current Batch = 170\n",
            "21/67 [========>.....................] - ETA: 25s - loss: 1.5703 - categorical_accuracy: 0.3651; Current Batch = 171\n",
            "22/67 [========>.....................] - ETA: 24s - loss: 1.6037 - categorical_accuracy: 0.3485; Current Batch = 172\n",
            "23/67 [=========>....................] - ETA: 23s - loss: 1.5730 - categorical_accuracy: 0.3623; Current Batch = 173\n",
            "24/67 [=========>....................] - ETA: 22s - loss: 1.5594 - categorical_accuracy: 0.3611; Current Batch = 174\n",
            "25/67 [==========>...................] - ETA: 22s - loss: 1.5786 - categorical_accuracy: 0.3600; Current Batch = 175\n",
            "26/67 [==========>...................] - ETA: 22s - loss: 1.5871 - categorical_accuracy: 0.3590; Current Batch = 176\n",
            "27/67 [===========>..................] - ETA: 21s - loss: 1.5980 - categorical_accuracy: 0.3457; Current Batch = 177\n",
            "28/67 [===========>..................] - ETA: 20s - loss: 1.5737 - categorical_accuracy: 0.3571; Current Batch = 178\n",
            "29/67 [===========>..................] - ETA: 20s - loss: 1.5802 - categorical_accuracy: 0.3448; Current Batch = 179\n",
            "30/67 [============>.................] - ETA: 19s - loss: 1.5537 - categorical_accuracy: 0.3556; Current Batch = 180\n",
            "31/67 [============>.................] - ETA: 19s - loss: 1.5342 - categorical_accuracy: 0.3656; Current Batch = 181\n",
            "32/67 [=============>................] - ETA: 19s - loss: 1.5317 - categorical_accuracy: 0.3542; Current Batch = 182\n",
            "33/67 [=============>................] - ETA: 18s - loss: 1.5214 - categorical_accuracy: 0.3535; Current Batch = 183\n",
            "34/67 [==============>...............] - ETA: 18s - loss: 1.5407 - categorical_accuracy: 0.3529; Current Batch = 184\n",
            "35/67 [==============>...............] - ETA: 18s - loss: 1.5505 - categorical_accuracy: 0.3429; Current Batch = 185\n",
            "36/67 [===============>..............] - ETA: 17s - loss: 1.5600 - categorical_accuracy: 0.3426; Current Batch = 186\n",
            "37/67 [===============>..............] - ETA: 16s - loss: 1.5408 - categorical_accuracy: 0.3514; Current Batch = 187\n",
            "38/67 [================>.............] - ETA: 16s - loss: 1.5445 - categorical_accuracy: 0.3509; Current Batch = 188\n",
            "39/67 [================>.............] - ETA: 15s - loss: 1.5296 - categorical_accuracy: 0.3590; Current Batch = 189\n",
            "40/67 [================>.............] - ETA: 15s - loss: 1.5263 - categorical_accuracy: 0.3583; Current Batch = 190\n",
            "41/67 [=================>............] - ETA: 14s - loss: 1.5317 - categorical_accuracy: 0.3577; Current Batch = 191\n",
            "42/67 [=================>............] - ETA: 13s - loss: 1.5148 - categorical_accuracy: 0.3730; Current Batch = 192\n",
            "43/67 [==================>...........] - ETA: 13s - loss: 1.5061 - categorical_accuracy: 0.3798; Current Batch = 193\n",
            "44/67 [==================>...........] - ETA: 12s - loss: 1.5145 - categorical_accuracy: 0.3788; Current Batch = 194\n",
            "45/67 [===================>..........] - ETA: 12s - loss: 1.5220 - categorical_accuracy: 0.3704; Current Batch = 195\n",
            "46/67 [===================>..........] - ETA: 11s - loss: 1.5137 - categorical_accuracy: 0.3696; Current Batch = 196\n",
            "47/67 [====================>.........] - ETA: 10s - loss: 1.5041 - categorical_accuracy: 0.3759; Current Batch = 197\n",
            "48/67 [====================>.........] - ETA: 10s - loss: 1.5014 - categorical_accuracy: 0.3819; Current Batch = 198\n",
            "49/67 [====================>.........] - ETA: 9s - loss: 1.4994 - categorical_accuracy: 0.3810 ; Current Batch = 199\n",
            "50/67 [=====================>........] - ETA: 9s - loss: 1.4911 - categorical_accuracy: 0.3867; Current Batch = 200\n",
            "51/67 [=====================>........] - ETA: 8s - loss: 1.4903 - categorical_accuracy: 0.3856; Current Batch = 201\n",
            "52/67 [======================>.......] - ETA: 8s - loss: 1.5041 - categorical_accuracy: 0.3782; Current Batch = 202\n",
            "53/67 [======================>.......] - ETA: 7s - loss: 1.4936 - categorical_accuracy: 0.3836; Current Batch = 203\n",
            "54/67 [=======================>......] - ETA: 6s - loss: 1.5084 - categorical_accuracy: 0.3765; Current Batch = 204\n",
            "55/67 [=======================>......] - ETA: 6s - loss: 1.5078 - categorical_accuracy: 0.3697; Current Batch = 205\n",
            "56/67 [========================>.....] - ETA: 5s - loss: 1.5023 - categorical_accuracy: 0.3690; Current Batch = 206\n",
            "57/67 [========================>.....] - ETA: 5s - loss: 1.5105 - categorical_accuracy: 0.3626; Current Batch = 207\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.4931 - categorical_accuracy: 0.3736; Current Batch = 208\n",
            "59/67 [=========================>....] - ETA: 4s - loss: 1.5046 - categorical_accuracy: 0.3729; Current Batch = 209\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.5120 - categorical_accuracy: 0.3722; Current Batch = 210\n",
            "61/67 [==========================>...] - ETA: 3s - loss: 1.5188 - categorical_accuracy: 0.3661; Current Batch = 211\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.5175 - categorical_accuracy: 0.3710; Current Batch = 212\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.5128 - categorical_accuracy: 0.3704; Current Batch = 213\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.5197 - categorical_accuracy: 0.3646; Current Batch = 214\n",
            "65/67 [============================>.] - ETA: 1s - loss: 1.5208 - categorical_accuracy: 0.3641; Current Batch = 215\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.5156 - categorical_accuracy: 0.3636; Current Batch = 216\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.5193 - categorical_accuracy: 0.3632; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "\n",
            "Epoch 21: saving model to model_init_2023-08-3109_43_59.510610/model-00021-1.51931-0.36318-1.25051-0.53000.h5\n",
            "67/67 [==============================] - 56s 847ms/step - loss: 1.5193 - categorical_accuracy: 0.3632 - val_loss: 1.2505 - val_categorical_accuracy: 0.5300 - lr: 8.0000e-06\n",
            "Epoch 22/25\n",
            "; Current Batch = 217\n",
            " 1/67 [..............................] - ETA: 4s - loss: 1.5749 - categorical_accuracy: 0.3333; Current Batch = 218\n",
            " 2/67 [..............................] - ETA: 22s - loss: 1.7828 - categorical_accuracy: 0.3333; Current Batch = 219\n",
            " 3/67 [>.............................] - ETA: 22s - loss: 1.4407 - categorical_accuracy: 0.5556; Current Batch = 220\n",
            " 4/67 [>.............................] - ETA: 28s - loss: 1.4968 - categorical_accuracy: 0.5833; Num of batches = 221\n",
            "; Current Batch = 0\n",
            " 5/67 [=>............................] - ETA: 37s - loss: 1.5189 - categorical_accuracy: 0.5333; Current Batch = 1\n",
            " 6/67 [=>............................] - ETA: 41s - loss: 1.5051 - categorical_accuracy: 0.5000; Current Batch = 2\n",
            " 7/67 [==>...........................] - ETA: 41s - loss: 1.5514 - categorical_accuracy: 0.4762; Current Batch = 3\n",
            " 8/67 [==>...........................] - ETA: 40s - loss: 1.4537 - categorical_accuracy: 0.5000; Current Batch = 4\n",
            " 9/67 [===>..........................] - ETA: 38s - loss: 1.4646 - categorical_accuracy: 0.4815; Current Batch = 5\n",
            "10/67 [===>..........................] - ETA: 36s - loss: 1.4638 - categorical_accuracy: 0.4667; Current Batch = 6\n",
            "11/67 [===>..........................] - ETA: 36s - loss: 1.4491 - categorical_accuracy: 0.4242; Current Batch = 7\n",
            "12/67 [====>.........................] - ETA: 34s - loss: 1.4322 - categorical_accuracy: 0.4167; Current Batch = 8\n",
            "13/67 [====>.........................] - ETA: 32s - loss: 1.4150 - categorical_accuracy: 0.4103; Current Batch = 9\n",
            "14/67 [=====>........................] - ETA: 31s - loss: 1.4834 - categorical_accuracy: 0.4048; Current Batch = 10\n",
            "15/67 [=====>........................] - ETA: 30s - loss: 1.4512 - categorical_accuracy: 0.4000; Current Batch = 11\n",
            "16/67 [======>.......................] - ETA: 30s - loss: 1.4889 - categorical_accuracy: 0.3958; Current Batch = 12\n",
            "17/67 [======>.......................] - ETA: 28s - loss: 1.4387 - categorical_accuracy: 0.4314; Current Batch = 13\n",
            "18/67 [=======>......................] - ETA: 28s - loss: 1.4763 - categorical_accuracy: 0.4259; Current Batch = 14\n",
            "19/67 [=======>......................] - ETA: 28s - loss: 1.4603 - categorical_accuracy: 0.4211; Current Batch = 15\n",
            "20/67 [=======>......................] - ETA: 27s - loss: 1.4397 - categorical_accuracy: 0.4167; Current Batch = 16\n",
            "21/67 [========>.....................] - ETA: 26s - loss: 1.4238 - categorical_accuracy: 0.4127; Current Batch = 17\n",
            "22/67 [========>.....................] - ETA: 26s - loss: 1.4540 - categorical_accuracy: 0.3939; Current Batch = 18\n",
            "23/67 [=========>....................] - ETA: 25s - loss: 1.4379 - categorical_accuracy: 0.4058; Current Batch = 19\n",
            "24/67 [=========>....................] - ETA: 24s - loss: 1.4196 - categorical_accuracy: 0.4306; Current Batch = 20\n",
            "25/67 [==========>...................] - ETA: 24s - loss: 1.3950 - categorical_accuracy: 0.4400; Current Batch = 21\n",
            "26/67 [==========>...................] - ETA: 23s - loss: 1.3946 - categorical_accuracy: 0.4231; Current Batch = 22\n",
            "27/67 [===========>..................] - ETA: 22s - loss: 1.4081 - categorical_accuracy: 0.4198; Current Batch = 23\n",
            "28/67 [===========>..................] - ETA: 21s - loss: 1.4091 - categorical_accuracy: 0.4167; Current Batch = 24\n",
            "29/67 [===========>..................] - ETA: 21s - loss: 1.4037 - categorical_accuracy: 0.4253; Current Batch = 25\n",
            "30/67 [============>.................] - ETA: 21s - loss: 1.3857 - categorical_accuracy: 0.4333; Current Batch = 26\n",
            "31/67 [============>.................] - ETA: 21s - loss: 1.4069 - categorical_accuracy: 0.4301; Current Batch = 27\n",
            "32/67 [=============>................] - ETA: 20s - loss: 1.4039 - categorical_accuracy: 0.4167; Current Batch = 28\n",
            "33/67 [=============>................] - ETA: 20s - loss: 1.3988 - categorical_accuracy: 0.4141; Current Batch = 29\n",
            "34/67 [==============>...............] - ETA: 19s - loss: 1.4069 - categorical_accuracy: 0.4118; Current Batch = 30\n",
            "35/67 [==============>...............] - ETA: 18s - loss: 1.4131 - categorical_accuracy: 0.4000; Current Batch = 31\n",
            "36/67 [===============>..............] - ETA: 18s - loss: 1.4363 - categorical_accuracy: 0.3889; Current Batch = 32\n",
            "37/67 [===============>..............] - ETA: 17s - loss: 1.4481 - categorical_accuracy: 0.3874; Current Batch = 33\n",
            "38/67 [================>.............] - ETA: 16s - loss: 1.4387 - categorical_accuracy: 0.3947; Current Batch = 34\n",
            "39/67 [================>.............] - ETA: 16s - loss: 1.4536 - categorical_accuracy: 0.3846; Current Batch = 35\n",
            "40/67 [================>.............] - ETA: 15s - loss: 1.4674 - categorical_accuracy: 0.3833; Current Batch = 36\n",
            "41/67 [=================>............] - ETA: 14s - loss: 1.4676 - categorical_accuracy: 0.3821; Current Batch = 37\n",
            "42/67 [=================>............] - ETA: 14s - loss: 1.4891 - categorical_accuracy: 0.3730; Current Batch = 38\n",
            "43/67 [==================>...........] - ETA: 13s - loss: 1.5073 - categorical_accuracy: 0.3643; Current Batch = 39\n",
            "44/67 [==================>...........] - ETA: 13s - loss: 1.5254 - categorical_accuracy: 0.3561; Current Batch = 40\n",
            "45/67 [===================>..........] - ETA: 12s - loss: 1.5250 - categorical_accuracy: 0.3556; Current Batch = 41\n",
            "46/67 [===================>..........] - ETA: 11s - loss: 1.5133 - categorical_accuracy: 0.3623; Current Batch = 42\n",
            "47/67 [====================>.........] - ETA: 11s - loss: 1.5407 - categorical_accuracy: 0.3546; Current Batch = 43\n",
            "48/67 [====================>.........] - ETA: 10s - loss: 1.5381 - categorical_accuracy: 0.3542; Current Batch = 44\n",
            "49/67 [====================>.........] - ETA: 9s - loss: 1.5466 - categorical_accuracy: 0.3537 ; Current Batch = 45\n",
            "50/67 [=====================>........] - ETA: 9s - loss: 1.5436 - categorical_accuracy: 0.3467; Current Batch = 46\n",
            "51/67 [=====================>........] - ETA: 8s - loss: 1.5365 - categorical_accuracy: 0.3464; Current Batch = 47\n",
            "52/67 [======================>.......] - ETA: 8s - loss: 1.5363 - categorical_accuracy: 0.3462; Current Batch = 48\n",
            "53/67 [======================>.......] - ETA: 7s - loss: 1.5329 - categorical_accuracy: 0.3459; Current Batch = 49\n",
            "54/67 [=======================>......] - ETA: 7s - loss: 1.5347 - categorical_accuracy: 0.3457; Current Batch = 50\n",
            "55/67 [=======================>......] - ETA: 6s - loss: 1.5245 - categorical_accuracy: 0.3515; Current Batch = 51\n",
            "56/67 [========================>.....] - ETA: 5s - loss: 1.5280 - categorical_accuracy: 0.3571; Current Batch = 52\n",
            "57/67 [========================>.....] - ETA: 5s - loss: 1.5184 - categorical_accuracy: 0.3626; Current Batch = 53\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.5162 - categorical_accuracy: 0.3621; Current Batch = 54\n",
            "59/67 [=========================>....] - ETA: 4s - loss: 1.5097 - categorical_accuracy: 0.3672; Current Batch = 55\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.5038 - categorical_accuracy: 0.3722; Current Batch = 56\n",
            "61/67 [==========================>...] - ETA: 3s - loss: 1.5114 - categorical_accuracy: 0.3661; Current Batch = 57\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.5114 - categorical_accuracy: 0.3656; Current Batch = 58\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.5116 - categorical_accuracy: 0.3651; Current Batch = 59\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.5188 - categorical_accuracy: 0.3594; Current Batch = 60\n",
            "65/67 [============================>.] - ETA: 1s - loss: 1.5203 - categorical_accuracy: 0.3538; Current Batch = 61\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.5267 - categorical_accuracy: 0.3535; Current Batch = 62\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.5330 - categorical_accuracy: 0.3483; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "\n",
            "Epoch 22: saving model to model_init_2023-08-3109_43_59.510610/model-00022-1.53301-0.34826-1.55649-0.43000.h5\n",
            "67/67 [==============================] - 56s 848ms/step - loss: 1.5330 - categorical_accuracy: 0.3483 - val_loss: 1.5565 - val_categorical_accuracy: 0.4300 - lr: 8.0000e-06\n",
            "Epoch 23/25\n",
            "; Current Batch = 63\n",
            " 1/67 [..............................] - ETA: 4s - loss: 1.9245 - categorical_accuracy: 0.3333; Current Batch = 64\n",
            " 2/67 [..............................] - ETA: 22s - loss: 1.5048 - categorical_accuracy: 0.3333; Current Batch = 65\n",
            " 3/67 [>.............................] - ETA: 21s - loss: 1.5530 - categorical_accuracy: 0.3333; Current Batch = 66\n",
            " 4/67 [>.............................] - ETA: 25s - loss: 1.5461 - categorical_accuracy: 0.3333; Current Batch = 67\n",
            " 5/67 [=>............................] - ETA: 25s - loss: 1.6264 - categorical_accuracy: 0.2667; Current Batch = 68\n",
            " 6/67 [=>............................] - ETA: 25s - loss: 1.4879 - categorical_accuracy: 0.3333; Current Batch = 69\n",
            " 7/67 [==>...........................] - ETA: 25s - loss: 1.4528 - categorical_accuracy: 0.3333; Current Batch = 70\n",
            " 8/67 [==>...........................] - ETA: 24s - loss: 1.5361 - categorical_accuracy: 0.3333; Current Batch = 71\n",
            " 9/67 [===>..........................] - ETA: 28s - loss: 1.4802 - categorical_accuracy: 0.3333; Current Batch = 72\n",
            "10/67 [===>..........................] - ETA: 31s - loss: 1.6313 - categorical_accuracy: 0.3333; Current Batch = 73\n",
            "11/67 [===>..........................] - ETA: 30s - loss: 1.5717 - categorical_accuracy: 0.3636; Current Batch = 74\n",
            "12/67 [====>.........................] - ETA: 29s - loss: 1.6053 - categorical_accuracy: 0.3333; Current Batch = 75\n",
            "13/67 [====>.........................] - ETA: 27s - loss: 1.5643 - categorical_accuracy: 0.3590; Current Batch = 76\n",
            "14/67 [=====>........................] - ETA: 26s - loss: 1.6109 - categorical_accuracy: 0.3333; Current Batch = 77\n",
            "15/67 [=====>........................] - ETA: 26s - loss: 1.5950 - categorical_accuracy: 0.3333; Current Batch = 78\n",
            "16/67 [======>.......................] - ETA: 25s - loss: 1.6031 - categorical_accuracy: 0.3333; Current Batch = 79\n",
            "17/67 [======>.......................] - ETA: 25s - loss: 1.6181 - categorical_accuracy: 0.3137; Current Batch = 80\n",
            "18/67 [=======>......................] - ETA: 25s - loss: 1.6007 - categorical_accuracy: 0.3148; Current Batch = 81\n",
            "19/67 [=======>......................] - ETA: 25s - loss: 1.5834 - categorical_accuracy: 0.2982; Current Batch = 82\n",
            "20/67 [=======>......................] - ETA: 24s - loss: 1.5840 - categorical_accuracy: 0.3000; Current Batch = 83\n",
            "21/67 [========>.....................] - ETA: 23s - loss: 1.5566 - categorical_accuracy: 0.3175; Current Batch = 84\n",
            "22/67 [========>.....................] - ETA: 22s - loss: 1.5370 - categorical_accuracy: 0.3333; Current Batch = 85\n",
            "23/67 [=========>....................] - ETA: 21s - loss: 1.5336 - categorical_accuracy: 0.3478; Current Batch = 86\n",
            "24/67 [=========>....................] - ETA: 21s - loss: 1.5291 - categorical_accuracy: 0.3611; Current Batch = 87\n",
            "25/67 [==========>...................] - ETA: 20s - loss: 1.5445 - categorical_accuracy: 0.3600; Current Batch = 88\n",
            "26/67 [==========>...................] - ETA: 20s - loss: 1.5543 - categorical_accuracy: 0.3590; Current Batch = 89\n",
            "27/67 [===========>..................] - ETA: 19s - loss: 1.5422 - categorical_accuracy: 0.3580; Current Batch = 90\n",
            "28/67 [===========>..................] - ETA: 18s - loss: 1.5396 - categorical_accuracy: 0.3571; Current Batch = 91\n",
            "29/67 [===========>..................] - ETA: 18s - loss: 1.5489 - categorical_accuracy: 0.3563; Current Batch = 92\n",
            "30/67 [============>.................] - ETA: 17s - loss: 1.5402 - categorical_accuracy: 0.3556; Current Batch = 93\n",
            "31/67 [============>.................] - ETA: 17s - loss: 1.5097 - categorical_accuracy: 0.3656; Current Batch = 94\n",
            "32/67 [=============>................] - ETA: 16s - loss: 1.5285 - categorical_accuracy: 0.3542; Current Batch = 95\n",
            "33/67 [=============>................] - ETA: 16s - loss: 1.5429 - categorical_accuracy: 0.3434; Current Batch = 96\n",
            "34/67 [==============>...............] - ETA: 15s - loss: 1.5542 - categorical_accuracy: 0.3333; Current Batch = 97\n",
            "35/67 [==============>...............] - ETA: 15s - loss: 1.5585 - categorical_accuracy: 0.3333; Current Batch = 98\n",
            "36/67 [===============>..............] - ETA: 14s - loss: 1.5526 - categorical_accuracy: 0.3333; Current Batch = 99\n",
            "37/67 [===============>..............] - ETA: 14s - loss: 1.5785 - categorical_accuracy: 0.3333; Current Batch = 100\n",
            "38/67 [================>.............] - ETA: 14s - loss: 1.5700 - categorical_accuracy: 0.3333; Current Batch = 101\n",
            "39/67 [================>.............] - ETA: 13s - loss: 1.5768 - categorical_accuracy: 0.3248; Current Batch = 102\n",
            "40/67 [================>.............] - ETA: 13s - loss: 1.5693 - categorical_accuracy: 0.3250; Current Batch = 103\n",
            "41/67 [=================>............] - ETA: 12s - loss: 1.5769 - categorical_accuracy: 0.3171; Current Batch = 104\n",
            "42/67 [=================>............] - ETA: 12s - loss: 1.5840 - categorical_accuracy: 0.3095; Current Batch = 105\n",
            "43/67 [==================>...........] - ETA: 11s - loss: 1.6031 - categorical_accuracy: 0.3023; Current Batch = 106\n",
            "44/67 [==================>...........] - ETA: 11s - loss: 1.6008 - categorical_accuracy: 0.3106; Current Batch = 107\n",
            "45/67 [===================>..........] - ETA: 10s - loss: 1.6126 - categorical_accuracy: 0.3037; Current Batch = 108\n",
            "46/67 [===================>..........] - ETA: 10s - loss: 1.6071 - categorical_accuracy: 0.3116; Current Batch = 109\n",
            "47/67 [====================>.........] - ETA: 9s - loss: 1.6103 - categorical_accuracy: 0.3050 ; Current Batch = 110\n",
            "48/67 [====================>.........] - ETA: 9s - loss: 1.6023 - categorical_accuracy: 0.3056; Current Batch = 111\n",
            "49/67 [====================>.........] - ETA: 8s - loss: 1.5991 - categorical_accuracy: 0.2993; Current Batch = 112\n",
            "50/67 [=====================>........] - ETA: 8s - loss: 1.5969 - categorical_accuracy: 0.3000; Current Batch = 113\n",
            "51/67 [=====================>........] - ETA: 7s - loss: 1.5924 - categorical_accuracy: 0.3072; Current Batch = 114\n",
            "52/67 [======================>.......] - ETA: 7s - loss: 1.5920 - categorical_accuracy: 0.3077; Current Batch = 115\n",
            "53/67 [======================>.......] - ETA: 6s - loss: 1.6075 - categorical_accuracy: 0.3019; Current Batch = 116\n",
            "54/67 [=======================>......] - ETA: 6s - loss: 1.6063 - categorical_accuracy: 0.3025; Current Batch = 117\n",
            "55/67 [=======================>......] - ETA: 5s - loss: 1.6055 - categorical_accuracy: 0.3030; Current Batch = 118\n",
            "56/67 [========================>.....] - ETA: 5s - loss: 1.5982 - categorical_accuracy: 0.3095; Current Batch = 119\n",
            "57/67 [========================>.....] - ETA: 4s - loss: 1.5898 - categorical_accuracy: 0.3099; Current Batch = 120\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.5942 - categorical_accuracy: 0.3046; Current Batch = 121\n",
            "59/67 [=========================>....] - ETA: 3s - loss: 1.5956 - categorical_accuracy: 0.3051; Current Batch = 122\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.5972 - categorical_accuracy: 0.3000; Current Batch = 123\n",
            "61/67 [==========================>...] - ETA: 2s - loss: 1.6074 - categorical_accuracy: 0.2951; Current Batch = 124\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.6048 - categorical_accuracy: 0.2957; Current Batch = 125\n",
            "63/67 [===========================>..] - ETA: 1s - loss: 1.6023 - categorical_accuracy: 0.2963; Current Batch = 126\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.6054 - categorical_accuracy: 0.2969; Current Batch = 127\n",
            "65/67 [============================>.] - ETA: 1s - loss: 1.6135 - categorical_accuracy: 0.2974; Current Batch = 128\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.6138 - categorical_accuracy: 0.2929; Current Batch = 129\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.6058 - categorical_accuracy: 0.2935; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "\n",
            "Epoch 23: saving model to model_init_2023-08-3109_43_59.510610/model-00023-1.60579-0.29353-1.39223-0.47000.h5\n",
            "67/67 [==============================] - 55s 827ms/step - loss: 1.6058 - categorical_accuracy: 0.2935 - val_loss: 1.3922 - val_categorical_accuracy: 0.4700 - lr: 8.0000e-06\n",
            "Epoch 24/25\n",
            "; Current Batch = 130\n",
            " 1/67 [..............................] - ETA: 3s - loss: 1.1618 - categorical_accuracy: 0.3333; Current Batch = 131\n",
            " 2/67 [..............................] - ETA: 35s - loss: 1.5777 - categorical_accuracy: 0.1667; Current Batch = 132\n",
            " 3/67 [>.............................] - ETA: 39s - loss: 1.4425 - categorical_accuracy: 0.3333; Current Batch = 133\n",
            " 4/67 [>.............................] - ETA: 33s - loss: 1.5662 - categorical_accuracy: 0.3333; Current Batch = 134\n",
            " 5/67 [=>............................] - ETA: 33s - loss: 1.4357 - categorical_accuracy: 0.3333; Current Batch = 135\n",
            " 6/67 [=>............................] - ETA: 34s - loss: 1.4885 - categorical_accuracy: 0.3333; Current Batch = 136\n",
            " 7/67 [==>...........................] - ETA: 35s - loss: 1.4533 - categorical_accuracy: 0.3333; Current Batch = 137\n",
            " 8/67 [==>...........................] - ETA: 41s - loss: 1.5067 - categorical_accuracy: 0.3333; Current Batch = 138\n",
            " 9/67 [===>..........................] - ETA: 38s - loss: 1.6155 - categorical_accuracy: 0.2963; Current Batch = 139\n",
            "10/67 [===>..........................] - ETA: 40s - loss: 1.6225 - categorical_accuracy: 0.2667; Current Batch = 140\n",
            "11/67 [===>..........................] - ETA: 37s - loss: 1.5792 - categorical_accuracy: 0.3030; Current Batch = 141\n",
            "12/67 [====>.........................] - ETA: 35s - loss: 1.6000 - categorical_accuracy: 0.2778; Current Batch = 142\n",
            "13/67 [====>.........................] - ETA: 34s - loss: 1.5559 - categorical_accuracy: 0.3077; Current Batch = 143\n",
            "14/67 [=====>........................] - ETA: 32s - loss: 1.5813 - categorical_accuracy: 0.3095; Current Batch = 144\n",
            "15/67 [=====>........................] - ETA: 31s - loss: 1.6189 - categorical_accuracy: 0.2889; Current Batch = 145\n",
            "16/67 [======>.......................] - ETA: 29s - loss: 1.6236 - categorical_accuracy: 0.2917; Current Batch = 146\n",
            "17/67 [======>.......................] - ETA: 28s - loss: 1.5971 - categorical_accuracy: 0.2941; Current Batch = 147\n",
            "18/67 [=======>......................] - ETA: 27s - loss: 1.6134 - categorical_accuracy: 0.2963; Current Batch = 148\n",
            "19/67 [=======>......................] - ETA: 26s - loss: 1.6137 - categorical_accuracy: 0.2982; Current Batch = 149\n",
            "20/67 [=======>......................] - ETA: 25s - loss: 1.5963 - categorical_accuracy: 0.3167; Current Batch = 150\n",
            "21/67 [========>.....................] - ETA: 24s - loss: 1.6052 - categorical_accuracy: 0.3016; Current Batch = 151\n",
            "22/67 [========>.....................] - ETA: 23s - loss: 1.6039 - categorical_accuracy: 0.3182; Current Batch = 152\n",
            "23/67 [=========>....................] - ETA: 23s - loss: 1.5766 - categorical_accuracy: 0.3333; Current Batch = 153\n",
            "24/67 [=========>....................] - ETA: 22s - loss: 1.5548 - categorical_accuracy: 0.3472; Current Batch = 154\n",
            "25/67 [==========>...................] - ETA: 22s - loss: 1.5477 - categorical_accuracy: 0.3467; Current Batch = 155\n",
            "26/67 [==========>...................] - ETA: 21s - loss: 1.5529 - categorical_accuracy: 0.3462; Current Batch = 156\n",
            "27/67 [===========>..................] - ETA: 20s - loss: 1.5411 - categorical_accuracy: 0.3457; Current Batch = 157\n",
            "28/67 [===========>..................] - ETA: 20s - loss: 1.5473 - categorical_accuracy: 0.3333; Current Batch = 158\n",
            "29/67 [===========>..................] - ETA: 19s - loss: 1.5572 - categorical_accuracy: 0.3333; Current Batch = 159\n",
            "30/67 [============>.................] - ETA: 19s - loss: 1.5594 - categorical_accuracy: 0.3222; Current Batch = 160\n",
            "31/67 [============>.................] - ETA: 18s - loss: 1.5501 - categorical_accuracy: 0.3226; Current Batch = 161\n",
            "32/67 [=============>................] - ETA: 18s - loss: 1.5522 - categorical_accuracy: 0.3333; Current Batch = 162\n",
            "33/67 [=============>................] - ETA: 18s - loss: 1.5320 - categorical_accuracy: 0.3434; Current Batch = 163\n",
            "34/67 [==============>...............] - ETA: 18s - loss: 1.5441 - categorical_accuracy: 0.3333; Current Batch = 164\n",
            "35/67 [==============>...............] - ETA: 18s - loss: 1.5369 - categorical_accuracy: 0.3333; Current Batch = 165\n",
            "36/67 [===============>..............] - ETA: 17s - loss: 1.5189 - categorical_accuracy: 0.3519; Current Batch = 166\n",
            "37/67 [===============>..............] - ETA: 17s - loss: 1.5139 - categorical_accuracy: 0.3514; Current Batch = 167\n",
            "38/67 [================>.............] - ETA: 16s - loss: 1.5153 - categorical_accuracy: 0.3596; Current Batch = 168\n",
            "39/67 [================>.............] - ETA: 16s - loss: 1.5119 - categorical_accuracy: 0.3590; Current Batch = 169\n",
            "40/67 [================>.............] - ETA: 15s - loss: 1.5129 - categorical_accuracy: 0.3667; Current Batch = 170\n",
            "41/67 [=================>............] - ETA: 15s - loss: 1.5178 - categorical_accuracy: 0.3577; Current Batch = 171\n",
            "42/67 [=================>............] - ETA: 14s - loss: 1.5179 - categorical_accuracy: 0.3571; Current Batch = 172\n",
            "43/67 [==================>...........] - ETA: 14s - loss: 1.5155 - categorical_accuracy: 0.3566; Current Batch = 173\n",
            "44/67 [==================>...........] - ETA: 13s - loss: 1.5228 - categorical_accuracy: 0.3561; Current Batch = 174\n",
            "45/67 [===================>..........] - ETA: 12s - loss: 1.5343 - categorical_accuracy: 0.3556; Current Batch = 175\n",
            "46/67 [===================>..........] - ETA: 12s - loss: 1.5264 - categorical_accuracy: 0.3623; Current Batch = 176\n",
            "47/67 [====================>.........] - ETA: 11s - loss: 1.5310 - categorical_accuracy: 0.3617; Current Batch = 177\n",
            "48/67 [====================>.........] - ETA: 10s - loss: 1.5314 - categorical_accuracy: 0.3611; Current Batch = 178\n",
            "49/67 [====================>.........] - ETA: 10s - loss: 1.5257 - categorical_accuracy: 0.3673; Current Batch = 179\n",
            "50/67 [=====================>........] - ETA: 9s - loss: 1.5236 - categorical_accuracy: 0.3733 ; Current Batch = 180\n",
            "51/67 [=====================>........] - ETA: 9s - loss: 1.5128 - categorical_accuracy: 0.3791; Current Batch = 181\n",
            "52/67 [======================>.......] - ETA: 8s - loss: 1.5344 - categorical_accuracy: 0.3718; Current Batch = 182\n",
            "53/67 [======================>.......] - ETA: 7s - loss: 1.5287 - categorical_accuracy: 0.3711; Current Batch = 183\n",
            "54/67 [=======================>......] - ETA: 7s - loss: 1.5323 - categorical_accuracy: 0.3704; Current Batch = 184\n",
            "55/67 [=======================>......] - ETA: 6s - loss: 1.5301 - categorical_accuracy: 0.3758; Current Batch = 185\n",
            "56/67 [========================>.....] - ETA: 6s - loss: 1.5149 - categorical_accuracy: 0.3869; Current Batch = 186\n",
            "57/67 [========================>.....] - ETA: 5s - loss: 1.5049 - categorical_accuracy: 0.3918; Current Batch = 187\n",
            "58/67 [========================>.....] - ETA: 5s - loss: 1.5082 - categorical_accuracy: 0.3908; Current Batch = 188\n",
            "59/67 [=========================>....] - ETA: 4s - loss: 1.5133 - categorical_accuracy: 0.3898; Current Batch = 189\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.5084 - categorical_accuracy: 0.3944; Current Batch = 190\n",
            "61/67 [==========================>...] - ETA: 3s - loss: 1.5216 - categorical_accuracy: 0.3934; Current Batch = 191\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.5300 - categorical_accuracy: 0.3871; Current Batch = 192\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.5308 - categorical_accuracy: 0.3810; Current Batch = 193\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.5360 - categorical_accuracy: 0.3802; Current Batch = 194\n",
            "65/67 [============================>.] - ETA: 1s - loss: 1.5277 - categorical_accuracy: 0.3846; Current Batch = 195\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.5197 - categorical_accuracy: 0.3889; Current Batch = 196\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.5251 - categorical_accuracy: 0.3831; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "\n",
            "Epoch 24: saving model to model_init_2023-08-3109_43_59.510610/model-00024-1.52510-0.38308-1.37907-0.50000.h5\n",
            "67/67 [==============================] - 59s 887ms/step - loss: 1.5251 - categorical_accuracy: 0.3831 - val_loss: 1.3791 - val_categorical_accuracy: 0.5000 - lr: 8.0000e-06\n",
            "Epoch 25/25\n",
            "; Current Batch = 197\n",
            " 1/67 [..............................] - ETA: 3s - loss: 0.6427 - categorical_accuracy: 1.0000; Current Batch = 198\n",
            " 2/67 [..............................] - ETA: 37s - loss: 1.4947 - categorical_accuracy: 0.6667; Current Batch = 199\n",
            " 3/67 [>.............................] - ETA: 44s - loss: 1.7092 - categorical_accuracy: 0.4444; Current Batch = 200\n",
            " 4/67 [>.............................] - ETA: 50s - loss: 1.7106 - categorical_accuracy: 0.3333; Current Batch = 201\n",
            " 5/67 [=>............................] - ETA: 52s - loss: 1.6777 - categorical_accuracy: 0.3333; Current Batch = 202\n",
            " 6/67 [=>............................] - ETA: 49s - loss: 1.6126 - categorical_accuracy: 0.3333; Current Batch = 203\n",
            " 7/67 [==>...........................] - ETA: 45s - loss: 1.6430 - categorical_accuracy: 0.3333; Current Batch = 204\n",
            " 8/67 [==>...........................] - ETA: 40s - loss: 1.6717 - categorical_accuracy: 0.2917; Current Batch = 205\n",
            " 9/67 [===>..........................] - ETA: 40s - loss: 1.6721 - categorical_accuracy: 0.2963; Current Batch = 206\n",
            "10/67 [===>..........................] - ETA: 38s - loss: 1.6044 - categorical_accuracy: 0.3000; Current Batch = 207\n",
            "11/67 [===>..........................] - ETA: 36s - loss: 1.6181 - categorical_accuracy: 0.2727; Current Batch = 208\n",
            "12/67 [====>.........................] - ETA: 34s - loss: 1.6334 - categorical_accuracy: 0.2778; Current Batch = 209\n",
            "13/67 [====>.........................] - ETA: 33s - loss: 1.5563 - categorical_accuracy: 0.3333; Current Batch = 210\n",
            "14/67 [=====>........................] - ETA: 32s - loss: 1.5393 - categorical_accuracy: 0.3333; Current Batch = 211\n",
            "15/67 [=====>........................] - ETA: 32s - loss: 1.5076 - categorical_accuracy: 0.3556; Current Batch = 212\n",
            "16/67 [======>.......................] - ETA: 30s - loss: 1.5282 - categorical_accuracy: 0.3333; Current Batch = 213\n",
            "17/67 [======>.......................] - ETA: 29s - loss: 1.5322 - categorical_accuracy: 0.3333; Current Batch = 214\n",
            "18/67 [=======>......................] - ETA: 28s - loss: 1.5093 - categorical_accuracy: 0.3333; Current Batch = 215\n",
            "19/67 [=======>......................] - ETA: 27s - loss: 1.5155 - categorical_accuracy: 0.3333; Current Batch = 216\n",
            "20/67 [=======>......................] - ETA: 26s - loss: 1.5300 - categorical_accuracy: 0.3167; Current Batch = 217\n",
            "21/67 [========>.....................] - ETA: 26s - loss: 1.5270 - categorical_accuracy: 0.3333; Current Batch = 218\n",
            "22/67 [========>.....................] - ETA: 26s - loss: 1.5055 - categorical_accuracy: 0.3333; Current Batch = 219\n",
            "23/67 [=========>....................] - ETA: 25s - loss: 1.4859 - categorical_accuracy: 0.3333; Current Batch = 220\n",
            "24/67 [=========>....................] - ETA: 24s - loss: 1.4875 - categorical_accuracy: 0.3472; Num of batches = 221\n",
            "; Current Batch = 0\n",
            "25/67 [==========>...................] - ETA: 23s - loss: 1.4780 - categorical_accuracy: 0.3467; Current Batch = 1\n",
            "26/67 [==========>...................] - ETA: 23s - loss: 1.4849 - categorical_accuracy: 0.3333; Current Batch = 2\n",
            "27/67 [===========>..................] - ETA: 22s - loss: 1.5222 - categorical_accuracy: 0.3210; Current Batch = 3\n",
            "28/67 [===========>..................] - ETA: 22s - loss: 1.5191 - categorical_accuracy: 0.3214; Current Batch = 4\n",
            "29/67 [===========>..................] - ETA: 21s - loss: 1.5076 - categorical_accuracy: 0.3333; Current Batch = 5\n",
            "30/67 [============>.................] - ETA: 21s - loss: 1.5396 - categorical_accuracy: 0.3222; Current Batch = 6\n",
            "31/67 [============>.................] - ETA: 20s - loss: 1.5230 - categorical_accuracy: 0.3333; Current Batch = 7\n",
            "32/67 [=============>................] - ETA: 20s - loss: 1.5322 - categorical_accuracy: 0.3333; Current Batch = 8\n",
            "33/67 [=============>................] - ETA: 19s - loss: 1.5263 - categorical_accuracy: 0.3333; Current Batch = 9\n",
            "34/67 [==============>...............] - ETA: 18s - loss: 1.5400 - categorical_accuracy: 0.3235; Current Batch = 10\n",
            "35/67 [==============>...............] - ETA: 18s - loss: 1.5279 - categorical_accuracy: 0.3333; Current Batch = 11\n",
            "36/67 [===============>..............] - ETA: 17s - loss: 1.5406 - categorical_accuracy: 0.3241; Current Batch = 12\n",
            "37/67 [===============>..............] - ETA: 16s - loss: 1.5198 - categorical_accuracy: 0.3333; Current Batch = 13\n",
            "38/67 [================>.............] - ETA: 16s - loss: 1.5195 - categorical_accuracy: 0.3246; Current Batch = 14\n",
            "39/67 [================>.............] - ETA: 15s - loss: 1.5135 - categorical_accuracy: 0.3248; Current Batch = 15\n",
            "40/67 [================>.............] - ETA: 14s - loss: 1.5230 - categorical_accuracy: 0.3250; Current Batch = 16\n",
            "41/67 [=================>............] - ETA: 14s - loss: 1.5213 - categorical_accuracy: 0.3333; Current Batch = 17\n",
            "42/67 [=================>............] - ETA: 13s - loss: 1.5124 - categorical_accuracy: 0.3413; Current Batch = 18\n",
            "43/67 [==================>...........] - ETA: 13s - loss: 1.5130 - categorical_accuracy: 0.3488; Current Batch = 19\n",
            "44/67 [==================>...........] - ETA: 12s - loss: 1.5190 - categorical_accuracy: 0.3485; Current Batch = 20\n",
            "45/67 [===================>..........] - ETA: 12s - loss: 1.5305 - categorical_accuracy: 0.3407; Current Batch = 21\n",
            "46/67 [===================>..........] - ETA: 11s - loss: 1.5149 - categorical_accuracy: 0.3478; Current Batch = 22\n",
            "47/67 [====================>.........] - ETA: 10s - loss: 1.5074 - categorical_accuracy: 0.3546; Current Batch = 23\n",
            "48/67 [====================>.........] - ETA: 10s - loss: 1.5145 - categorical_accuracy: 0.3542; Current Batch = 24\n",
            "49/67 [====================>.........] - ETA: 9s - loss: 1.5118 - categorical_accuracy: 0.3537 ; Current Batch = 25\n",
            "50/67 [=====================>........] - ETA: 9s - loss: 1.5092 - categorical_accuracy: 0.3600; Current Batch = 26\n",
            "51/67 [=====================>........] - ETA: 8s - loss: 1.5002 - categorical_accuracy: 0.3725; Current Batch = 27\n",
            "52/67 [======================>.......] - ETA: 8s - loss: 1.5002 - categorical_accuracy: 0.3654; Current Batch = 28\n",
            "53/67 [======================>.......] - ETA: 7s - loss: 1.5057 - categorical_accuracy: 0.3648; Current Batch = 29\n",
            "54/67 [=======================>......] - ETA: 6s - loss: 1.5118 - categorical_accuracy: 0.3642; Current Batch = 30\n",
            "55/67 [=======================>......] - ETA: 6s - loss: 1.5043 - categorical_accuracy: 0.3697; Current Batch = 31\n",
            "56/67 [========================>.....] - ETA: 5s - loss: 1.5043 - categorical_accuracy: 0.3690; Current Batch = 32\n",
            "57/67 [========================>.....] - ETA: 5s - loss: 1.4918 - categorical_accuracy: 0.3743; Current Batch = 33\n",
            "58/67 [========================>.....] - ETA: 4s - loss: 1.4902 - categorical_accuracy: 0.3736; Current Batch = 34\n",
            "59/67 [=========================>....] - ETA: 4s - loss: 1.4825 - categorical_accuracy: 0.3785; Current Batch = 35\n",
            "60/67 [=========================>....] - ETA: 3s - loss: 1.4852 - categorical_accuracy: 0.3778; Current Batch = 36\n",
            "61/67 [==========================>...] - ETA: 3s - loss: 1.4818 - categorical_accuracy: 0.3770; Current Batch = 37\n",
            "62/67 [==========================>...] - ETA: 2s - loss: 1.5063 - categorical_accuracy: 0.3710; Current Batch = 38\n",
            "63/67 [===========================>..] - ETA: 2s - loss: 1.4950 - categorical_accuracy: 0.3757; Current Batch = 39\n",
            "64/67 [===========================>..] - ETA: 1s - loss: 1.4865 - categorical_accuracy: 0.3802; Current Batch = 40\n",
            "65/67 [============================>.] - ETA: 1s - loss: 1.4898 - categorical_accuracy: 0.3744; Current Batch = 41\n",
            "66/67 [============================>.] - ETA: 0s - loss: 1.4789 - categorical_accuracy: 0.3838; Current Batch = 42\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.4786 - categorical_accuracy: 0.3781; Current Batch = 7\n",
            "; Current Batch = 8\n",
            "; Current Batch = 9\n",
            "; Num of batches = 10\n",
            "; Current Batch = 0\n",
            "; Current Batch = 1\n",
            "; Current Batch = 2\n",
            "; Current Batch = 3\n",
            "; Current Batch = 4\n",
            "; Current Batch = 5\n",
            "; Current Batch = 6\n",
            "; Current Batch = 7\n",
            "\n",
            "Epoch 25: saving model to model_init_2023-08-3109_43_59.510610/model-00025-1.47862-0.37811-1.47431-0.45000.h5\n",
            "\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "67/67 [==============================] - 56s 847ms/step - loss: 1.4786 - categorical_accuracy: 0.3781 - val_loss: 1.4743 - val_categorical_accuracy: 0.4500 - lr: 8.0000e-06\n"
          ]
        }
      ],
      "source": [
        "base_model_history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "tObNARbfK_yC",
        "outputId": "7e579c1b-ea0a-4539-b29c-ca8bb97c1788"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max. Training Accuracy 0.4029850661754608\n",
            "Max. Validaiton Accuracy 0.5299999713897705\n"
          ]
        }
      ],
      "source": [
        "plotModelHistory(base_model_history)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}